{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WClw_VPOmwXd"
   },
   "source": [
    "# Data Science for Social Justice Workshop: Module 02\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36-_8ZvxmwXf"
   },
   "source": [
    "## Preprocessing Text Data\n",
    "\n",
    "One type of data that we often work with is text data. This could include social media posts, novels, customer reviews, or interview transcripts. Text is a powerful source of information, but requires specific preprocessing in order to be used in machine learning methods. In this notebook we will go over the preprocessing text, which is the first step that we need to take in order to be able to visualize, model, and analyze text.\n",
    "\n",
    "This notebook is designed to help you: \n",
    "\n",
    "1. Work with CSV files in a Pandas Dataframe;\n",
    "2. Preprocess text from a DataFrame including the following key skills: tokenizing, stopword removal, N-grams extraction, and lemmatization using spaCy;\n",
    "3. Saving the processed text to the DataFrame.\n",
    "\n",
    "In module 1, you completed an introduction to Python fundamentals, including the basics of the Pandas package, which we will use extensively in this notebook. This module is designed to accelerate your coding skills so that you can use Python tools effectively for generating research results. However, if you have less experience with Python, some of this will be overwhelming. That's totally normal, since it takes more than a couple of hours to learn how to code! If you are feeling overwhelmed, it can be helpful to focus on the broader purpose of the functions in the notebook for now and how we can use them to further our research purposes, rather than the details in the code. It can also help to remember that you don't need to memorize every function you want to use. Even programmers who have been working with Python for years will regularly refer to documentation and resources (like this notebook!) while they are developing programs. With time and experience as you progress through the notebooks you will get more comfortable with the Python code underlying this notebook. \n",
    "\n",
    "**Note**: In this notebook, swap out the data in the code for your own data.\n",
    "\n",
    "**Note**: Some of the code will take a long time to execute. The `*` on the left side of the cell will indicate that the cell is still running. The nice thing about preprocessing is that once we have the pipeline complete, we will only have to run these cells one time, even if they take a while.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YUirSNPmwXf"
   },
   "source": [
    "## Changing the Working Directory\n",
    "\n",
    "First, we will use `!pwd` to check the location of our **working directory**. This is the folder that the program is looking into when you are loading and saving files. The default the folder that the current Jupyter Notebook is in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1647459224432,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "BWrHsWO2Bhii",
    "outputId": "4960f553-b1e6-46dc-c637-4b44c27e973e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/emilygrabowski/Documents/GitHub/Data-Science-Social-Justice/notebooks/module02\r\n"
     ]
    }
   ],
   "source": [
    "# \"print\" working directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd6CI1oAbEJq"
   },
   "source": [
    "We can also navigate around by importing the `os` module and using the `chdir()` ('change directory') method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our default working directory is wherever we launched the notebook - in our case the ```module02``` folder. We want to access the \"Data\" folder, which is two levels \"up\".\n",
    "\n",
    "We can change the file path using the following code:  `os.chdir(\"../../Data\")`\n",
    "\n",
    "This means \"go up two levels from the current directory, then go into the folder 'Data'\". You can think of this as the computer navigating the file tree. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# We include two ../ because we want to go two levels up in the file structure\n",
    "os.chdir('../../Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Data folder will always be on the same **relative path** to all of our Jupyter notebooks, we can re-use this line of code whenever we want to change our working directory to the 'Data' folder (Just make sure to import `os` as well!)\n",
    "\n",
    "Now let's check our working directory again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/emilygrabowski/Documents/GitHub/Data-Science-Social-Justice/data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we can use another function from `os` called `listdir()` ('list directory') which returns a list of all of the files in the working directory. We will use this code to make sure we are in the right place (do you see the data folders in the list?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01_preprocessing.ipynb', '03_tf_idf.ipynb', '02_distant_reading.ipynb', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "list_of_files = os.listdir()\n",
    "print(list_of_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your file paths get messed up in all of this navigation, reset the notebook is to restart your kernel using Kernel--> Restart Kernel. This is a helpful trick at any point if the code seems to start behaving unexpectedly: restart the kernel and rerun all cells in order until you find the issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgBDYstgmwX6"
   },
   "source": [
    "## Importing Data with Pandas\n",
    "\n",
    "First step complete! We see our .csv files with data in it. The next step is to **import** the data we are going to work with into Python so that we can work with it. The best way to do that is to import the .csv file as a Pandas dataframe, which will make a data table that we can work with. \n",
    "\n",
    "In this course we will use a dataset taken from [r/amitheasshole](reddit.com/r/AmItheAsshole). \n",
    "\n",
    "The subreddit describes itself as \"A catharsis for the frustrated moral philosopher in all of us, and a place to finally find out if you were wrong in an argument that's been bothering you. Tell us about any non-violent conflict you have experienced; give us both sides of the story, and find out if you're right, or you're the asshole.\"\n",
    "\n",
    "The subreddit has structures in place that the community follow to come to a decision about the situation. First, OP (original poster) writes up the situation, asking AITA (Am I The Asshole). In response, for eighteen hours, the community of the subreddit will respond to the post with one of five judgments: YTA (Youâ€™re The Asshole), NTA (Not The Asshole), ESH (Everyone Sucks Here), NAH (No Assholes Here), or INFO (Not Enough Info).\n",
    "\n",
    "For more info on the subreddit, see [here]('https://www.inverse.com/culture/am-i-the-asshole/amp'). \n",
    "\n",
    "First, we have to retrieve the data. We'll use a subset of the full dataset consisting of the top most popular posts, the assumption being that this will yield the most interesting results (`aita_sub_top_sm.csv`). The full dataset is available as well (in `aita_sub_full.csv`). We will use `pd.read_csv()` to import the .csv file as a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2525,
     "status": "ok",
     "timestamp": 1647459522220,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "bIB1SsShP9iL",
    "outputId": "ab2ec235-c74b-4f81-f7dc-d41825605164"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'aita_sub_top_sm.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1047caf9c8b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# importing file in df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aita_sub_top_sm.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'aita_sub_top_sm.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# importing file in df\n",
    "df = pd.read_csv('aita_sub_top_sm.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded in the file, let's take a look at what's been imported `len(df)` will give the number of rows in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df.dtypes` will list the type of each column in the DataFrame. Note that some columns are already imported as integers or floats. Others are of the 'object type'. If we look at the first few rows in the cell below, we see that those are the columns containing strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1639763487161,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "NcQ9kgs3a0Vo",
    "outputId": "ce80ac74-f323-46bc-e5e0-7505732fc195"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idint                int64\n",
       "idstr               object\n",
       "created              int64\n",
       "self               float64\n",
       "nsfw               float64\n",
       "author              object\n",
       "title               object\n",
       "url                float64\n",
       "selftext            object\n",
       "score              float64\n",
       "subreddit           object\n",
       "distinguish         object\n",
       "textlen            float64\n",
       "num_comments       float64\n",
       "flair_text          object\n",
       "flair_css_class     object\n",
       "augmented_at       float64\n",
       "augmented_count    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzhPGc5HEe7x"
   },
   "source": [
    "We can also show the first few lines of the dataframe using the `.head()` method. This helps confirm that the file imported properly, and see examples of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1647459542614,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "TkI-FuY2mwX6",
    "outputId": "87472d59-a42b-4a76-ff02-3aaa0f474e78",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c42a15b2c7cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzrQAWgVqgwD"
   },
   "source": [
    "This particular dataset only includes the original posts in the subreddit (so not the comments on the posts). The \"selftext\" column contains the actual posts.\n",
    "\n",
    "other columns contain valuable metadata you can use in your analyses, such as: \n",
    "- `created`: the time of the post's creation\n",
    "- `score`: amount of upvotes minus downvotes\n",
    "- `textlen`: amount of words\n",
    "- `num_comments`: the amount of comments\n",
    "- `distinguish`: posts that have been moderated\n",
    "- `nsfw`: posts flagged for NSFW content\n",
    "- `flair_text`: a 'tag' that users within a subreddit can add\n",
    "- `augmented_count`: how often a user or moderator has edited the text\n",
    "\n",
    "`NaN` labels indicate missing values. Missing values are like holes in the DataFrame, and dealing with missing values is an important part of preprocessing. \n",
    "\n",
    "If you want to access the columns of the DataFrame, you can use `df.columns`. This is used frequently for getting the exact spelling of a column, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1639763489064,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "7oZfTJi2rA6Z",
    "outputId": "88d7211b-3e6e-447c-87da-fcffadc71a58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['idint', 'idstr', 'created', 'self', 'nsfw', 'author', 'title', 'url',\n",
       "       'selftext', 'score', 'subreddit', 'distinguish', 'textlen',\n",
       "       'num_comments', 'flair_text', 'flair_css_class', 'augmented_at',\n",
       "       'augmented_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This allows you to quickly see which columns you have\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Columns and Missing Values\n",
    "\n",
    "Now that we have imported the data and confirmed the import worked, let's start preprocessing the **raw data** into **processed data**. Let's first remove some columns that we are not going to use. This is helpful in keeping the data more manageable.\n",
    "\n",
    "[`df.drop()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) takes a list of items to drop (columns or row names). The axis argument indicates whether to drop rows (`axis=0`) or columns (`axis=1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first remove some columns that we are not going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6892b3c656d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'self'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'subreddit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'augmented_at'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'augmented_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df = df.drop(['self','url', 'subreddit', 'augmented_at', 'augmented_count'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaH8ZNOv50it"
   },
   "source": [
    "Let's get rid of some missing values. First, we want to select the rows of deleted posts. On Reddit, removed posts get flagged as \"[removed]\" or \"[deleted]\", so we have to get rid of this too. We can do that with the [`isin()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isin.html) method. Then we can check the new length of the DataFrame to see how many rows are left.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select all rows that don't have 'removed' or 'deleted'\n",
    "df = df[~df['selftext'].isin(['[removed]', '[deleted]' ])]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to drop **null values** These are values that are totally missing, for example if the webscraper wasn't able to extract text for the post. They are replaced with **NaN** which stands for a null value in pandas. We can use [`dropna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html?highlight=dropna#pandas.DataFrame.dropna) to remove the rows with null values in the `selftext` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset='selftext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-Vy6u-1ZWy9"
   },
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Prepocessing is the very first step of NLP projects. The main point of preprocessing is to make the text more interpretable for computers in later machine learning steps. For example if we are counting instances of \"weather\" in text, we might want \"weather\", \"weather.\", \"Weather\" to all be counted as instances of the same word, but in raw text form, these would be treated as separate words. To do so, we can take some common preprocessing steps to help the machine learning process. Some common preprocessing steps are:\n",
    "\n",
    "- Removing punctuations like . , ! $( ) * % @\n",
    "- Removing URLs\n",
    "- Removing stopwords (non-content words like a, the, is)\n",
    "- Lowercasing\n",
    "- Tokenization (e.g. splitting a sentence into words)\n",
    "- Stemming (e.g. places -> place')\n",
    "- Lemmatization (changing words to 'dictionary form' e.g. runs, running, run -> run)\n",
    "\n",
    "Fortunately, we don't need to code every one of these steps. Instead, we will use [spaCy](https://spacy.io/) to do these things. If the text you'd like to process is general-purpose English language text (i.e., not domain-specific, like medical literature), spaCy is ready to use out-of-the-box. We will use the [`en_core_web_sm`](https://spacy.io/models/en/#en_core_web_sm) pipeline to cover the steps listed above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1405,
     "status": "ok",
     "timestamp": 1647462821529,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "XYssvJqaKnua"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7cb7772b8623>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#parse the first reddit post in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mparsed_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselftext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "#load the English preprocessing pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#parse the first reddit post in the dataset\n",
    "parsed_sub = nlp(df.selftext[0])\n",
    "print(parsed_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n94uTq2ZK3X-"
   },
   "source": [
    "The base text looks the same, but we can take a closer look at `parsed_sub` to see what happened under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49r2HQiZK23a"
   },
   "outputs": [],
   "source": [
    "#print each sentence in the parse\n",
    "for num, sentence in enumerate(parsed_sub.sents):\n",
    "    print('Sentence {}:'.format(num + 1))\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1647459908468,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "NaRq9HYRZpxm",
    "outputId": "221cd867-478e-43db-e168-db49aab22186"
   },
   "outputs": [],
   "source": [
    "#print each entity in the parse\n",
    "for num, entity in enumerate(parsed_sub.ents):\n",
    "    print('Entity {}:'.format(num + 1), entity, '-', entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "er_QkUK0RmwU"
   },
   "source": [
    "We can also print out a DataFrame with some of the information spaCy has extracted from our text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1639763496316,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "-bLDqUnULOij",
    "outputId": "c1bb5d1c-1e37-4c76-8847-c69312df02e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>token_stop</th>\n",
       "      <th>token_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My</td>\n",
       "      <td>DET</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girlfriend</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>girlfriend</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recently</td>\n",
       "      <td>ADV</td>\n",
       "      <td>recently</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>went</td>\n",
       "      <td>VERB</td>\n",
       "      <td>go</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>ADP</td>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>beach</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>beach</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>with</td>\n",
       "      <td>ADP</td>\n",
       "      <td>with</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>few</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>few</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>of</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>her</td>\n",
       "      <td>DET</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>friends</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>friend</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text part_of_speech token_lemma entity_type  token_stop  token_punct\n",
       "0           My            DET      -PRON-                    True        False\n",
       "1   girlfriend           NOUN  girlfriend                   False        False\n",
       "2     recently            ADV    recently                   False        False\n",
       "3         went           VERB          go                   False        False\n",
       "4           to            ADP          to                    True        False\n",
       "5          the            DET         the                    True        False\n",
       "6        beach           NOUN       beach                   False        False\n",
       "7         with            ADP        with                    True        False\n",
       "8            a            DET           a                    True        False\n",
       "9          few            ADJ         few                    True        False\n",
       "10          of            ADP          of                    True        False\n",
       "11         her            DET      -PRON-                    True        False\n",
       "12     friends           NOUN      friend                   False        False\n",
       "13           .          PUNCT           .                   False         True\n",
       "14                      SPACE                               False        False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract the first 15 items for the following properties of the parse\n",
    "#orthography \n",
    "token_text = [token.orth_ for token in parsed_sub][:15]   \n",
    "#part of speech \n",
    "token_pos = [token.pos_ for token in parsed_sub][:15]   \n",
    "#lemma (or 'dictionary form')\n",
    "token_lemma = [token.lemma_ for token in parsed_sub][:15]\n",
    "\n",
    "#stop word? t/f\n",
    "token_stop = [token.is_stop for token in parsed_sub][:15]\n",
    "\n",
    "#puncutation? t/f\n",
    "token_punct = [token.is_punct for token in parsed_sub][:15]\n",
    "\n",
    "#make a dataframe with these items\n",
    "pd.DataFrame(zip(token_text, token_pos, token_lemma, token_entity_type, token_stop, token_punct),\n",
    "             columns=['token_text','part_of_speech','token_lemma','entity_type', 'token_stop', 'token_punct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynLOmKULRtwz"
   },
   "source": [
    "Turns out spaCy does a *lot* of work. \n",
    "\n",
    "- It did part of speech tagging (nouns, verbs etc.)\n",
    "- It identified stop words and punctuation\n",
    "- It lemmatized the text. The goal of lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form called a lemma. For instance, \"been\" $\\Rightarrow$ \"be\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ist25mezRy8"
   },
   "source": [
    "## Leftover Tokens\n",
    "\n",
    "While spaCy's tokenizer is great, it has limitations (like any tokenizer). Language is always domain-specific and our data might contain text that spaCy doesn't properly deal with. \n",
    "\n",
    "It is always worth going over some test data to check if your tokenizer works well, and then make some adjustments where needed. For instance, see the following sentence. As a test, we can process the sentence \"That's actually not true\" and check the lemmatized tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "error",
     "timestamp": 1647461221106,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "SHYYe-IlnPAq",
    "outputId": "c038f1b6-c4f7-4526-a75e-bac41702820d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 'â€™', 'actually', 'not', 'true']\n"
     ]
    }
   ],
   "source": [
    "#preprocess the sentence\n",
    "test = nlp(\"Thatâ€™s actually not true\")\n",
    "tokens = []\n",
    "#for each token in the processed object\n",
    "for token in test:\n",
    "    #check if the token is punctuation\n",
    "    if not token.is_punct:\n",
    "        #append the lower-case lemma to the list\n",
    "        tokens.append(token.lemma_.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSub1OMt05kB"
   },
   "source": [
    "Even though we are using an if-statement to filter out punctuation, the apostrophe in `that's` is not getting marked as punctuation in the processing.\n",
    "\n",
    "Let's further investigate with an example with many apostrophes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1639836207497,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "to96M9Iu0eg-",
    "outputId": "0163422e-b34e-4ea2-bfcb-596d2fe13adb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 i\n",
      "1 â€™m\n",
      "2 you\n",
      "3 be\n",
      "4 he\n",
      "5 be\n",
      "6 he\n",
      "7 â€™\n",
      "8 that\n",
      "9 â€™s\n",
      "10 that\n",
      "11 be\n",
      "12 he\n",
      "13 â€™\n",
      "14 he\n",
      "15 's\n",
      "16 father\n",
      "17 â€™s\n",
      "18 father\n",
      "19 's\n"
     ]
    }
   ],
   "source": [
    "test = nlp(\"Iâ€™m you're he's heâ€™s Thatâ€™s that's heâ€™s he's fatherâ€™s father's\")\n",
    "tokens = [token.lemma_.lower() for token in test if not token.is_punct]\n",
    "for i, tokens in enumerate(tokens):\n",
    "    print(i, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCm-8IuD1moC"
   },
   "source": [
    "We can add our own process to enhance the automatic parser in order to deal with these cases. For example, let's remove lemmas that are associated with the possesive \"'s\". How about just adding a little rule of our own: after our first pass, let's add another if-statement to remove problematic lemmas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1639836309108,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "uG3MXlgWmsoS",
    "outputId": "aaffa863-5b39-4618-d3d0-b4597d3d12a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 i\n",
      "1 â€™m\n",
      "2 you\n",
      "3 be\n",
      "4 he\n",
      "5 be\n",
      "6 he\n",
      "7 that\n",
      "8 that\n",
      "9 be\n",
      "10 he\n",
      "11 he\n",
      "12 father\n",
      "13 father\n"
     ]
    }
   ],
   "source": [
    "test = nlp(\"Iâ€™m you're he's heâ€™s Thatâ€™s that's heâ€™s he's fatherâ€™s father's\")\n",
    "tokens = [token.lemma_.lower() if token.lemma_ != '-PRON-' else token.lower_ for token in test if not token.is_punct and not token.is_digit]\n",
    "leftover = [\"'s\",  \"â€™s\", \"â€™\"]\n",
    "tokens_c = [token for token in tokens if not token in leftover]\n",
    "tokens = []\n",
    "for token in test:\n",
    "    if not token.is_punct and not token.is_digit:\n",
    "        if not token.lemma_.lower() in leftover:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "\n",
    "\n",
    "for i, t in enumerate(tokens):\n",
    "    print(i, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still not perfect, but now a lot of those apostrophe's are gone as lemmas. At this point, you can continue to fine tune the automatic text processing pipeline until you are happy with the final result. After that, the next step is to process all of the items in the dataset with that processor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OA0wy-ttNE7u"
   },
   "source": [
    "## Preprocessing all data\n",
    "Now we can scale up to our data. Up until this point, we have run the preprocessing pipeline on a single post, but we want to automate our code to process all posts at once. \n",
    "\n",
    "To do this, Let's define a few helper functions that we'll use for text normalization. In particular, the `lemmatized_sentence_corpus ` generator function will use spaCy to:\n",
    "\n",
    "- Iterate over the DF\n",
    "- Segment the threads into individual sentences\n",
    "- Remove punctuation and excess whitespace\n",
    "- Lemmatize the text\n",
    "\n",
    "These helper functions will automate the preprocessing for the posts in this dataset. Don't worry too much about deciphering each line of code, the main goal of these helper functions is to do a lot of the preprocessing for you so that you can use this text in analysis going forward. We can use these functions wholesale, especially the `preprocess()` function to preprocess our reddit data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1647460151917,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "qtrgcSZH4Rnb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilygrabowski/opt/anaconda3/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "def clean(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation, whitespace, or digits\n",
    "    \"\"\"\n",
    "    return token.is_punct or token.is_space or token.is_digit\n",
    "\n",
    "def line_read(df):\n",
    "    \"\"\"\n",
    "    generator function to read in text from df\n",
    "    and get rid of line breaks in the text\n",
    "    \"\"\"    \n",
    "    for text in df.selftext:\n",
    "        yield text.replace('\\n', '')\n",
    "\n",
    "def preprocess(df, allowed_postags=['NOUN', 'ADJ']):\n",
    "    for parsed in nlp.pipe(line_read(df), batch_size=1000, disable=[\"tok2vec\", \"ner\"]):\n",
    "        lemmas = [token.lemma_.lower() if token.lemma_ != '-PRON-' else token.lower_ for token in parsed if not clean(token)]\n",
    "        lemmas_c = [l for l in lemmas if not l in [\"'s\",  \"â€™s\", \"â€™\"]]\n",
    "        nostops = [term for term in lemmas_c if term not in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "        yield ' '.join(nostops)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run `preprocess()` over our DataFrame and look at the first output. Although it looks less like coherent text, it is cleaner and will be much easier for the computer to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "OjbgskrlDg16"
   },
   "outputs": [],
   "source": [
    "# This will take a while\n",
    "\n",
    "lemmas = [line for line in preprocess(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1639843861051,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "e8fBm9qk-tEb",
    "outputId": "018f2ee6-5596-4715-8a3a-84912495f07d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'girlfriend recently beach friend tiny bikini basically thong hate wear public wear wear pose bathroom mirror hotel room profile picture stick post snapchat story worth mention friend snapchat reason similar sick fight girl night post video sit table like dude invite girl completely unknown arrive guy add snapchat save send pic beach trip screenshot particular snap leave camera roll ass intentional know claim like way stomach look pic beach black bikini look micro bikini bathroom mirror ocean sand friend tit ass bathroom mirror age typical year old behavior nowadays wrong think inappropriate behavior relationship asshole big deal it?**edit guy let clear try control point time tell wear yes hate ass completely bikini tell wear problem wear general wear summer long despite groaning issue find super annoying yes issue send picture ass snapchat plain simple draw line break wear break turn sideways bathroom mirror stick butt picture send snapchat borderline nude suit wildly inappropriate clarity jealous friend beach guy friend completely ok meet guy friend hang issue feel worth mention issue girl night come play explicitly tell come girl night later find bunch dude girlfriend trust thing try big deal remove friend snapchat anymore honesty true problem guy particular hit message non stop start date know boyfriend time snub girl night particular fella attendance night know want want try stop sanity remove snapchat point post dump want detail people control jealous break send pic ass snapchat reason tell wild party phase look start family relationship single mom year old daughter love piece chase year try date tell step way little wild look stage life come early year mature partying wild stuff ready start family agree commit date woman hey pant simply tell stuff tell beginning interest date girl stick wild college party phase end tell mature want agree date begin look think jealous control open honest look year know'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNQTjb50E1Fm"
   },
   "source": [
    "## Phrase modeling with Gensim\n",
    "\n",
    "Many kinds of NLP methods work better when using **N-grams**. An n-gram treats small groups of words as tokens rather than single words. This allows words that frequently appearing together to be concatenated (e.g. \"new york\" means something different and more specific than \"new\" and \"york\" separately). Most commonly, a **bigram** = 2-gram and a **trigram** = 3-gram.\n",
    "\n",
    "**Phrase modeling** is an approach to learning combinations of tokens that together represent meaningful multi-word concepts. We can develop phrase models by looping over the the words in our lemmatized dataset and looking for words that co-occur (i.e., appear one after another) together much more frequently than you would expect them to by random chance. \n",
    "\n",
    "Gensimâ€™s [`Phrases`](https://radimrehurek.com/gensim/models/phrases.html) model implements bigrams, trigrams, quadgrams, etc. `Phrases` detects phrases based on collocation counts. It builds a model of input text that you then can use on other data.\n",
    "\n",
    "\n",
    "Gensim detects a bigram if a scoring function for two words exceeds a threshold. The two important arguments to `Phrases` are `min_count` and `threshold`. The higher the values of these parameters, the harder it is for words to be combined to bigrams. We can change the value of these parameters to fine-tune our model. Try changing `min count` and `threshold` below. How does that change the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1639840527645,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "B_VI6UYEBirn",
    "outputId": "d38f9936-85b7-4d98-a847-f4da9a438ecf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['new_york', 'is', 'great'],\n",
       " ['new_york', 'is', 'in', 'the', 'united', 'states'],\n",
       " ['i', 'love', 'to', 'stay', 'in', 'new_york'],\n",
       " ['people', 'visit', 'the', 'united', 'states']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "docs = ['new york is great', 'new york is in the united states',\n",
    "        'i love to stay in new york', 'people visit the united states']\n",
    "\n",
    "tokens = [doc.split(\" \") for doc in docs]\n",
    "bigram = Phrases(tokens, min_count=2, threshold=3,delimiter='_')\n",
    "bigram_phraser = Phraser(bigram)\n",
    "[bigram_phraser[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a bigram and trigram model for our data. Starting from the preprocessed lemmas from the `preprocess()` function above, we can use the gensim models to identify bigrams and trigrams in the dataset. Again, the `min_count` and `threshold` arguments can be modified to change the output of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63667,
     "status": "ok",
     "timestamp": 1639846696683,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "MxeZ7mc9DeMj",
    "outputId": "a478fee9-f5b1-4562-e3a6-860e244e5719"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-05ff9c375ba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create bigram and trigram models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlemmas_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemmas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmas_s\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbigram_phraser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhraser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lemmas' is not defined"
     ]
    }
   ],
   "source": [
    "# create bigram and trigram models\n",
    "lemmas_s = [doc.split(\" \") for doc in lemmas]\n",
    "bigram = Phrases(lemmas_s, min_count=10, threshold=100)\n",
    "trigram = Phrases(bigram[lemmas_s], min_count=10, threshold=50)  \n",
    "bigram_phraser = Phraser(bigram)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_phraser[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [' '.join(trigram_phraser[bigram_phraser[doc]]) for doc in texts]\n",
    "\n",
    "# Form trigrams\n",
    "trigrams = make_trigrams(lemmas_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1639846157869,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "ic7Il8lyVHpk",
    "outputId": "0ddd51f2-47b4-4a97-dc1d-8fc6db2f85dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parent smoke like chimney use quit wife young son vacation time week invite parent come visit watch son wife date live hour half away spend night come visit occasion tell remind time son bear month ago absolutely smoking address inside outside street car driveway near property want son want ask question big idea know okay morning leave review footage security_camera door dad step driveway smoke time hour wife mom texte dad afterward tell disrespect wish smoke property meet public come house ask stay want stay overnight hotel smoke grandson parent basically think ridiculous smoke brother fine actually true young brother asthma chronic ear infection kid sure exacerbate smoking plus pick year quit grandson danger air thank babysitting babysitte hour carte blanche disrespect wife wish big_deal wonder harsh rule stand maybe nice'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh5peeC6Fi6H"
   },
   "source": [
    "Once our phrase model has been trained on our total dataset, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1639846257823,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "FCR6DWQeFjt8",
    "outputId": "0c7b6e9c-ebe1-400d-e073-7c104aeddd02"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trigram_phraser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e8ae98df43ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrigram_phraser\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"That\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"was\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"not\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"big\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"deal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trigram_phraser' is not defined"
     ]
    }
   ],
   "source": [
    "trigram_phraser[\"That\", \"was\", \"not\", \"a\", \"big\", \"deal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the bigram parser. We can use `.keys()` to identify the bigrams in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1639846804300,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "xO4enOmLeXXi",
    "outputId": "921b6ce4-3390-4f90-ab13-e7bd4e0ce11f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_phraser.phrasegrams.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first few bigrams identified in the model as well to check if they seem like appropriate bigrams. If not, we can change the parameters of the bigram model to adjust the sensitivity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1639846859895,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "uwySeLM4df97",
    "outputId": "0940755f-397e-48cc-ecdb-3558341cb7ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'bathing', b'suit'),\n",
       " (b'mash', b'potato'),\n",
       " (b'social', b'medium'),\n",
       " (b'final', b'straw'),\n",
       " (b'ice', b'cream'),\n",
       " (b'red', b'flag'),\n",
       " (b'credit', b'card'),\n",
       " (b'cerebral', b'palsy'),\n",
       " (b'fast', b'forward'),\n",
       " (b'paternity', b'test')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[bigram for bigram in bigram_phraser.phrasegrams.keys()][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVgem81TZWzu"
   },
   "source": [
    "## Adding and Saving to .csv\n",
    "Finally, let's add our new preprocessed data to our .csv in a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "QBe1-7806O0A"
   },
   "outputs": [],
   "source": [
    "# inserting next to selftext column\n",
    "df.insert(loc=7, column='lemmas', value=trigrams)\n",
    "# removing empty rows in lemmas\n",
    "df = df[~df['lemmas'].isin([''])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to new csv\n",
    "df.to_csv('aita_sub_top_sm_lemmas.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the new column in the dataframe using `.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1639846939166,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "QxzOU6g5fmp8",
    "outputId": "53775808-ac75-41e7-cc5f-86b5f6a4489f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idint</th>\n",
       "      <th>idstr</th>\n",
       "      <th>created</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>score</th>\n",
       "      <th>distinguish</th>\n",
       "      <th>textlen</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>flair_css_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>427576402</td>\n",
       "      <td>t3_72kg2a</td>\n",
       "      <td>1506433689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ritsku</td>\n",
       "      <td>AITA for breaking up with my girlfriend becaus...</td>\n",
       "      <td>My girlfriend recently went to the beach with ...</td>\n",
       "      <td>girlfriend recently beach friend tiny bikini b...</td>\n",
       "      <td>679.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4917.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>no a--holes here</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>551887974</td>\n",
       "      <td>t3_94kvhi</td>\n",
       "      <td>1533404095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hhhhhhffff678</td>\n",
       "      <td>AITA for banning smoking in my house and telli...</td>\n",
       "      <td>My parents smoke like chimneys. I used to as w...</td>\n",
       "      <td>parent smoke like chimney use quit wife young ...</td>\n",
       "      <td>832.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2076.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>asshole</td>\n",
       "      <td>ass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552654542</td>\n",
       "      <td>t3_951az2</td>\n",
       "      <td>1533562299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>creepatthepool</td>\n",
       "      <td>AITA? Creep wears skimpy bathing suit to pool</td>\n",
       "      <td>Hi guys. Throwaway for obv reasons.\\n\\nI'm a f...</td>\n",
       "      <td>hi guy throwaway obv reason i'm female child b...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1741.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>Shitpost</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idint      idstr     created  nsfw          author  \\\n",
       "0  427576402  t3_72kg2a  1506433689   0.0          Ritsku   \n",
       "1  551887974  t3_94kvhi  1533404095   0.0   hhhhhhffff678   \n",
       "2  552654542  t3_951az2  1533562299   0.0  creepatthepool   \n",
       "\n",
       "                                               title  \\\n",
       "0  AITA for breaking up with my girlfriend becaus...   \n",
       "1  AITA for banning smoking in my house and telli...   \n",
       "2      AITA? Creep wears skimpy bathing suit to pool   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  My girlfriend recently went to the beach with ...   \n",
       "1  My parents smoke like chimneys. I used to as w...   \n",
       "2  Hi guys. Throwaway for obv reasons.\\n\\nI'm a f...   \n",
       "\n",
       "                                              lemmas  score distinguish  \\\n",
       "0  girlfriend recently beach friend tiny bikini b...  679.0         NaN   \n",
       "1  parent smoke like chimney use quit wife young ...  832.0         NaN   \n",
       "2  hi guy throwaway obv reason i'm female child b...   23.0         NaN   \n",
       "\n",
       "   textlen  num_comments        flair_text flair_css_class  \n",
       "0   4917.0         434.0  no a--holes here             NaN  \n",
       "1   2076.0         357.0           asshole             ass  \n",
       "2   1741.0         335.0          Shitpost             NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now have the ability to preprocess text data into trigrams and save the output. This gives us clean text that we can then use in our further analysis. This notebook can be used as a primer for preprocessing data and for further reference. However, once our text is processed and the DataFrame saved to a .csv, all we have to do for our next step in the analysis is to load that processed .csv file, rather than having to re-run these models every time we work with the dataset."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 1 Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
