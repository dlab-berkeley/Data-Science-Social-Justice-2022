{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJHDHrQPbjif"
   },
   "source": [
    "# Exploring Texts with `pandas` and `nltk`\n",
    "\n",
    "This notebook introduces some methods to explore text using `pandas` and the Natural Language Toolkit, [`nltk`](https://www.nltk.org/). We'll keep builing on our AITA DataFrame, discussing some simple ways to explore data. \n",
    "\n",
    "This notebook is designed to help you:\n",
    "1. Do some more data operations in Pandas;\n",
    "2. Use NLTK's `Text()` object to perform some basic distant reading operations on a subreddit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6erwYIhh-cR"
   },
   "source": [
    "## Retrieving the Dataset\n",
    "\n",
    "Let's get the data. Make sure you're in the `data` directory when importing by running the magic command `%pwd`.\n",
    "If you're not in the right directory, use `os.chdir` to navigate there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22903,
     "status": "ok",
     "timestamp": 1647477678422,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "qqV-FJC9aHuT",
    "outputId": "9126a240-03b0-4403-eda9-afb71f673262"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# We include two ../ because we want to go two levels up in the file structure\n",
    "os.chdir('../../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKZ6v_EzpUCi"
   },
   "source": [
    "Let's now import the processed data from the last notebook.\n",
    "\n",
    "**Note:** You can replace this line with whichever processed data you have saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5173,
     "status": "ok",
     "timestamp": 1647477683593,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "WBLyQUt1qZRG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('aita_sub_top_sm_lemmas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuIFFo8clcwo"
   },
   "source": [
    "## Diving Deeper into `pandas`\n",
    "\n",
    "Let's get started by looking at some more useful `pandas` operations. First, let's use `head()` to remind ourselves what the data look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 549
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1647477683596,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "zipTYx8rlbOo",
    "outputId": "b92d1d7b-071b-4223-f204-ffeb61a2c802"
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RW9zLOkjNok-"
   },
   "source": [
    "Using the `.sort_values()` method we can sort the df by particular columms. We use two parameters: the `by` parameter indicates by which column we want to sort, the `ascending` parameter indicated whether our sortation is in ascending or descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1647477683596,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "XogbDC5iNok-",
    "outputId": "73ab007f-8658-4813-db51-f2e3c2c3f3e7"
   },
   "outputs": [],
   "source": [
    "# Sort dataframe by highest scores\n",
    "df.sort_values(by=['score'], ascending=False)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5TvYkHCqytt"
   },
   "source": [
    "One thing we often do when weâ€™re exploring a dataset is filtering based on a given condition. For example, we might need to find all the rows in our dataset where the score is over 500. We can use the `.loc[]` method to do so.\n",
    "\n",
    "`.loc[]` is a powerful method that can be used for all kinds of research purposes, if you want to filter or prune your dataset based on some condition. For more info, see [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html).\n",
    "\n",
    "The basic structure of `.loc[]` is `pd.loc[rows, columns]`, where `rows` is the rows to select (usually a conditional as in the cell below), and `columns` is a list of names of columns to select. Either value can be replaced by `:` to mean 'select all columns/rows'\n",
    "\n",
    "For instance, if we only want rows with a score higher than 500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1647477683596,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "5DZbBlbkqytt",
    "outputId": "9e72f5f5-54d3-4042-fc42-da09d1398414"
   },
   "outputs": [],
   "source": [
    "df_top = df.loc[df['score'] >= 500, :]\n",
    "len(df_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of different ways to subset data, but `.loc[]` is one of the most versatile and powerful ways to select different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XTgmxK_pOFx"
   },
   "source": [
    "## Value Counts\n",
    "We can also look at unique value counts for a column by running `value_counts()`. Value counts indicate the number of samples with each unique value for a column.\n",
    "\n",
    "Let's have a look at two particularly interesting columns: `\"flair_text\"` and `\"flair_css_class\"`. A flair, in Reddit, allows users to tag posts or usernames in certain subreddits to add context or humor. Here, the flair attached to the posts are created by moderators after the community votes on whether an OP was or wasn't the asshole. There are also other flairs such as \"No assholes here\" or \"Everyone sucks\". See [here](https://www.reddit.com/r/AmItheAsshole/wiki/faq) for more information.\n",
    "\n",
    "This community-driven data segmentation is quite helpful for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1639766457857,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "rn3_LO27pL9S",
    "outputId": "291cef5e-7263-4f34-87e3-4e88a7f1cd2a"
   },
   "outputs": [],
   "source": [
    "df.flair_text.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a2w-4BvmwYJ"
   },
   "source": [
    "## Type-token ratio\n",
    "Next, let's figure out the **type-token ratio** (TTR) for our posts. Type-token ratio is a crude algorithm to gauge language complexity. First, we'll create a function that computes the TTR. From just reading the code, what does a higher TTR correspond to? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lo1Gz5hmwYJ"
   },
   "outputs": [],
   "source": [
    "def typeTokenRatio(tokens):\n",
    "    \"\"\"Calculates type-token ratio on tokens.\"\"\"\n",
    "    numTokens = len(tokens)\n",
    "    numTypes = len(set(tokens))\n",
    "    return numTypes / numTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPddUWELmwYM"
   },
   "source": [
    "Finally, we loop over the first 10 lemmatized submissions in our dataframe. Do you think that the TTR is capturing a varation in language complexity?\n",
    "\n",
    "The benefit of the TTR is a single number telling us some characteristics of the text, while the drawbacks are just that: a single number will not capture al of the complexity of a text. However, in concert with other techniques, this can be a helpful tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmas'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1647470189383,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "LN3t9CwKIvqy",
    "outputId": "7bb5dff4-00d2-4f08-d326-6e5b20e50b4d"
   },
   "outputs": [],
   "source": [
    "for text in df['lemmas'][:10]:\n",
    "    tokens = text.split()\n",
    "    print('Text:\\n', text)\n",
    "    print('TTR:', typeTokenRatio(tokens), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yK1DCuAJbjkh"
   },
   "source": [
    "## Processing and Analyzing Language with `Text()`\n",
    "\n",
    "Let's have another look at our data. Another powerful tool for initially exploring texts, `nltk` provides a `Text()` class that can process text and supports opreations such as **counting**, **concordancing**, and **collocation discovery**. \n",
    "\n",
    "Let's use our \"lemmas\" data we created in the last notebook. All we need to do to get the total tokens is run `split()` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if you do not have nltk installed\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for idx, row in enumerate(df['lemmas']):\n",
    "    # Notice that we put all tokens in the same list\n",
    "    tokens.extend(row.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1639772391510,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "cSnOXiTK8KjJ",
    "outputId": "fff12139-267a-4dbd-bc97-cef445f9d3c6"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.text import Text\n",
    "\n",
    "aita_tokens = Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAdI-HgMbjkl"
   },
   "source": [
    "Let's print out the \"docstring\" of the [`Text()`](https://www.nltk.org/api/nltk.text.html) object, as well as all the things you can do with this object. Have a read through this to see what it allows you to do! We will talk about five simple operations here, but there's many others that can be helpful - take a look at the docstring below to read a bit more about them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvexG4e7bjkm"
   },
   "outputs": [],
   "source": [
    "help(Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPatGDHDOpaU"
   },
   "source": [
    "### Concordances \n",
    "One of the most basic, but quite helpful, ways to quickly get an overview of the contexts in which a word appears is through a **concordance** view. `Text.concordance()` takes as input a word and a window width, and shows all cases where that word appears in a text and a window of context around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1639772396041,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "nkDsXJkPbjkp",
    "outputId": "b9f3ec29-0485-438f-d1ab-628168a0a81e"
   },
   "outputs": [],
   "source": [
    "aita_tokens.concordance('mistake', width=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJxpt0sDgJ6A"
   },
   "source": [
    "### Collocations\n",
    "A **collocation** is a sequence of words that often appear together. The `.collocation_list()` method will help identify words that often appear together in our texts. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1763,
     "status": "ok",
     "timestamp": 1639772405159,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "rWz18pTIgGtq",
    "outputId": "75eb133e-f126-412a-b8a4-ae9929bc6fe3"
   },
   "outputs": [],
   "source": [
    "aita_tokens.collocation_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change input arguments\n",
    "aita_tokens.collocation_list(num=30, window_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np8M59BeDJll"
   },
   "source": [
    "### Word Plotting\n",
    "\n",
    "Using the `dispersion_plot()` method we can easily visualize how often some word appears throughout the text. We input a list of words to plot, and identify how often they occur in the data over time.\n",
    "\n",
    "This is useful when you are interested in seeing how words behave in the text over time, e.g., when novel words begin to appear in text. Note: the dataframe will need to be sorted by date for this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 903,
     "status": "ok",
     "timestamp": 1639772409569,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "_myUpUcqDIdh",
    "outputId": "86b411c5-743e-4781-f6a4-0f0cf10b647d"
   },
   "outputs": [],
   "source": [
    "aita_tokens.dispersion_plot([\"unhappy\", \"marriage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5q_G1GKkZSy"
   },
   "source": [
    "### Similar Words\n",
    "\n",
    "We might also want to look at similar words in the dataset (as defined as appearing in the same context). This is **distributional similarity**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4512,
     "status": "ok",
     "timestamp": 1639772469227,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "mWhoVtkrjBx_",
    "outputId": "10e2820d-d39e-4c3c-8799-42a5b78f5c3a"
   },
   "outputs": [],
   "source": [
    "aita_tokens.similar('partner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOC5V4Pje0G9"
   },
   "source": [
    "### Common Context\n",
    "\n",
    "The `.common_contexts()` method allows us to study the **common context** of two or more words: when they occur in the same immediate context. We must enclose these words in square brackets and round brackets, separated by commas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1639772490454,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "ccJOlE_se6Dp",
    "outputId": "1e9e204d-36d5-4dcd-8fd8-fac68833f0d2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aita_tokens.common_contexts(['mom', 'dad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating Time\n",
    "\n",
    "One of the most valuable metadata variables working with text is time. If we know when a text was created, then we can explore all sorts of properties in the text, and their dynamics.\n",
    "\n",
    "In this dataset, the time is contained in the `created` column. It is a Unix timestamp: the number of seconds that have elapsed since the Unix epoch, minus leap seconds; the Unix epoch is 00:00:00 UTC on 1 January 1970. While this works great for computers, it's not a particularly helpful format for humans! So we use `pd.datetime()` to convert this number to a more readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(1207632114, unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new column in `pandas` is as easy as using the bracket notation to write a new column name, then assigning it. In this case, we just use the `.to_datetime()` method again to point to the entire \"created\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(loc=3, column='created_datetime', value=pd.to_datetime(df['created'], unit='s'))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select particular components of the timestamps by calling `.year`, `.month`,`.day`, etc. on the datetime column.\n",
    "\n",
    "Thinking back to the flair column for this dataset, let's see if we can find out whether people are considered assholes more frequently in particular months. \n",
    "We'll first create a new df with just the submissions from 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2021 = df.loc[pd.DatetimeIndex(df['created_datetime']).year == 2021, :]\n",
    "len(df_2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `month_name()` method of `DateTimeIndex`, we can see which month each post was written in:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_array = pd.DatetimeIndex(df_2021['created_datetime']).month_name()\n",
    "months_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all we need to visualize the data. We will use the seaborn library to plot `df_2021`, using the `Datetimeindex` array we just created to separate counts on the x-axis. Do you see any pattnerns in the flair assignment across months?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "sns.set(rc={'figure.figsize': (7, 5)})\n",
    "\n",
    "p = sns.histplot(\n",
    "    data=df_2021, \n",
    "    x=months_array,\n",
    "    hue=\"flair_css_class\",\n",
    "    multiple=\"stack\")\n",
    "\n",
    "plt.xticks(rotation=70)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our data with the datetime objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('aita_sub_top_sm_lemmas.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we covered some techniques for summarizing text:\n",
    "\n",
    "- Using `pandas` to sort, filter and summarize text.\n",
    "- Using metrics like the token-type-ratio.\n",
    "- Using the `nltk` package to quickly process and visualize text.\n",
    "\n",
    "These techniques are particularly helpful for getting to know your dataset, and exploring text in an efficient way. In the next notebook we will talk about converting text to numeric data that can be used for many useful machine learning techniques."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 2 Distant Reading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
