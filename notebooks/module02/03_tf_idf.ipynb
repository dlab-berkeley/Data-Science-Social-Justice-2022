{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bhs6S4deZFWW"
   },
   "source": [
    "# Data Science for Social Justice Workshop: Module 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJHDHrQPbjif"
   },
   "source": [
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "In Notebook 1, we covered methods to clean and tokenize text into units (like words, bigrams, and trigrams). In Notebook 2, we covered methods to explore the text, including collocations, frequency, and common context. However, for many data science applications, we need a way to convert the content of a text (aka the list of tokens) into useful numbers that can be used as the features of a model or analysis. One way to do this is to to use a method called Term Frequency- Inverse Document Frequency (TF-IDF). This notebook introduces TF-IDF for comparing subsets of a reddit. \n",
    "\n",
    "This notebook is designed to help you use TF-IDF to:\n",
    "1. Ccompare (subsets of) datasets\n",
    "2. Find most-distinctive words in a subreddit\n",
    "3. Find similar posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6erwYIhh-cR"
   },
   "source": [
    "## Retrieving the dataset\n",
    "Let's get the data. Make sure you're in the \"Data\" directory when importing by running the magic command `%pwd`.\n",
    "If you're not in the right directory, use `os.chdir` to navigate there. We are importing the processed data saved at the end of Notebook 1.\n",
    "\n",
    "**Note**: Feel free to replace the file with the filename of your project data for the workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/emilygrabowski/Documents/GitHub/Data-Science-Social-Justice/data'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22903,
     "status": "ok",
     "timestamp": 1647477678422,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "qqV-FJC9aHuT",
    "outputId": "9126a240-03b0-4403-eda9-afb71f673262"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# We include two ../ because we want to go two levels up in the file structure\n",
    "#os.chdir('../../Data')\n",
    "\n",
    "reddf = pd.read_csv('aita_sub_top_sm_lemmas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idint</th>\n",
       "      <th>idstr</th>\n",
       "      <th>created</th>\n",
       "      <th>created_datetime</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>score</th>\n",
       "      <th>distinguish</th>\n",
       "      <th>textlen</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>flair_css_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>427576402</td>\n",
       "      <td>t3_72kg2a</td>\n",
       "      <td>1506433689</td>\n",
       "      <td>2017-09-26 13:48:09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ritsku</td>\n",
       "      <td>AITA for breaking up with my girlfriend becaus...</td>\n",
       "      <td>My girlfriend recently went to the beach with ...</td>\n",
       "      <td>girlfriend recently went beach friends tiny bi...</td>\n",
       "      <td>679.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4917.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>no a--holes here</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>551887974</td>\n",
       "      <td>t3_94kvhi</td>\n",
       "      <td>1533404095</td>\n",
       "      <td>2018-08-04 17:34:55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hhhhhhffff678</td>\n",
       "      <td>AITA for banning smoking in my house and telli...</td>\n",
       "      <td>My parents smoke like chimneys. I used to as w...</td>\n",
       "      <td>parents smoke like chimneys quit wife got youn...</td>\n",
       "      <td>832.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2076.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>asshole</td>\n",
       "      <td>ass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552654542</td>\n",
       "      <td>t3_951az2</td>\n",
       "      <td>1533562299</td>\n",
       "      <td>2018-08-06 13:31:39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>creepatthepool</td>\n",
       "      <td>AITA? Creep wears skimpy bathing suit to pool</td>\n",
       "      <td>Hi guys. Throwaway for obv reasons.\\n\\nI'm a f...</td>\n",
       "      <td>hi guys throwaway obv reasons i'm female child...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1741.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>Shitpost</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idint      idstr     created     created_datetime  nsfw  \\\n",
       "0  427576402  t3_72kg2a  1506433689  2017-09-26 13:48:09   0.0   \n",
       "1  551887974  t3_94kvhi  1533404095  2018-08-04 17:34:55   0.0   \n",
       "2  552654542  t3_951az2  1533562299  2018-08-06 13:31:39   0.0   \n",
       "\n",
       "           author                                              title  \\\n",
       "0          Ritsku  AITA for breaking up with my girlfriend becaus...   \n",
       "1   hhhhhhffff678  AITA for banning smoking in my house and telli...   \n",
       "2  creepatthepool      AITA? Creep wears skimpy bathing suit to pool   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  My girlfriend recently went to the beach with ...   \n",
       "1  My parents smoke like chimneys. I used to as w...   \n",
       "2  Hi guys. Throwaway for obv reasons.\\n\\nI'm a f...   \n",
       "\n",
       "                                              lemmas  score distinguish  \\\n",
       "0  girlfriend recently went beach friends tiny bi...  679.0         NaN   \n",
       "1  parents smoke like chimneys quit wife got youn...  832.0         NaN   \n",
       "2  hi guys throwaway obv reasons i'm female child...   23.0         NaN   \n",
       "\n",
       "   textlen  num_comments        flair_text flair_css_class  \n",
       "0   4917.0         434.0  no a--holes here             NaN  \n",
       "1   2076.0         357.0           asshole             ass  \n",
       "2   1741.0         335.0          Shitpost             NaN  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpWynSoy4t8o"
   },
   "source": [
    "## Implementing TF-IDF\n",
    "TF-IDF, short for **term frequency–inverse document frequency**, is a metric that reflects how important a word is to a **document** in a collection or **corpus**. When talking about text datasets, the dataset is called a corpus, and each datapoint is a document. A document can be a post, a paragraph, a webpage, whatever is considered the individual unit of text for a given datset. **Term** is each unique token in a document (we previously also referred to this as **type**). \n",
    "\n",
    "For example in a corpus of sentences, a document might be: \"I went to New York City in New York state.\" \n",
    "\n",
    "The processed tokens in that document might be: [went, new_york, city, new_york, state]. \n",
    "\n",
    "The document would have four unique terms: [went, new_york, city, state].\n",
    "\n",
    "The TF-IDF value increases proportionally to the number of times a word appears in the document (the term frequency, or TF), and is offset by the number of documents in the corpus that contain the word (the inverse document frequency, or IDF). This helps to adjust for the fact that some words appear more frequently in general – such as articles and prepositions.\n",
    "\n",
    "We'll talk about the math behind calculating the TF-IDF but the key components to remember are: \n",
    "1. There is one TF-IDF score per unique term and each document\n",
    "2. A high TF-IDF score means that term is descriptive of that document\n",
    "3. A low TF-IDF score may be because either the term is not frequent in that document, or that it is frequent in many documents in the dataset - either way, it is not a good descriptor of that document.\n",
    "\n",
    "The intuition is that if a word occurs many times in one post but rarely in the rest of the corpus, it is probably useful for characterizing that post; conversely, if a word occurs frequently in a post but also occurs frequently in the corpus, it is probably less characteristic of that post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [`CountVectorizer()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to generate the term-frequency matrix for a given corpus, which is the first part of calculating the tf-idf. This is a table where nrows = # documents, and ncols = # terms in the corpus. Each cell gives a count for the frequency of that term in the document (which may be zero). Let's try it on a toy dataset. Conveniently, there is a built-in tokenizer in CountVectorizer that we will use for now to preprocess the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1639773021096,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "GLxYUFszEA0M",
    "outputId": "ff40b717-b7f4-4710-bd43-a0c7613422f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agree</th>\n",
       "      <th>but</th>\n",
       "      <th>can</th>\n",
       "      <th>cat</th>\n",
       "      <th>does</th>\n",
       "      <th>dog</th>\n",
       "      <th>has</th>\n",
       "      <th>let</th>\n",
       "      <th>likes</th>\n",
       "      <th>my</th>\n",
       "      <th>not</th>\n",
       "      <th>our</th>\n",
       "      <th>out</th>\n",
       "      <th>paws</th>\n",
       "      <th>really</th>\n",
       "      <th>the</th>\n",
       "      <th>we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agree  but  can  cat  does  dog  has  let  likes  my  not  our  out  paws  \\\n",
       "0      0    0    0    1     0    0    1    0      0   1    0    0    0     1   \n",
       "1      0    0    1    0     0    1    0    1      0   0    0    0    1     0   \n",
       "2      1    1    0    2     1    1    0    0      1   0    1    1    0     0   \n",
       "\n",
       "   really  the  we  \n",
       "0       0    0   0  \n",
       "1       0    1   1  \n",
       "2       1    2   0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "  'My cat has paws.',\n",
    "  'Can we let the dog out?',\n",
    "  'Our dog really likes the cat but the cat does not agree.']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "#vectorizer.get_feature_names_out()\n",
    "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtaTTaP8EA0M"
   },
   "source": [
    "Each column in the matrix represents a unique word in the vocabulary, while each row represents the document in our dataset. In this case, we have three sentences (i.e. the document), and therefore we three rows. The values in each cell are the word counts. Note that with this representation, counts of some words could be 0 if the word did not appear in the corresponding document. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsNok5KyEA0M"
   },
   "source": [
    "Lets look at the shape of the matrix. How many unique words are there in the matrix?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1639773036420,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "6mKvaLCnEA0M",
    "outputId": "259f70fb-d186-4330-d9a7-d726459105fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 17)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6kPKfAnFvKY"
   },
   "source": [
    "Now we have numbers representing the contents of the documents!  This is the first step of Matrices like this are the simplest way to represent texts. However, it biases most frequent words and ends up ignoring rare words which could have helped is in processing our data more efficiently.\n",
    "\n",
    "Oftentimes we not only want to focus on the frequency of words present in the corpus but also want to know the importance of the words. This is where tf-idf  (term frequency-inverse document frequency) comes in. Tf-idf is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "This is done by combining the **term frequency** calculated above and the **inverse document frequency** of the word across a set of documents. For the latter, we divide the total number of documents by the number of documents containing the term, and then take the logarithm of that quotient. (This is calculated for us by the computer). This tells us if a word is common or rare across all documents.\n",
    "\n",
    "The combination of these two metrics into the TF-IDF will identify how unique a term is to that document.\n",
    "\n",
    "\n",
    "**Note**: Word order is not retained in this type of featurization, since all that is counted is overall frequency of a word in a document. This is called a **bag-of-words** approach, and significantly simplifies the representation problem, but has been found to be effective in capturing key features of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvxZ-5f6ZFWe"
   },
   "source": [
    "### Testing tf-idf with a toy dataset\n",
    "\n",
    "Let's try tf-idf out with a toy dataset. Here we have three documents about Python, but with different meanings. If we are trying to distinguish between these documents, the word \"Python\" would not be very useful, since it occurs in all of the documents, but other terms might, like \"Monty\",\"snake\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "MjjWn-phZFWf"
   },
   "outputs": [],
   "source": [
    "document1 = \"\"\"Python is a 2000 made-for-TV horror movie directed by Richard\n",
    "Clabaugh. The film features several cult favorite actors, including William\n",
    "Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy,\n",
    "Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the\n",
    "A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean\n",
    "Whalen.\"\"\"\n",
    "\n",
    "document2 = \"\"\"Python, from the Greek word (πύθων/πύθωνας), is a genus of\n",
    "nonvenomous pythons[2] found in Africa and Asia. Currently, 7 species are\n",
    "recognised.[2] A member of this genus, P. reticulatus, is among the longest\n",
    "snakes known.\"\"\"\n",
    "\n",
    "document3 = \"\"\"Monty Python (also collectively known as the Pythons) are a British \n",
    "surreal comedy group who created the sketch comedy television show Monty Python's \n",
    "Flying Circus, which first aired on the BBC in 1969. Forty-five episodes were made \n",
    "over four series.\"\"\"\n",
    "\n",
    "document4 = \"\"\"Python is an interpreted, high-level, general-purpose programming language. \n",
    "Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes \n",
    "code readability with its notable use of significant whitespace. Its language constructs and \n",
    "object-oriented approach aim to help programmers write clear, logical code for small and \n",
    "large-scale projects.\"\"\"\n",
    "\n",
    "document5 = \"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
    "manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
    "It is sometimes referred to as a \"Combat Magnum\". It was first introduced\n",
    "in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
    "Colt Python targeted the premium revolver market segment.\"\"\"\n",
    "\n",
    "document6 = \"\"\"The Pythonidae, commonly known simply as pythons, from the Greek word python \n",
    "(πυθων), are a family of nonvenomous snakes found in Africa, Asia, and Australia. \n",
    "Among its members are some of the largest snakes in the world. Eight genera and 31\n",
    "species are currently recognized.\"\"\"\n",
    "\n",
    "test_list = [document1, document2, document3, document4, document5, document6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESR7eMThTkZE"
   },
   "source": [
    "Let's `CountVectorizer()` again, and customize some parameters. Using `max_df` (max document frequency) we can get rid of words that appear in more than 85% of the corpus, and using `stop_words` we can insert an English stopword list to leave out of the calculations as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1639776660955,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "Os8jrh3RM_GJ",
    "outputId": "b7adadfc-b6fa-4752-b8c1-08527bd07703"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1955</th>\n",
       "      <th>1969</th>\n",
       "      <th>1991</th>\n",
       "      <th>2000</th>\n",
       "      <th>31</th>\n",
       "      <th>357</th>\n",
       "      <th>44</th>\n",
       "      <th>actors</th>\n",
       "      <th>africa</th>\n",
       "      <th>aim</th>\n",
       "      <th>...</th>\n",
       "      <th>wil</th>\n",
       "      <th>william</th>\n",
       "      <th>word</th>\n",
       "      <th>world</th>\n",
       "      <th>write</th>\n",
       "      <th>year</th>\n",
       "      <th>zabka</th>\n",
       "      <th>πυθων</th>\n",
       "      <th>πύθων</th>\n",
       "      <th>πύθωνας</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1955  1969  1991  2000  31  357  44  actors  africa  aim  ...  wil  \\\n",
       "0     0     0     0     1   0    0   0       1       0    0  ...    1   \n",
       "1     0     0     0     0   0    0   0       0       1    0  ...    0   \n",
       "2     0     1     0     0   0    0   0       0       0    0  ...    0   \n",
       "3     0     0     1     0   0    0   0       0       0    1  ...    0   \n",
       "4     1     0     0     0   0    1   1       0       0    0  ...    0   \n",
       "5     0     0     0     0   1    0   0       0       1    0  ...    0   \n",
       "\n",
       "   william  word  world  write  year  zabka  πυθων  πύθων  πύθωνας  \n",
       "0        1     0      0      0     0      1      0      0        0  \n",
       "1        0     1      0      0     0      0      0      1        1  \n",
       "2        0     0      0      0     0      0      0      0        0  \n",
       "3        0     0      0      1     0      0      0      0        0  \n",
       "4        0     0      0      0     1      0      0      0        0  \n",
       "5        0     1      1      0     0      0      1      0        0  \n",
       "\n",
       "[6 rows x 147 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_df=0.85, stop_words='english')\n",
    "word_count_vector = cv.fit_transform(test_list)\n",
    "pd.DataFrame(word_count_vector.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkpVqpPzNHRd"
   },
   "source": [
    "How many documents and unique words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVLHjD29ZFWi"
   },
   "source": [
    "## Using `TfidfTransformer`\n",
    "\n",
    "Next, we need to compute the inverse document frequency values. We'll call [`tfidf_transformer.fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) on the word counts we computed earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1639776806708,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "WeTAynM9xxxr",
    "outputId": "a5c7fd51-b4cf-4c16-e643-388303891e01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer() \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Uoj5xOi8vu_"
   },
   "source": [
    "To get a glimpse of how the IDF values look, let's put these into a DataFrame and sort by weights. Remember, a low idf indicates something that is less unique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1639776807884,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "gRjP38dV-7I7",
    "outputId": "ba06ac97-0405-47f1-d01d-db1bbf5ca801"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>known</th>\n",
       "      <td>1.336472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pythons</th>\n",
       "      <td>1.559616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greek</th>\n",
       "      <td>1.847298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nonvenomous</th>\n",
       "      <td>1.847298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created</th>\n",
       "      <td>1.847298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>2.252763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>films</th>\n",
       "      <td>2.252763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flying</th>\n",
       "      <td>2.252763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fame</th>\n",
       "      <td>2.252763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>πύθωνας</th>\n",
       "      <td>2.252763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             idf_weights\n",
       "known           1.336472\n",
       "pythons         1.559616\n",
       "greek           1.847298\n",
       "nonvenomous     1.847298\n",
       "created         1.847298\n",
       "...                  ...\n",
       "film            2.252763\n",
       "films           2.252763\n",
       "flying          2.252763\n",
       "fame            2.252763\n",
       "πύθωνας         2.252763\n",
       "\n",
       "[147 rows x 1 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(), columns=[\"idf_weights\"]) \n",
    " \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFeXhx71O_4n"
   },
   "source": [
    "Notice that the words \"python\" and \"in\" have the lowest idf values. This is expected: these words appear in each and every document in our collection. The lower the idf value of a word, the less unique it is to any particular document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGuoetn0ASpf"
   },
   "source": [
    "Now that we have the idf values, we can compute the tf-idf scores for our set of documents using `.transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "hWPqOTR_AIM0"
   },
   "outputs": [],
   "source": [
    "tf_idf_vector=tfidf_transformer.transform(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0-6H9L9PnLA"
   },
   "source": [
    "By invoking `tfidf_transformer.transform()` we are computing the tf-idf scores for our docs. Internally this is weighting tf scores by their idf scores, so that the more unique a word, the more its frequency counts in a given document.\n",
    "\n",
    "Let’s print the tf-idf values of the first document to see if it makes sense. What we are doing below is, placing the tf-idf scores from the third document into a pandas data frame and sorting it in descending order of scores. We can replace the index in `tf_idf_vector[]` to select a different document to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1639776812749,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "Qu40IZhpP0b2",
    "outputId": "fbf33f79-7be4-4c0e-8853-02ab6b7cf616"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>comedy</th>\n",
       "      <td>0.424705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monty</th>\n",
       "      <td>0.424705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>circus</th>\n",
       "      <td>0.212353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>television</th>\n",
       "      <td>0.212353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surreal</th>\n",
       "      <td>0.212353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>englund</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emphasizes</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elm</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discontinued</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>πύθωνας</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tfidf\n",
       "comedy        0.424705\n",
       "monty         0.424705\n",
       "circus        0.212353\n",
       "television    0.212353\n",
       "surreal       0.212353\n",
       "...                ...\n",
       "englund       0.000000\n",
       "emphasizes    0.000000\n",
       "elm           0.000000\n",
       "discontinued  0.000000\n",
       "πύθωνας       0.000000\n",
       "\n",
       "[147 rows x 1 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names() \n",
    "  \n",
    "#print the scores \n",
    "df = pd.DataFrame(tf_idf_vector[2].T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9qVYEm_ZFW2"
   },
   "source": [
    "What are the most distinctive words for document 3 (Highest tf-idf scores?) Does it made sense given the document and corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq3QvXEcZFXS"
   },
   "source": [
    "## Using tf-idf on Reddit datasets\n",
    "Now that we have a good grasp of how TF-IDF is calculated, let's perform this method on our Reddit data. Let's say, that rather than wanting to compare individual posts (for now) we want to compare terms that are important to two subsets of the Reddit. First, let's create two dataframes to seperate the data based on whether they were classified as \"assholeish\" and \"non-assholeish\" posts by the communities. Essentially our corpus will have two documents, one for each division of the subreddit. We'll put the relevant lemmatized texts into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ass = reddf.loc[reddf.flair_css_class == \"ass\"]\n",
    "df_ass.reset_index(inplace=True)\n",
    "df_not = reddf.loc[reddf.flair_css_class == \"not\"]\n",
    "df_not.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "aSQHlQliZFXV"
   },
   "outputs": [],
   "source": [
    "aita_list = [' '.join(df_ass['lemmas']), ' '.join(df_not['lemmas'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgKvuY96ZFXY"
   },
   "source": [
    "This time, to save time, we will be using Scikit-LEARN [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer). It is a class that basically allows us to create a matrix of word counts (what we just did with `CountVectorizer`), and immediately transform them into tf-idf values. \n",
    "\n",
    "We simply instantiate an object of the `TfidfVectorizer`. Then, we run it by applying the `fit_transform()` method to our two-document corpus `aita_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "1vRKbLpDZFXY"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# settings that you use for count vectorizer will go here\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, decode_error='ignore',\n",
    "                                   stop_words='english',smooth_idf=True,use_idf=True)\n",
    "\n",
    "# fit and transform the texts\n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(aita_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our first df (assholes), and see which words are typical of those posts when compared to the posts which are by non-assholes. Do the numbers make sense? Do you notice patterns in the terms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1639778604305,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "cghW5hxxZ8Ec",
    "outputId": "d6b9cf01-37c7-4fc1-dc6d-3c8aff1a6e16"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>johanna</th>\n",
       "      <td>0.279583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sabrina</th>\n",
       "      <td>0.186388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hayleigh</th>\n",
       "      <td>0.155324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elena</th>\n",
       "      <td>0.139791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gfm</th>\n",
       "      <td>0.116493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jeremy</th>\n",
       "      <td>0.116493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bryan</th>\n",
       "      <td>0.108727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zach</th>\n",
       "      <td>0.108727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timmy</th>\n",
       "      <td>0.100960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elmo</th>\n",
       "      <td>0.093194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bess</th>\n",
       "      <td>0.093194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catalina</th>\n",
       "      <td>0.093194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milo</th>\n",
       "      <td>0.093194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacey</th>\n",
       "      <td>0.093194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alana</th>\n",
       "      <td>0.085428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alia</th>\n",
       "      <td>0.085428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>martina</th>\n",
       "      <td>0.085428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clover</th>\n",
       "      <td>0.085428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elias</th>\n",
       "      <td>0.085428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terry</th>\n",
       "      <td>0.085428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>juni</th>\n",
       "      <td>0.085428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tristan</th>\n",
       "      <td>0.077662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laurie</th>\n",
       "      <td>0.077662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude</th>\n",
       "      <td>0.077662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denise</th>\n",
       "      <td>0.077662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talia</th>\n",
       "      <td>0.077662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skyla</th>\n",
       "      <td>0.077662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roomba</th>\n",
       "      <td>0.069896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evie</th>\n",
       "      <td>0.069896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buford</th>\n",
       "      <td>0.069896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tfidf\n",
       "johanna   0.279583\n",
       "sabrina   0.186388\n",
       "hayleigh  0.155324\n",
       "elena     0.139791\n",
       "gfm       0.116493\n",
       "jeremy    0.116493\n",
       "bryan     0.108727\n",
       "zach      0.108727\n",
       "timmy     0.100960\n",
       "elmo      0.093194\n",
       "bess      0.093194\n",
       "catalina  0.093194\n",
       "milo      0.093194\n",
       "stacey    0.093194\n",
       "alana     0.085428\n",
       "alia      0.085428\n",
       "martina   0.085428\n",
       "clover    0.085428\n",
       "elias     0.085428\n",
       "terry     0.085428\n",
       "juni      0.085428\n",
       "tristan   0.077662\n",
       "laurie    0.077662\n",
       "claude    0.077662\n",
       "denise    0.077662\n",
       "talia     0.077662\n",
       "skyla     0.077662\n",
       "roomba    0.069896\n",
       "evie      0.069896\n",
       "buford    0.069896"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_tfidfvectorizer = tfidf_vectorizer_vectors[0] # Note that 0 refers to our first df (assholes), due to zero-based indexing\n",
    "\n",
    "# place tf-idf values in a DataFrame\n",
    "df = pd.DataFrame(vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the other subset of our data. Do the high- tf-idf scores make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>billy</th>\n",
       "      <td>0.144455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>derek</th>\n",
       "      <td>0.134137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bella</th>\n",
       "      <td>0.121755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thank_awards</th>\n",
       "      <td>0.117628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pam</th>\n",
       "      <td>0.115564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tammy</th>\n",
       "      <td>0.101118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phil</th>\n",
       "      <td>0.099055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>burrito</th>\n",
       "      <td>0.084609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mandy</th>\n",
       "      <td>0.082546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sasha</th>\n",
       "      <td>0.074291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lea</th>\n",
       "      <td>0.074291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nate</th>\n",
       "      <td>0.070164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kaylee</th>\n",
       "      <td>0.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dale</th>\n",
       "      <td>0.066037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nat</th>\n",
       "      <td>0.063973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>larry</th>\n",
       "      <td>0.063973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daphne</th>\n",
       "      <td>0.061909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gary</th>\n",
       "      <td>0.061909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlm</th>\n",
       "      <td>0.059846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peter</th>\n",
       "      <td>0.059846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>natalie</th>\n",
       "      <td>0.059846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carla</th>\n",
       "      <td>0.059846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patty</th>\n",
       "      <td>0.059846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tics</th>\n",
       "      <td>0.057782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rory</th>\n",
       "      <td>0.055718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dylan</th>\n",
       "      <td>0.055718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emergency_fund</th>\n",
       "      <td>0.055718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conner</th>\n",
       "      <td>0.055718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cody</th>\n",
       "      <td>0.053655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keto</th>\n",
       "      <td>0.053655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   tfidf\n",
       "billy           0.144455\n",
       "derek           0.134137\n",
       "bella           0.121755\n",
       "thank_awards    0.117628\n",
       "pam             0.115564\n",
       "tammy           0.101118\n",
       "phil            0.099055\n",
       "burrito         0.084609\n",
       "mandy           0.082546\n",
       "sasha           0.074291\n",
       "lea             0.074291\n",
       "nate            0.070164\n",
       "kaylee          0.068100\n",
       "dale            0.066037\n",
       "nat             0.063973\n",
       "larry           0.063973\n",
       "daphne          0.061909\n",
       "gary            0.061909\n",
       "mlm             0.059846\n",
       "peter           0.059846\n",
       "natalie         0.059846\n",
       "carla           0.059846\n",
       "patty           0.059846\n",
       "tics            0.057782\n",
       "rory            0.055718\n",
       "dylan           0.055718\n",
       "emergency_fund  0.055718\n",
       "conner          0.055718\n",
       "cody            0.053655\n",
       "keto            0.053655"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_tfidfvectorizer = tfidf_vectorizer_vectors[1] # Note that 1 refers to our second df (non-assholes), due to zero-based indexing\n",
    "\n",
    "# place tf-idf values in a DataFrame\n",
    "df = pd.DataFrame(vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YoFv8pXZFXa"
   },
   "source": [
    "## Using TF-IDF to find similar posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjvztHrBZFXb"
   },
   "source": [
    "We can also use TF-IDF to work out the similarity between any pair of documents. So given one post or comment, we could see which posts or comments are most similar. This can be useful if you're trying to find other examples of a pattern you have found and want to explore further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aluiuMKvZFXb"
   },
   "source": [
    "This time, our \"documents\" will not be entire subreddits, but posts/submissions within one subreddit. Let's import the submissions and run the vectorizer without the preprocessing and lemmatizing. Tf-idf will still work this way, and this way, we will be able to read our posts.\n",
    "\n",
    "Let's run TF-IDF over the entire corpus, so each post is compared to all the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "04hxWFEXZFXc"
   },
   "outputs": [],
   "source": [
    "# we could even add trigrams here by adding \"ngram_range=(1,3)\" to params\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', max_df = .70, stop_words = 'english')\n",
    "word_count_vectors = tfidf_vectorizer.fit_transform([post for post in df_ass['selftext']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "px4XE13U4Wwb"
   },
   "source": [
    "We'll start by finding a post with a clear topic. Let's grab an entry in our dataframe. What is this post about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1639779122560,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "6n1IDR9y4Wwc",
    "outputId": "87225c31-bb88-4860-a0bc-46a60573fdb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\'s 6PM on a Friday, store has hundreds of people, there are only 3 registers open and lines are ridiculously long. The self checkout has a line that wraps but its the fastest moving line so we wait about 15 minutes in line to check ourselves out.\\n\\nAfter we check out, a line is forming to exit the building because everyone is waiting for the walmart receipt checker to glance at their receipt.\\n\\nI\\'m already frustrated because of the wait so I skip the line, and the checker anxiously tries to get my attention and loudly says \"SIR I NEED TO CHECK YOUR RECEIPT!\", I respond with a loud(because its loud in the store) \"NO THANK YOU\", and walk out of the building.  People start following my example.\\n\\nThus ensues a 15 minute fight in the car with the wife because she feels I made a scene.\\n\\nEDIT: Well this turned out well.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ass['selftext'][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6l6lqgF4B0L"
   },
   "source": [
    "Let's have a quick look at the tfidf scores for the words in this submission to see if these words are indeed typical for this particular submission. Do the distinctive words have to do with the topic of the post?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1639779140786,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "rpOde4Sr4AzJ",
    "outputId": "8beb14d4-9dab-4266-8f69-4a9ca73259cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>line</th>\n",
       "      <td>0.388256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receipt</th>\n",
       "      <td>0.345678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>checker</th>\n",
       "      <td>0.309109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>0.230729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building</th>\n",
       "      <td>0.194553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loud</th>\n",
       "      <td>0.174266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store</th>\n",
       "      <td>0.158222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anxiously</th>\n",
       "      <td>0.154554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensues</th>\n",
       "      <td>0.154554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wait</th>\n",
       "      <td>0.152997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tfidf\n",
       "line       0.388256\n",
       "receipt    0.345678\n",
       "checker    0.309109\n",
       "check      0.230729\n",
       "building   0.194553\n",
       "loud       0.174266\n",
       "store      0.158222\n",
       "anxiously  0.154554\n",
       "ensues     0.154554\n",
       "wait       0.152997"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a vector out\n",
    "vector_tfidfvectorizer = word_count_vectors[7] # change this number if you want to pick out a different vector / text\n",
    "\n",
    "# place tf-idf values in a pandas data frame\n",
    "df = pd.DataFrame(vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiTzCg4xZFXi"
   },
   "source": [
    "Now let's find the closest posts to this one. The fact that our documents are now in a vector space allows us to make use of mathematical similarity metrics.\n",
    "\n",
    "**Cosine similarity** is one metric used to measure how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space.\n",
    "\n",
    "We can use the heper functoin below to calculate cosine similarities and find the documents that are closest to the selected document. Just add the `word_count_vectors` for the corpus, the index of the document you want to find similar documents to, and the number of similar documents you want to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "Rbfy9VoCEiPc"
   },
   "outputs": [],
   "source": [
    "def find_similar(word_count_vectors, index, top_n = 5):   # you can change the `top_n` parameter if you want to retrieve more similar documents\n",
    "    cosine_similarities = linear_kernel(word_count_vectors[index:index+1], word_count_vectors).flatten()\n",
    "    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
    "    return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuATLPrFZFXe"
   },
   "source": [
    "We can now throw the resulting scores and similar posts in a list, feed that list into a DataFrame, and check out one of them to see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1639779179955,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "6MuqmwKBZFXk",
    "outputId": "71bf05cc-58c7-4d3d-a0dc-e1406e6967d3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cos_score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.182479</td>\n",
       "      <td>I was in the TSA Precheck line where people ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.159239</td>\n",
       "      <td>About 2 weeks ago I went to the drive-thru at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.142716</td>\n",
       "      <td>Went to WalMart recently for batteries. I have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.139136</td>\n",
       "      <td>This happened today and I feel like it was jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.123122</td>\n",
       "      <td>I (38m) stopped by a local grocery store on my...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cos_score                                               text\n",
       "0   0.182479  I was in the TSA Precheck line where people ar...\n",
       "1   0.159239  About 2 weeks ago I went to the drive-thru at ...\n",
       "2   0.142716  Went to WalMart recently for batteries. I have...\n",
       "3   0.139136  This happened today and I feel like it was jus...\n",
       "4   0.123122  I (38m) stopped by a local grocery store on my..."
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine = []\n",
    "for index, score in find_similar(word_count_vectors, 7):\n",
    "    cosine.append(\n",
    "        {'cos_score': score, \n",
    "        'text': df_ass['selftext'][index]\n",
    "        })\n",
    "cosine_df = pd.DataFrame(cosine)\n",
    "cosine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first document's text. Does this post seem comparable to the one we selected above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1639779186306,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "-dEbe-PoIizd",
    "outputId": "012bdd5a-c563-4a73-9806-9ee882836949"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I was in the TSA Precheck line where people are generally cognizant of the regulations. Unfortunately, a woman with her 3 children somehow ended up in this line. I have no patience for people who hold up the TSA screening line. This woman had to fish out all her feeding bottles from her 3 or 4 poorly organized bags and empty them before putting her belongings through security screening. As a result, she held up the security line for a long time. I finally had it and told her she should have been aware of the rules about liquids before getting in line and should have prepared for security screening before getting in line. I suggested that she step aside and let other travelers through and my suggestion was met with a few cheers. \\n\\nShe seemed really embarrassed but didn’t apologize at all. After I passed the screen I mentioned to the TSA agent he should manage the queue better. When a traveler was holding up the line he needed to step in, and he failed. He apologized and I moved on. I don’t think I’m the ass but my co-worker mentioned I could have shown some compassion for the woman who was clearly struggling and clueless.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r16ecNp45gWB"
   },
   "source": [
    "In this notebook we introduced how to calculate TF-IDF scores for a corpus and used it to explore the Reddit data a little further. In this last example, we looked at similar documents. In the next module, we will use **topic modelling** to take this technique one step further by identifying groups of documents with similar themes, based off of the kinds of calculations done in this notebook."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 2 Distant Reading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
