{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bhs6S4deZFWW"
   },
   "source": [
    "Data Science + Social Justice<br>\n",
    "TF-IDF <br>\n",
    "Created by Tom van Nuenen (tom.van_nuenen@kcl.ac.uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"640\"\n",
       "            height=\"360\"\n",
       "            src=\"https://www.youtube.com/embed/Bf5gBRs_xZQ\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7f9d0065a0a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please run this cell for the accompanying video.\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('Bf5gBRs_xZQ', width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJHDHrQPbjif"
   },
   "source": [
    "# TF-IDF\n",
    "\n",
    "This notebook introduces TF-IDF, which allows us to compare different subsets of our subreddit. \n",
    "\n",
    "**After completing this notebook, you will be able to:**\n",
    "1. Understand how tf-idf can be used to compare (subsets of) datasets;\n",
    "2. Find most-distinctive words in a subreddit using tf-idf;\n",
    "3. Find similar posts using tf-idf.\n",
    "\n",
    "There are several basic programming exercises scattered throughout for those who need it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6erwYIhh-cR"
   },
   "source": [
    "## Retrieving the dataset\n",
    "Let's get the data. Make sure you're in the \"Data\" directory when importing by running the magic command `%pwd`.\n",
    "If you're not in the right directory, use `os.chdir` to navigate there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tomvannuenen/Downloads/DIGHUM160/Notebooks/Week 2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22903,
     "status": "ok",
     "timestamp": 1647477678422,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "qqV-FJC9aHuT",
    "outputId": "9126a240-03b0-4403-eda9-afb71f673262"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# We include two ../ because we want to go two levels up in the file structure\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../Data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../Data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# We include two ../ because we want to go two levels up in the file structure\n",
    "os.chdir('../../Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKZ6v_EzpUCi"
   },
   "source": [
    "Importing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 5173,
     "status": "ok",
     "timestamp": 1647477683593,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "WBLyQUt1qZRG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('aita_sub_top_sm_lemmas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idint</th>\n",
       "      <th>idstr</th>\n",
       "      <th>created</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>score</th>\n",
       "      <th>distinguish</th>\n",
       "      <th>textlen</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>flair_css_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>427576402</td>\n",
       "      <td>t3_72kg2a</td>\n",
       "      <td>1506433689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ritsku</td>\n",
       "      <td>AITA for breaking up with my girlfriend becaus...</td>\n",
       "      <td>My girlfriend recently went to the beach with ...</td>\n",
       "      <td>girlfriend recently beach friend tiny bikini b...</td>\n",
       "      <td>679.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4917.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>no a--holes here</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>551887974</td>\n",
       "      <td>t3_94kvhi</td>\n",
       "      <td>1533404095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hhhhhhffff678</td>\n",
       "      <td>AITA for banning smoking in my house and telli...</td>\n",
       "      <td>My parents smoke like chimneys. I used to as w...</td>\n",
       "      <td>parent smoke like chimney use quit wife young ...</td>\n",
       "      <td>832.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2076.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>asshole</td>\n",
       "      <td>ass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552654542</td>\n",
       "      <td>t3_951az2</td>\n",
       "      <td>1533562299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>creepatthepool</td>\n",
       "      <td>AITA? Creep wears skimpy bathing suit to pool</td>\n",
       "      <td>Hi guys. Throwaway for obv reasons.\\n\\nI'm a f...</td>\n",
       "      <td>hi guy throwaway obv reason i'm female child b...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1741.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>Shitpost</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idint      idstr     created  nsfw          author  \\\n",
       "0  427576402  t3_72kg2a  1506433689   0.0          Ritsku   \n",
       "1  551887974  t3_94kvhi  1533404095   0.0   hhhhhhffff678   \n",
       "2  552654542  t3_951az2  1533562299   0.0  creepatthepool   \n",
       "\n",
       "                                               title  \\\n",
       "0  AITA for breaking up with my girlfriend becaus...   \n",
       "1  AITA for banning smoking in my house and telli...   \n",
       "2      AITA? Creep wears skimpy bathing suit to pool   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  My girlfriend recently went to the beach with ...   \n",
       "1  My parents smoke like chimneys. I used to as w...   \n",
       "2  Hi guys. Throwaway for obv reasons.\\n\\nI'm a f...   \n",
       "\n",
       "                                              lemmas  score distinguish  \\\n",
       "0  girlfriend recently beach friend tiny bikini b...  679.0         NaN   \n",
       "1  parent smoke like chimney use quit wife young ...  832.0         NaN   \n",
       "2  hi guy throwaway obv reason i'm female child b...   23.0         NaN   \n",
       "\n",
       "   textlen  num_comments        flair_text flair_css_class  \n",
       "0   4917.0         434.0  no a--holes here             NaN  \n",
       "1   2076.0         357.0           asshole             ass  \n",
       "2   1741.0         335.0          Shitpost             NaN  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpWynSoy4t8o"
   },
   "source": [
    "## Implementing TF_IDF\n",
    "TF-IDF, short for *term frequency–inverse document frequency*, is a numerical statistic that reflects how important a word is to a document in a collection or corpus.\n",
    "TF-IDF is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The TF-IDF value increases proportionally to the number of times a word appears in the document (the term frequency, or TF), and is offset by the number of documents in the corpus that contain the word (the inverse document frequency, or IDF). This helps to adjust for the fact that some words appear more frequently in general – such as articles and prepositions.\n",
    "\n",
    "Terms with high tfidf values for a given document are generally the most descriptive of that document. If a word occurs many times in one post but rarely in the rest of the corpus, it is probably useful for characterizing that post; conversely, if a word occurs frequently in a post but also occurs frequently in the corpus, it is probably less characteristic of that post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1639773021096,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "GLxYUFszEA0M",
    "outputId": "ff40b717-b7f4-4710-bd43-a0c7613422f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 0, 2, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 2, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "  'My cat has paws.',\n",
    "  'Can we let the dog out?',\n",
    "  'Our dog really likes the cat but the cat does not agree.']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "#vectorizer.get_feature_names_out()\n",
    "X.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtaTTaP8EA0M"
   },
   "source": [
    "Each column in the matrix represents a unique word in the vocabulary, while each row represents the document in our dataset. In this case, we only have one book title (i.e. the document), and therefore we have only 1 row. The values in each cell are the word counts. Note that with this representation, counts of some words could be 0 if the word did not appear in the corresponding document.\n",
    "\n",
    "Let's show the resulting vocabulary. Note that the numbers are not counts, they are the position in the sparse vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1639773033313,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "tAERnKWKEA0M",
    "outputId": "b9453764-4477-46c8-a3bc-c08a258c0b9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 9,\n",
       " 'cat': 3,\n",
       " 'has': 6,\n",
       " 'paws': 13,\n",
       " 'can': 2,\n",
       " 'we': 16,\n",
       " 'let': 7,\n",
       " 'the': 15,\n",
       " 'dog': 5,\n",
       " 'out': 12,\n",
       " 'our': 11,\n",
       " 'really': 14,\n",
       " 'like': 8,\n",
       " 'but': 1,\n",
       " 'does': 4,\n",
       " 'not': 10,\n",
       " 'agree': 0}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsNok5KyEA0M"
   },
   "source": [
    "Lets see how many docs and unique words we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1639773036420,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "6mKvaLCnEA0M",
    "outputId": "259f70fb-d186-4330-d9a7-d726459105fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 17)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6kPKfAnFvKY"
   },
   "source": [
    "Sparse matrices are the simplest way to represent texts. However, it biases most frequent words and ends up ignoring rare words which could have helped is in processing our data more efficiently.\n",
    "\n",
    "Oftentimes we not only want to focus on the frequency of words present in the corpus but also want to know the importance of the words. This is where tf-idf  (term frequency-inverse document frequency) comes in. Tf-idf is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "This is done by multiplying two metrics: how many times a word appears in a document, and the *inverse document frequency* of the word across a set of documents. For the latter, we divide the total number of documents by the number of documents containing the term, and then take the logarithm of that quotient. This tells us if a word is common or rare across all documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvxZ-5f6ZFWe"
   },
   "source": [
    "### Testing tf-idf with a toy dataset\n",
    "\n",
    "Let's try tf-idf out with a toy dataset. Here we have three documents about Python, but with different meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MjjWn-phZFWf"
   },
   "outputs": [],
   "source": [
    "document1 = \"\"\"Python is a 2000 made-for-TV horror movie directed by Richard\n",
    "Clabaugh. The film features several cult favorite actors, including William\n",
    "Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy,\n",
    "Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the\n",
    "A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean\n",
    "Whalen.\"\"\"\n",
    "\n",
    "document2 = \"\"\"Python, from the Greek word (πύθων/πύθωνας), is a genus of\n",
    "nonvenomous pythons[2] found in Africa and Asia. Currently, 7 species are\n",
    "recognised.[2] A member of this genus, P. reticulatus, is among the longest\n",
    "snakes known.\"\"\"\n",
    "\n",
    "document3 = \"\"\"Monty Python (also collectively known as the Pythons) are a British \n",
    "surreal comedy group who created the sketch comedy television show Monty Python's \n",
    "Flying Circus, which first aired on the BBC in 1969. Forty-five episodes were made \n",
    "over four series.\"\"\"\n",
    "\n",
    "document4 = \"\"\"Python is an interpreted, high-level, general-purpose programming language. \n",
    "Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes \n",
    "code readability with its notable use of significant whitespace. Its language constructs and \n",
    "object-oriented approach aim to help programmers write clear, logical code for small and \n",
    "large-scale projects.\"\"\"\n",
    "\n",
    "document5 = \"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
    "manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
    "It is sometimes referred to as a \"Combat Magnum\". It was first introduced\n",
    "in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
    "Colt Python targeted the premium revolver market segment.\"\"\"\n",
    "\n",
    "document6 = \"\"\"The Pythonidae, commonly known simply as pythons, from the Greek word python \n",
    "(πυθων), are a family of nonvenomous snakes found in Africa, Asia, and Australia. \n",
    "Among its members are some of the largest snakes in the world. Eight genera and 31\n",
    "species are currently recognized.\"\"\"\n",
    "\n",
    "test_list = [document1, document2, document3, document4, document5, document6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESR7eMThTkZE"
   },
   "source": [
    "We're running `CountVectorizer()` again, only this time we add some parameters. Using `max_df` we can get rid of words that appear in more than 85% of the corpus, and using `stop_words` we can insert an English stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1639776660955,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "Os8jrh3RM_GJ",
    "outputId": "b7adadfc-b6fa-4752-b8c1-08527bd07703"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 3, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_df=0.85, stop_words='english')\n",
    "word_count_vector = cv.fit_transform(test_list)\n",
    "word_count_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1639776793790,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "dUZve8daNHRc",
    "outputId": "5dc8054a-6557-4cf0-b6bf-248870b2345a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2000': 3,\n",
       " 'tv': 130,\n",
       " 'horror': 65,\n",
       " 'movie': 90,\n",
       " 'directed': 42,\n",
       " 'richard': 111,\n",
       " 'clabaugh': 23,\n",
       " 'film': 52,\n",
       " 'features': 51,\n",
       " 'cult': 36,\n",
       " 'favorite': 50,\n",
       " 'actors': 7,\n",
       " 'including': 66,\n",
       " 'william': 138,\n",
       " 'zabka': 143,\n",
       " 'karate': 70,\n",
       " 'kid': 72,\n",
       " 'fame': 48,\n",
       " 'wil': 137,\n",
       " 'wheaton': 135,\n",
       " 'casper': 21,\n",
       " 'van': 132,\n",
       " 'dien': 41,\n",
       " 'jenny': 69,\n",
       " 'mccarthy': 86,\n",
       " 'keith': 71,\n",
       " 'coogan': 34,\n",
       " 'robert': 112,\n",
       " 'englund': 46,\n",
       " 'best': 17,\n",
       " 'known': 73,\n",
       " 'role': 113,\n",
       " 'freddy': 55,\n",
       " 'krueger': 74,\n",
       " 'nightmare': 91,\n",
       " 'elm': 44,\n",
       " 'street': 126,\n",
       " 'series': 118,\n",
       " 'films': 53,\n",
       " 'dana': 38,\n",
       " 'barron': 15,\n",
       " 'david': 39,\n",
       " 'bowe': 18,\n",
       " 'sean': 116,\n",
       " 'whalen': 134,\n",
       " 'greek': 59,\n",
       " 'word': 139,\n",
       " 'πύθων': 145,\n",
       " 'πύθωνας': 146,\n",
       " 'genus': 58,\n",
       " 'nonvenomous': 92,\n",
       " 'pythons': 103,\n",
       " 'africa': 8,\n",
       " 'asia': 13,\n",
       " 'currently': 37,\n",
       " 'species': 125,\n",
       " 'recognised': 105,\n",
       " 'member': 87,\n",
       " 'reticulatus': 109,\n",
       " 'longest': 80,\n",
       " 'snakes': 124,\n",
       " 'monty': 89,\n",
       " 'collectively': 26,\n",
       " 'british': 19,\n",
       " 'surreal': 127,\n",
       " 'comedy': 29,\n",
       " 'group': 60,\n",
       " 'created': 35,\n",
       " 'sketch': 121,\n",
       " 'television': 129,\n",
       " 'flying': 54,\n",
       " 'circus': 22,\n",
       " 'aired': 10,\n",
       " 'bbc': 16,\n",
       " '1969': 1,\n",
       " 'episodes': 47,\n",
       " 'interpreted': 67,\n",
       " 'high': 64,\n",
       " 'level': 78,\n",
       " 'general': 57,\n",
       " 'purpose': 101,\n",
       " 'programming': 99,\n",
       " 'language': 75,\n",
       " 'guido': 61,\n",
       " 'rossum': 114,\n",
       " 'released': 108,\n",
       " '1991': 2,\n",
       " 'design': 40,\n",
       " 'philosophy': 96,\n",
       " 'emphasizes': 45,\n",
       " 'code': 25,\n",
       " 'readability': 104,\n",
       " 'notable': 93,\n",
       " 'use': 131,\n",
       " 'significant': 119,\n",
       " 'whitespace': 136,\n",
       " 'constructs': 33,\n",
       " 'object': 94,\n",
       " 'oriented': 95,\n",
       " 'approach': 12,\n",
       " 'aim': 9,\n",
       " 'help': 63,\n",
       " 'programmers': 98,\n",
       " 'write': 141,\n",
       " 'clear': 24,\n",
       " 'logical': 79,\n",
       " 'small': 122,\n",
       " 'large': 76,\n",
       " 'scale': 115,\n",
       " 'projects': 100,\n",
       " 'colt': 27,\n",
       " '357': 5,\n",
       " 'magnum': 82,\n",
       " 'caliber': 20,\n",
       " 'revolver': 110,\n",
       " 'manufactured': 83,\n",
       " 'manufacturing': 84,\n",
       " 'company': 31,\n",
       " 'hartford': 62,\n",
       " 'connecticut': 32,\n",
       " 'referred': 107,\n",
       " 'combat': 28,\n",
       " 'introduced': 68,\n",
       " '1955': 0,\n",
       " 'year': 142,\n",
       " 'smith': 123,\n",
       " 'amp': 11,\n",
       " 'wesson': 133,\n",
       " 'm29': 81,\n",
       " '44': 6,\n",
       " 'discontinued': 43,\n",
       " 'targeted': 128,\n",
       " 'premium': 97,\n",
       " 'market': 85,\n",
       " 'segment': 117,\n",
       " 'pythonidae': 102,\n",
       " 'commonly': 30,\n",
       " 'simply': 120,\n",
       " 'πυθων': 144,\n",
       " 'family': 49,\n",
       " 'australia': 14,\n",
       " 'members': 88,\n",
       " 'largest': 77,\n",
       " 'world': 140,\n",
       " 'genera': 56,\n",
       " '31': 4,\n",
       " 'recognized': 106}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkpVqpPzNHRd"
   },
   "source": [
    "How many docs and unique words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1639776805351,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "cD1ru07WNHRd",
    "outputId": "f31955ed-c788-41dc-b651-b2a22d57296d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 147)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVLHjD29ZFWi"
   },
   "source": [
    "## Using `TfidfTransformer`\n",
    "\n",
    "Next, we need to compute the idf values. We'll call `tfidf_transformer.fit(word_count_vector)` on the word counts we computed earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1639776806708,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "WeTAynM9xxxr",
    "outputId": "a5c7fd51-b4cf-4c16-e643-388303891e01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer() \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Uoj5xOi8vu_"
   },
   "source": [
    "To get a glimpse of how the IDF values look, we are going to print it by placing the idf values in a python DataFrame. The values will be sorted in ascending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1639776807884,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "gRjP38dV-7I7",
    "outputId": "ba06ac97-0405-47f1-d01d-db1bbf5ca801"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomvannuenen/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>known</th>\n",
       "      <td>1.336472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pythons</th>\n",
       "      <td>1.559616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greek</th>\n",
       "      <td>1.847298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nonvenomous</th>\n",
       "      <td>1.847298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created</th>\n",
       "      <td>1.847298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>2.252763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>films</th>\n",
       "      <td>2.252763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flying</th>\n",
       "      <td>2.252763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fame</th>\n",
       "      <td>2.252763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>πύθωνας</th>\n",
       "      <td>2.252763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             idf_weights\n",
       "known           1.336472\n",
       "pythons         1.559616\n",
       "greek           1.847298\n",
       "nonvenomous     1.847298\n",
       "created         1.847298\n",
       "...                  ...\n",
       "film            2.252763\n",
       "films           2.252763\n",
       "flying          2.252763\n",
       "fame            2.252763\n",
       "πύθωνας         2.252763\n",
       "\n",
       "[147 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(), columns=[\"idf_weights\"]) \n",
    " \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFeXhx71O_4n"
   },
   "source": [
    "Notice that the words \"python\" and \"in\" have the lowest idf values. This is expected: these words appear in each and every document in our collection. The lower the idf value of a word, the less unique it is to any particular document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGuoetn0ASpf"
   },
   "source": [
    "Now that we have the idf values, we can compute the tf-idf scores for our set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hWPqOTR_AIM0"
   },
   "outputs": [],
   "source": [
    "tf_idf_vector=tfidf_transformer.transform(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0-6H9L9PnLA"
   },
   "source": [
    "By invoking `tfidf_transformer.transform()` we are computing the tf-idf scores for our docs. Internally this is computing the tf * idf  multiplication where our term frequencies (in the `word_count_vector` array) are weighted by their IDF values.\n",
    "\n",
    "Let’s print the tf-idf values of the first document to see if it makes sense. What we are doing below is, placing the tf-idf scores from the third document into a pandas data frame and sorting it in descending order of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1639776812749,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "Qu40IZhpP0b2",
    "outputId": "fbf33f79-7be4-4c0e-8853-02ab6b7cf616"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>comedy</th>\n",
       "      <td>0.424705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>monty</th>\n",
       "      <td>0.424705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>circus</th>\n",
       "      <td>0.212353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>television</th>\n",
       "      <td>0.212353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surreal</th>\n",
       "      <td>0.212353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>englund</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emphasizes</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elm</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discontinued</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>πύθωνας</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tfidf\n",
       "comedy        0.424705\n",
       "monty         0.424705\n",
       "circus        0.212353\n",
       "television    0.212353\n",
       "surreal       0.212353\n",
       "...                ...\n",
       "englund       0.000000\n",
       "emphasizes    0.000000\n",
       "elm           0.000000\n",
       "discontinued  0.000000\n",
       "πύθωνας       0.000000\n",
       "\n",
       "[147 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names_out() \n",
    "  \n",
    "#print the scores \n",
    "df = pd.DataFrame(tf_idf_vector[2].T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9qVYEm_ZFW2"
   },
   "source": [
    "Through tf-idf, we have found the words that are most-distinctive of `document3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq3QvXEcZFXS"
   },
   "source": [
    "## Using tf-idf on Reddit datasets\n",
    "Time to perform tf-idf on Reddit. We could do this on a document level (more on that later), but also on a higher level. For instance, we might be interested in the words with high TF-IDF counts when comparing two subsets of a subreddit. First, let's create two dataframes to seperate the data based on whether they were classified as \"assholeish\" and \"non-assholeish\" posts by the communities. We'll put the relevant lemmatized texts into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ass = df.loc[df.flair_css_class == \"ass\"]\n",
    "df_ass.reset_index(inplace=True)\n",
    "df_not = df.loc[df.flair_css_class == \"not\"]\n",
    "df_not.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "aSQHlQliZFXV"
   },
   "outputs": [],
   "source": [
    "aita_list = [' '.join(df_ass['lemmas']), ' '.join(df_not['lemmas'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgKvuY96ZFXY"
   },
   "source": [
    "This time, to save time, we will be using Scikit-LEARN `TfidfVectorizer`. It is a class that basically allows us to create a matrix of word counts (what we just did with `CountVectorizer`), and immediately transform them into tf-idf values. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) for the documentation.\n",
    "\n",
    "We simply instantiate an object of the `TfidfVectorizer`. Then, we run it by applying the `fit_transform()` method to our \"aita_list\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "1vRKbLpDZFXY"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# settings that you use for count vectorizer will go here\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, decode_error='ignore', stop_words='english',smooth_idf=True,use_idf=True)\n",
    "\n",
    "# fit and transform the texts\n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(aita_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our first df (assholes), and see which words are typical of those posts when compared to the posts which are by non-assholes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1639778604305,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "cghW5hxxZ8Ec",
    "outputId": "d6b9cf01-37c7-4fc1-dc6d-3c8aff1a6e16"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>johanna</th>\n",
       "      <td>0.287494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sabrina</th>\n",
       "      <td>0.191663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hayleigh</th>\n",
       "      <td>0.159719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mittens</th>\n",
       "      <td>0.151733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elena</th>\n",
       "      <td>0.143747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jeremy</th>\n",
       "      <td>0.119789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gfm</th>\n",
       "      <td>0.119789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zach</th>\n",
       "      <td>0.111803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bryan</th>\n",
       "      <td>0.111803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timmy</th>\n",
       "      <td>0.103817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacey</th>\n",
       "      <td>0.095831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catalina</th>\n",
       "      <td>0.095831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bess</th>\n",
       "      <td>0.095831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elmo</th>\n",
       "      <td>0.095831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milo</th>\n",
       "      <td>0.095831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terry</th>\n",
       "      <td>0.087846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alana</th>\n",
       "      <td>0.087846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>martina</th>\n",
       "      <td>0.087846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>juni</th>\n",
       "      <td>0.087846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alia</th>\n",
       "      <td>0.087846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clover</th>\n",
       "      <td>0.087846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skyla</th>\n",
       "      <td>0.079860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denise</th>\n",
       "      <td>0.079860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tristan</th>\n",
       "      <td>0.079860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laurie</th>\n",
       "      <td>0.079860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude</th>\n",
       "      <td>0.079860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talia</th>\n",
       "      <td>0.079860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evie</th>\n",
       "      <td>0.071874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roomba</th>\n",
       "      <td>0.071874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kimchi</th>\n",
       "      <td>0.071874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tfidf\n",
       "johanna   0.287494\n",
       "sabrina   0.191663\n",
       "hayleigh  0.159719\n",
       "mittens   0.151733\n",
       "elena     0.143747\n",
       "jeremy    0.119789\n",
       "gfm       0.119789\n",
       "zach      0.111803\n",
       "bryan     0.111803\n",
       "timmy     0.103817\n",
       "stacey    0.095831\n",
       "catalina  0.095831\n",
       "bess      0.095831\n",
       "elmo      0.095831\n",
       "milo      0.095831\n",
       "terry     0.087846\n",
       "alana     0.087846\n",
       "martina   0.087846\n",
       "juni      0.087846\n",
       "alia      0.087846\n",
       "clover    0.087846\n",
       "skyla     0.079860\n",
       "denise    0.079860\n",
       "tristan   0.079860\n",
       "laurie    0.079860\n",
       "claude    0.079860\n",
       "talia     0.079860\n",
       "evie      0.071874\n",
       "roomba    0.071874\n",
       "kimchi    0.071874"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_tfidfvectorizer = tfidf_vectorizer_vectors[0] # Note that 0 refers to our first df (assholes), due to zero-based indexing\n",
    "\n",
    "# place tf-idf values in a DataFrame\n",
    "df = pd.DataFrame(vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names_out(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)[:30]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YoFv8pXZFXa"
   },
   "source": [
    "# 5. Using TF-IDF to find similar posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjvztHrBZFXb"
   },
   "source": [
    "We can also use tf-idf to work out the similarity between any pair of documents. So given one post or comment, we could see which posts or comments are most similar. This can be useful if you're trying to find other examples of a pattern you have found and want to explore further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aluiuMKvZFXb"
   },
   "source": [
    "This time, our \"documents\" will not be entire subreddits, but posts/submissions within one subreddit. Let's import the submissions and run the vectorizer without the preprocessing and lemmatizing. Tf-idf will still work this way, and this way, we will be able to read our posts.\n",
    "\n",
    "Let's run TF-IDF over the entire corpus, so each post is compared to all the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "04hxWFEXZFXc"
   },
   "outputs": [],
   "source": [
    "# we could even add trigrams here by adding \"ngram_range=(1,3)\" to params\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', max_df = .70, stop_words = 'english')\n",
    "word_count_vectors = tfidf_vectorizer.fit_transform([post for post in df_ass['selftext']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "px4XE13U4Wwb"
   },
   "source": [
    "We'll start by finding a post with a clear topic. Let's grab an entry in our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1639779122560,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "6n1IDR9y4Wwc",
    "outputId": "87225c31-bb88-4860-a0bc-46a60573fdb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\'s 6PM on a Friday, store has hundreds of people, there are only 3 registers open and lines are ridiculously long. The self checkout has a line that wraps but its the fastest moving line so we wait about 15 minutes in line to check ourselves out.\\n\\nAfter we check out, a line is forming to exit the building because everyone is waiting for the walmart receipt checker to glance at their receipt.\\n\\nI\\'m already frustrated because of the wait so I skip the line, and the checker anxiously tries to get my attention and loudly says \"SIR I NEED TO CHECK YOUR RECEIPT!\", I respond with a loud(because its loud in the store) \"NO THANK YOU\", and walk out of the building.  People start following my example.\\n\\nThus ensues a 15 minute fight in the car with the wife because she feels I made a scene.\\n\\nEDIT: Well this turned out well.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ass['selftext'][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6l6lqgF4B0L"
   },
   "source": [
    "Let's have a quick look at the tfidf scores for the words in this submission to see if these words are indeed typical for this particular submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1639779140786,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "rpOde4Sr4AzJ",
    "outputId": "8beb14d4-9dab-4266-8f69-4a9ca73259cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>line</th>\n",
       "      <td>0.388256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receipt</th>\n",
       "      <td>0.345678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>checker</th>\n",
       "      <td>0.309109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>check</th>\n",
       "      <td>0.230729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building</th>\n",
       "      <td>0.194553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loud</th>\n",
       "      <td>0.174266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store</th>\n",
       "      <td>0.158222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anxiously</th>\n",
       "      <td>0.154554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ensues</th>\n",
       "      <td>0.154554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wait</th>\n",
       "      <td>0.152997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tfidf\n",
       "line       0.388256\n",
       "receipt    0.345678\n",
       "checker    0.309109\n",
       "check      0.230729\n",
       "building   0.194553\n",
       "loud       0.174266\n",
       "store      0.158222\n",
       "anxiously  0.154554\n",
       "ensues     0.154554\n",
       "wait       0.152997"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a vector out\n",
    "vector_tfidfvectorizer = word_count_vectors[7] # change this number if you want to pick out a different vector / text\n",
    "\n",
    "# place tf-idf values in a pandas data frame\n",
    "df = pd.DataFrame(vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names_out(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiTzCg4xZFXi"
   },
   "source": [
    "Now let's find the top document(s). \n",
    "The fact that our documents are now in a vector space is convenient: it allows us to make use of mathematical similarity metrics.\n",
    "\n",
    "**Cosine similarity** is one metric used to measure how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. Cosine similarity compensates for the effect of the different lengths of our subreddits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Rbfy9VoCEiPc"
   },
   "outputs": [],
   "source": [
    "def find_similar(word_count_vectors, index, top_n = 5):   # you can change the `top_n` parameter if you want to retrieve more similar documents\n",
    "    cosine_similarities = linear_kernel(word_count_vectors[index:index+1], word_count_vectors).flatten()\n",
    "    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
    "    return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuATLPrFZFXe"
   },
   "source": [
    "The above function finds similar words. It uses scikit-LEARN's `linear kernel`, which uses cosine similarity to find documents that are most alike.\n",
    "\n",
    "We can now throw the resulting scores and similar posts in a list, feed that list into a DataFrame, and check out one of them to see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1639779179955,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "6MuqmwKBZFXk",
    "outputId": "71bf05cc-58c7-4d3d-a0dc-e1406e6967d3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cos_score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.182479</td>\n",
       "      <td>I was in the TSA Precheck line where people ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.159239</td>\n",
       "      <td>About 2 weeks ago I went to the drive-thru at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.142716</td>\n",
       "      <td>Went to WalMart recently for batteries. I have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.139136</td>\n",
       "      <td>This happened today and I feel like it was jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.123122</td>\n",
       "      <td>I (38m) stopped by a local grocery store on my...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cos_score                                               text\n",
       "0   0.182479  I was in the TSA Precheck line where people ar...\n",
       "1   0.159239  About 2 weeks ago I went to the drive-thru at ...\n",
       "2   0.142716  Went to WalMart recently for batteries. I have...\n",
       "3   0.139136  This happened today and I feel like it was jus...\n",
       "4   0.123122  I (38m) stopped by a local grocery store on my..."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine = []\n",
    "for index, score in find_similar(word_count_vectors, 7):\n",
    "    cosine.append(\n",
    "        {'cos_score': score, \n",
    "        'text': df_ass['selftext'][index]\n",
    "        })\n",
    "cosine_df = pd.DataFrame(cosine)\n",
    "cosine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1639779186306,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "-dEbe-PoIizd",
    "outputId": "012bdd5a-c563-4a73-9806-9ee882836949"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I was in the TSA Precheck line where people are generally cognizant of the regulations. Unfortunately, a woman with her 3 children somehow ended up in this line. I have no patience for people who hold up the TSA screening line. This woman had to fish out all her feeding bottles from her 3 or 4 poorly organized bags and empty them before putting her belongings through security screening. As a result, she held up the security line for a long time. I finally had it and told her she should have been aware of the rules about liquids before getting in line and should have prepared for security screening before getting in line. I suggested that she step aside and let other travelers through and my suggestion was met with a few cheers. \\n\\nShe seemed really embarrassed but didn’t apologize at all. After I passed the screen I mentioned to the TSA agent he should manage the queue better. When a traveler was holding up the line he needed to step in, and he failed. He apologized and I moved on. I don’t think I’m the ass but my co-worker mentioned I could have shown some compassion for the woman who was clearly struggling and clueless.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r16ecNp45gWB"
   },
   "source": [
    "This post does seem comparable!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 2 Distant Reading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
