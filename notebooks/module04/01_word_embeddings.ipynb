{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Uds2RtAZOog"
   },
   "source": [
    "# Data Science for Social Justice Workshop: Module 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfjNP33MZOoi"
   },
   "source": [
    "## Word Embeddings\n",
    "\n",
    "In this notebook, we'll work with word embeddings using [`gensim`](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "The goal of word embedding models is to learn **numerical representations** of text corpora. We already did that to a certain extent when we did topic modeling. In this case, we're going to be more explicit about how we construct that numerical representation: for each word, we're going to find a **vector** of numbers to represent it. The actual numbers themselves won't be meaningful to us as humans. However, if successful, the vectors for each term should encode information about the meaning or concept the term represents, as well as the relationship between it and other terms in the vocabulary.\n",
    "\n",
    "Word vector models are fully **unsupervised**: they learn all of these meanings and relationships without any advance knowledge. Unsupervised learning requires the specification of a right task. We won't go into detail in this lesson, but you can roughly think of the task as predicting nearby words, given a specific word. Read [this post](https://tomvannuenen.medium.com/analyzing-reddit-communities-with-python-part-6-word-embeddings-f92bba876d60) for a deeper introduction to word embeddings.\n",
    "\n",
    "This notebook is designed to help you:\n",
    "\n",
    "* Use `gensim`'s [`word2vec`](https://radimrehurek.com/gensim/models/word2vec.html) method to create word vectors for a corpus;\n",
    "* Use these word vectors to reflect on implicit binaries and normativities in your data;\n",
    "* Visualize topic models using K-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment and run the code in this cell if you are running this notebook on DataHub or Binder\n",
    "#!pip install spacy\n",
    "#!pip install gensim\n",
    "#!pip install pandas\n",
    "#!pip install -U scikit-learn \n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "As we will be considering the language biases in the next notebook, we will use the comments of the AITA subreddit this time. The thinking behind this is that this data will be derived from more people, and include more evaluative statements (after all, comments on r/amitheasshole generally evaluate the original posts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check sklearn version\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your sklearn version < 1.0, run the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory\n",
    "# We include two ../ because we want to go two levels up in the file structure\n",
    "os.chdir(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5723,
     "status": "ok",
     "timestamp": 1639942437840,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "bIB1SsShP9iL",
    "outputId": "01fa486d-4a39-48ef-f6f1-076478c6e0fc"
   },
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "df = pd.read_csv('aita_sub_top_sm.csv')\n",
    "df.head(3)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remove comments that were removed or deleted, and additionally only take comments that are sufficiently long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1639942442529,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "B2igBxlIu7Hw",
    "outputId": "806927f1-bd1e-4517-ec05-a45773bb92d8"
   },
   "outputs": [],
   "source": [
    "# Remove comments that are [removed] or [deleted]\n",
    "df = df[~df['selftext'].isin(['[removed]', '[deleted]'])].dropna(subset=['selftext'])\n",
    "# Remove comments less than 15 characters long\n",
    "df = df[df['selftext'].str.len() >= 15]\n",
    "len(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll import `spacy` and `gensim` to do some preprocessing. We have functions written here for you to help streamline the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wt4BIM-cfhK3"
   },
   "outputs": [],
   "source": [
    "def clean(token):\n",
    "    \"\"\"Helper function that specifies whether a token is:\n",
    "        - punctuation\n",
    "        - space\n",
    "        - digit\n",
    "    \"\"\"\n",
    "    return token.is_punct or token.is_space or token.is_digit\n",
    "\n",
    "def line_read(df, text_col='selftext'):\n",
    "    \"\"\"Generator function to read in text from df and get rid of line breaks.\"\"\"    \n",
    "    for text in df[text_col]:\n",
    "        yield text.replace('\\n', '')\n",
    "\n",
    "def preprocess(df, text_col='selftext', allowed_postags=['NOUN', 'ADJ']):\n",
    "    \"\"\"Preprocessing function to apply to a dataframe.\"\"\"\n",
    "    for parsed in nlp.pipe(line_read(df, text_col), batch_size=1000, disable=[\"tok2vec\", \"ner\"]):\n",
    "        # Gather lowercased, lemmatized tokens\n",
    "        tokens = [token.lemma_.lower() if token.lemma_ != '-PRON-'\n",
    "                  else token.lower_ \n",
    "                  for token in parsed if not clean(token)]\n",
    "        # Remove specific lemmatizations, and words that are not nouns or adjectives\n",
    "        tokens = [lemma\n",
    "                  for lemma in tokens\n",
    "                  if not lemma in [\"'s\",  \"’s\", \"’\"] and not lemma in allowed_postags]\n",
    "        # Remove stop words\n",
    "        tokens = [token for token in tokens if token not in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "        yield tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the `preprocess()` function to each comment in the dataframe, producing a `docs` output. This call might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHOu2vdsfl3Q"
   },
   "outputs": [],
   "source": [
    "docs = [line for line in preprocess(df, text_col='selftext')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create bigrams. Bigrams consist of pairs of words that appear commonly together (e.g., \"New York\"). `gensim` provides some functions to detect bigrams by finding words appear often enough together that we should include them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 126716,
     "status": "ok",
     "timestamp": 1639949141836,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "C4RT1pLBfgaz",
    "outputId": "ae080bf7-fd7b-40a5-df98-79f35ed717aa"
   },
   "outputs": [],
   "source": [
    "# Create bigram model: pass docs into Phrases class\n",
    "bigrams = Phrases(docs, min_count=20, threshold=300)\n",
    "# Create a \"frozen\" bigram model using the Phraser class\n",
    "bigram_phraser = Phraser(bigrams)\n",
    "# Now, create bigrams \n",
    "docs_bigrams = [bigram_phraser[doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's nothing stopping us from going further: we can create trigrams or even $n$-grams. We'll make some trigrams and build our word2vec model on top of them. A trigram can be constructed by simply looking for bigrams in a bigrams corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = Phrases(bigrams[docs], min_count=20, threshold=100)  \n",
    "trigram_phraser = Phraser(trigrams)\n",
    "docs_trigrams = [trigram_phraser[doc] for doc in docs_bigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the data to an external JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GS_5VJXogUMn"
   },
   "outputs": [],
   "source": [
    "with open('aita_com_top_lemmas.json', 'w') as write:\n",
    "    json.dump(docs_trigrams, write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the same file works as follows:\n",
    "with open(\"aita_com_top_lemmas.json\") as f:\n",
    "    trigrams = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wk2mxX1HZOpk"
   },
   "source": [
    "## Constructing a Word2Vec Model\n",
    "\n",
    "Let's create our word embeddings model. \n",
    "\n",
    "While last week's LDA method was focused on finding topics in a collection of documents (or in our case, submissions), word embeddings models focus on individual words, and learning vector representations of these words.\n",
    "\n",
    "The input to the model is a text corpus split up into sentences – in word embeddings, there is no concept of \"documents\". The model's output is a set of \"vectors\" (one for each word) in N dimensions (a parameter of the model). Think of these vectors as \"features\", capturing latent meaning.\n",
    "\n",
    "This model allows us to group the vectors of similar words together in vector space. We can then reduce the dimensionality to visualize the results in a way humans can understand (such as in a 2-dimensional space), or to perform linear algebra operations in order to find out to what extent words are related.\n",
    "\n",
    "Word2Vec is one example of a word embeddings model. It learns by taking words and their contexts (e.g. sentences) into account, and can then try to predict other words. Given enough data, usage and contexts, word2vec can make accurate guesses about a word’s meaning based on its appearances. Those guesses can be used to establish a word’s association with other words (e.g. \"Paris\" is to \"France\" as “Berlin” is to “Germany”), or cluster documents and classify them by topic.\n",
    "\n",
    "We now instantiate and train our Word2Vec model, using the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQq117-pZOpl"
   },
   "outputs": [],
   "source": [
    "# Count the number of cores you have at your disposal\n",
    "cores = multiprocessing.cpu_count()\n",
    "# Word vector dimensionality (how many features each word will be given)\n",
    "n_features = 300\n",
    "# Minimum word count to be taken into account\n",
    "min_word_count = 10\n",
    "# Number of threads to run in parallel (equal to your amount of cores)\n",
    "n_workers = cores\n",
    "# Context window size\n",
    "window = 5\n",
    "# Downsample setting for frequent words\n",
    "downsampling = 1e-2\n",
    "# Seed for the random number generator (to create reproducible results)\n",
    "seed = 1 \n",
    "# Skip-gram = 1, CBOW = 0\n",
    "sg = 1\n",
    "epochs = 20\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=trigrams,\n",
    "    workers=n_workers,\n",
    "    vector_size=n_features,\n",
    "    min_count=min_word_count,\n",
    "    window=window,\n",
    "    sample=downsampling,\n",
    "    seed=seed,\n",
    "    sg=sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 282395,
     "status": "ok",
     "timestamp": 1639949578498,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "tjydPTHRbJwR",
    "outputId": "b1fcf929-7255-426a-fc65-58c5747b23be"
   },
   "outputs": [],
   "source": [
    "model.train(trigrams, total_examples=model.corpus_count, epochs=10)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrNV3eYeZOpo"
   },
   "source": [
    "That was it! We have a Word Embeddings model now. Let's save it so that we don't have to train it again. Then, we can reload the embeddings so that we don't have to train it every single time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRX-lzwSzFI5"
   },
   "outputs": [],
   "source": [
    "model.save('aita.emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZMDwWBuGTn7"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('aita.emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWQC4S9IZOpu"
   },
   "source": [
    "How many terms are in our vocabulary? Whenever interacting with the word vector dictionary, we use the `wv` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVdNakwKfm3H"
   },
   "source": [
    "Let's take a peek at the word vectors our model has learned. We can take a look at the individual words using the `index_to_key` attribute, and the word vectors themselves can be accessed with the `vectors` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 1224,
     "status": "ok",
     "timestamp": 1640016826009,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "R6A19I38fjTT",
    "outputId": "e6d18f9d-cd53-4ae6-d17e-7cf353618735"
   },
   "outputs": [],
   "source": [
    "model.wv.index_to_key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at it won't make a whole lot of sense to us! It's just a bunch of numbers. However, we can do semantic operations on these vectors, such as getting related terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw4th__0ZOp0"
   },
   "source": [
    "### Word Similarity\n",
    "\n",
    "With the information in our word embeddings model, we can try to find similarities between words that interest us (i.e. words that have a similar vector). Let's create a function that retrieves related terms to some input. We're going to use the [`most_similar()`](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html) function in `gensim` as part of this helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1HVouFqZOp0"
   },
   "outputs": [],
   "source": [
    "def get_most_similar_terms(model, token, topn=20):\n",
    "    \"\"\"Look up the top N most similar terms to the token.\"\"\"\n",
    "    for word, similarity in model.wv.most_similar(positive=[token], topn=topn):\n",
    "        print(f\"{word}: {round(similarity, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1639950052918,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "CDyV-ntGZOp3",
    "outputId": "d72277e1-e4e1-451d-e706-70b7fb7ebb02"
   },
   "outputs": [],
   "source": [
    "get_most_similar_terms(model, 'asshole')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some other terms. What else interests you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar_terms(model, 'empathy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar_terms(model, 'relationship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar_terms(model, 'power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar_terms(model, 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar_terms(model, 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAF3aRsHZOp6"
   },
   "source": [
    "### Word Analogies\n",
    "\n",
    "One of the most famous usages of `word2vec` is via word analogies. For example:\n",
    "\n",
    "`Paris : France :: Berlin : Germany`\n",
    "\n",
    "Here, the analogy is between (Paris, France) and (Berlin, Germany), with \"capital city\" being the concept that connects them. We can abstract the \"analogy\" relationship to vector modeling. Let's pretend we're working with each of the vectors. Then, the analogy is\n",
    "\n",
    "$$\\mathbf{v}_{\\text{France}} - \\mathbf{v}_{\\text{Paris}} \\approx \\mathbf{v}_{\\text{Germany}} - \\mathbf{v}_{\\text{Berlin}}.$$\n",
    "\n",
    "The vector difference here represents the notion of \"capital city\". Presumably, going from the Paris vector to the France vector (i.e., the vector difference) will be the same as going from the Berlin vector to the Germany vector, if that difference carries similar semantic meaning.\n",
    "\n",
    "Let's test this directly. We'll do so by rewriting the above expression:\n",
    "\n",
    "$$\\mathbf{v}_{\\text{France}} - \\mathbf{v}_{\\text{Paris}} + \\mathbf{v}_{\\text{Berlin}} \\approx \\mathbf{v}_{\\text{Germany}}.$$\n",
    "\n",
    "The core idea is that once words are represented as numerical vectors, you can do \"math\" with them. In `gensim`, this works with the `most_similar` function, which takes `positive` and `negative` arguments. In the above scenario, the positive terms would be Berlin and France, while the negative term is Paris. You can roughly think of this as: \"What is the vector most similar to Berlin and France, but opposite Paris?\"\n",
    "\n",
    "We can't do this example in our corpus, because we don't have all these words represented. Another example we can do is perhaps t he most well known example:\n",
    "\n",
    "`Man : King :: Woman : ?`\n",
    "\n",
    "What does the function tell us is on the other side of the analogy? Remember, analogies are constructions by humans: they quite literally encode semantic relationships, and thus enforce norms. The fact that the word embedding learns this analogy implies that it has inherited norms practiced by humans, whether those norms are biased or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1639950123172,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "hXhZB8SOZOp9",
    "outputId": "93a8e656-42b6-49ea-eacb-13fff7504f30",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['woman', 'king'], negative='man')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6Y4eXciZOqb"
   },
   "source": [
    "## Clustering Word Vectors\n",
    "\n",
    "One convenience of word embeddings is that we can \"cluster\" them. We can find a group of word vectors that are close to each other, and call it a related \"cluster\". Since we expected word vectors that are semantically similar to be close to each other in space, we might expect the clusters to be semantically meaningful. The clustering algorithm we use is called **K-means clustering**. \n",
    "\n",
    "K-Means clustering aims to group $N$ observations into $K$ clusters (we choose $K$) in which each observation belongs to the cluster with the nearest mean (called the \"cluster center\"), which serves as a prototype of the cluster.\n",
    "\n",
    "Since our words are all represented as vectors, applying K-means is easy to do since the clustering algorithm will simply look at differences between vectors (and centers).\n",
    "\n",
    "A package called [`scikit-learn`](https://scikit-learn.org/stable/) provides us a couple algorithms that will be useful for this section: [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and [`KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html?highlight=kdtree#sklearn.neighbors.KDTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a helper function to perform the clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJodQkitZOqb"
   },
   "outputs": [],
   "source": [
    "def clustering_on_wordvecs(word_vecs, n_clusters):\n",
    "    \"\"\"Clusters a set of word vectors and returns the center of each cluster.\"\"\"\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++')\n",
    "    cluster_ids = kmeans.fit_predict(word_vecs)\n",
    "    return kmeans.cluster_centers_, cluster_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the raw word vectors with the `vectors` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Em2CvpgRZOqf"
   },
   "outputs": [],
   "source": [
    "n_clusters = 20\n",
    "centers, cluster_ids = clustering_on_wordvecs(model.wv.vectors, n_clusters)\n",
    "centroid_map = dict(zip(model.wv.index_to_key, centers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15_523YKZOqj"
   },
   "source": [
    "Next, we get words in each cluster that are closest to the cluster center. To do this, we initialize a data structure called a KDTree on the word vectors, and query it for the top $K$ words on each cluster center. We will use a helper function to print this into a convenient dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9aD76xcZOqk"
   },
   "outputs": [],
   "source": [
    "def get_top_words(model, n_closest, centers):\n",
    "    \"\"\"Get the words closest to each cluster center.\"\"\"\n",
    "    # Create KD Tree\n",
    "    tree = KDTree(model.wv.vectors)\n",
    "    # Use closest points for each cluster center to query the closest points to it\n",
    "    closest_points = tree.query(centers, k=n_closest)[1]\n",
    "    # Query word index for each position\n",
    "    closest_words = {}\n",
    "    for cluster_idx, cluster in enumerate(closest_points):\n",
    "        closest_words[f'Cluster {cluster_idx + 1}'] = [model.wv.index_to_key[idx] for idx in cluster]\n",
    "    # Create DataFrame from dictionary\n",
    "    df = pd.DataFrame(closest_words)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtBFZL_HZOqm"
   },
   "source": [
    "Let’s get the top 50 words for each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_words(model, 50, centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EF6bB0rVReh"
   },
   "source": [
    "## Visualizing High Dimensional Spaces with $t$-SNE\n",
    "\n",
    "The word embeddings we created are what's called a **high-dimensional representation** of the text. That is, we take a word in the corpus, and represent it using, in this case, 300 numbers. We can plot 3 numbers at a time - that's in 3 dimensions - but there's no way for humans to visualize something in a 300-dimensional space. \n",
    "\n",
    "So, **dimensionality reduction** is a big part of machine learning. How can we take vectors that are 300-dimensional, and visualize them in 2-dimensions, while keeping the structure between vectors the same? How can we reduce the dimensionality?\n",
    "\n",
    "One of the most popular methods for dimensionality reduction is called $t$-SNE ($t$-Distributed Stochastic Neighbor Embedding). The details are not important, but using it in practice is a useful skill to learn (if you want to read more, [here](https://lvdmaaten.github.io/tsne/) is a good starting point). Roughly, it tries to keep the relative distances between points as closely as possible in both high-dimensional and low-dimensional space.\n",
    "\n",
    "So, we'll use $t$-SNE to take all the word vectors, and obtain a **low dimensional representation**. We can then visualize it, which may reveal semantic and syntactic trends in the data.\n",
    "\n",
    "A [$t$-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html?highlight=tsne#sklearn.manifold.TSNE) implementation is available via `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you get error in the line tsne=TSNE(), you can try to run the following line:\n",
    "#!pip install -U scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pri9cdRfHlJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jd60bqFxe9z1"
   },
   "outputs": [],
   "source": [
    "# Create some filepaths to save our model\n",
    "tsne_path = 'tsne_model'\n",
    "tsne_vectors_path = 'tsne_vectors.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(init='pca', learning_rate='auto')\n",
    "tsne_vectors = tsne.fit_transform(model.wv.vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our low dimensional representation. Now, let's store the 2 dimensions in a dataframe, with the word as the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the t-SNE vectors\n",
    "tsne_vectors = pd.DataFrame(tsne_vectors,\n",
    "                            index=pd.Index(model.wv.index_to_key),\n",
    "                            columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBe7lNE3e7oQ"
   },
   "outputs": [],
   "source": [
    "with open(tsne_path, 'wb') as f:\n",
    "    pickle.dump(tsne, f)\n",
    "\n",
    "tsne_vectors.to_pickle(tsne_vectors_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a convenient code block to load this data, to start from this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tsne_path, 'rb') as f:\n",
    "    tsne = pickle.load(f)\n",
    "    \n",
    "tsne_vectors = pd.read_pickle(tsne_vectors_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to visualize the 2-dimensional space using a package called `bokeh`. This package is nice for this because it allows for some degree of interactivity: we can go over each point and dynamically get information about the word denoting that vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AE4-OrmKmKG"
   },
   "outputs": [],
   "source": [
    "import bokeh\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "\n",
    "output_notebook()\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "executionInfo": {
     "elapsed": 747,
     "status": "ok",
     "timestamp": 1640017607454,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "myJtActVdjn4",
    "outputId": "5f283a90-c0f4-4c31-88ea-5f7bfa6917ee"
   },
   "outputs": [],
   "source": [
    "# Add our DataFrame as a ColumnDataSource for Bokeh\n",
    "plot_data = ColumnDataSource(tsne_vectors)\n",
    "\n",
    "# Create the plot and configure the title, dimensions, and tools\n",
    "tsne_plot = figure(title='t-SNE Word Embeddings',\n",
    "                   plot_width=800,\n",
    "                   plot_height=800)\n",
    "\n",
    "# Add a hover tool to display words on roll-over\n",
    "tsne_plot.add_tools(HoverTool(tooltips='@index') )\n",
    "\n",
    "# Draw the words as circles on the plot\n",
    "tsne_plot.circle('x', 'y',\n",
    "                 source=plot_data,\n",
    "                 color='blue',\n",
    "                 line_alpha=0.2,\n",
    "                 fill_alpha=0.1,\n",
    "                 size=10,\n",
    "                 hover_line_color='black')\n",
    "\n",
    "# Configure visual elements of the plot\n",
    "tsne_plot.xaxis.visible = False\n",
    "tsne_plot.yaxis.visible = False\n",
    "tsne_plot.grid.grid_line_color = None\n",
    "tsne_plot.outline_line_color = None\n",
    "\n",
    "# Engage!\n",
    "show(tsne_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection: The Hermeneutics of Word Embeddings\n",
    "\n",
    "“In vector space, identities and differences change in nature. Similarity and belonging no longer rely on resemblance or a common genesis but on measures of proximity or distance, on flat loci that run as vectors through the space.” (Dourish 2018: 73-4)\n",
    "\n",
    "As we've seen, word embeddings are essentially a set of vectors. We should reflect on this. What is vectorization? It is reducing linguistic complexity. Or rather, it produces a common space that juxtaposes and mixes complex localized realities. Anything can be turned into a vector operation, but what do we lose when doing so? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 4-1 Word Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
