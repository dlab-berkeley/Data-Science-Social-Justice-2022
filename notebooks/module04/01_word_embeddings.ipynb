{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Uds2RtAZOog"
   },
   "source": [
    "# Data Science for Social Justice Workshop: Module 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfjNP33MZOoi"
   },
   "source": [
    "## Word Embeddings\n",
    "\n",
    "In this notebook, we'll work with word embeddings using [`gensim`](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "The goal of word embedding models is to learn **numerical representations** of text corpora. We already did that to a certain extent when we did topic modeling. In this case, we're going to be more explicit about how we construct that numerical representation: for each word, we're going to find a **vector** of numbers to represent it. The actual numbers themselves won't be meaningful to us as humans. However, if successful, the vectors for each term should encode information about the meaning or concept the term represents, as well as the relationship between it and other terms in the vocabulary.\n",
    "\n",
    "Word vector models are fully **unsupervised**: they learn all of these meanings and relationships without any advance knowledge. Unsupervised learning requires the specification of a right task. We won't go into detail in this lesson, but you can roughly think of the task as predicting nearby words, given a specific word. Read [this post](https://tomvannuenen.medium.com/analyzing-reddit-communities-with-python-part-6-word-embeddings-f92bba876d60) for a deeper introduction to word embeddings.\n",
    "\n",
    "This notebook is designed to help you:\n",
    "\n",
    "* Use `gensim`'s [`word2vec`](https://radimrehurek.com/gensim/models/word2vec.html) method to create word vectors for a corpus;\n",
    "* Use these word vectors to reflect on implicit binaries and normativities in your data;\n",
    "* Visualize topic models using K-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "As we will be considering the language biases in the next notebook, we will use the comments of the AITA subreddit this time. The thinking behind this is that this data will be derived from more people, and include more evaluative statements (after all, comments on r/amitheasshole generally evaluate the original posts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the following two lines if you are running this notebook on Binder.\n",
    "#!pip install pandas\n",
    "#!pip install spacy\n",
    "# Package imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3883fc170219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Change directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# We include two ../ because we want to go two levels up in the file structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data'"
     ]
    }
   ],
   "source": [
    "# Change directory\n",
    "# We include two ../ because we want to go two levels up in the file structure\n",
    "os.chdir(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5723,
     "status": "ok",
     "timestamp": 1639942437840,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "bIB1SsShP9iL",
    "outputId": "01fa486d-4a39-48ef-f6f1-076478c6e0fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 18)\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "df = pd.read_csv('aita_sub_top_sm.csv')\n",
    "df.head(3)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remove comments that were removed or deleted, and additionally only take comments that are sufficiently long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1639942442529,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "B2igBxlIu7Hw",
    "outputId": "806927f1-bd1e-4517-ec05-a45773bb92d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16307"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove comments that are [removed] or [deleted]\n",
    "df = df[~df['selftext'].isin(['[removed]', '[deleted]'])].dropna(subset=['selftext'])\n",
    "# Remove comments less than 15 characters long\n",
    "df = df[df['selftext'].str.len() >= 15]\n",
    "len(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll import `spacy` and `gensim` to do some preprocessing. We have functions written here for you to help streamline the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilygrabowski/opt/anaconda3/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Wt4BIM-cfhK3"
   },
   "outputs": [],
   "source": [
    "def clean(token):\n",
    "    \"\"\"Helper function that specifies whether a token is:\n",
    "        - punctuation\n",
    "        - space\n",
    "        - digit\n",
    "    \"\"\"\n",
    "    return token.is_punct or token.is_space or token.is_digit\n",
    "\n",
    "def line_read(df, text_col='selftext'):\n",
    "    \"\"\"Generator function to read in text from df and get rid of line breaks.\"\"\"    \n",
    "    for text in df[text_col]:\n",
    "        yield text.replace('\\n', '')\n",
    "\n",
    "def preprocess(df, text_col='selftext', allowed_postags=['NOUN', 'ADJ']):\n",
    "    \"\"\"Preprocessing function to apply to a dataframe.\"\"\"\n",
    "    for parsed in nlp.pipe(line_read(df, text_col), batch_size=1000, disable=[\"tok2vec\", \"ner\"]):\n",
    "        # Gather lowercased, lemmatized tokens\n",
    "        tokens = [token.lemma_.lower() if token.lemma_ != '-PRON-'\n",
    "                  else token.lower_ \n",
    "                  for token in parsed if not clean(token)]\n",
    "        # Remove specific lemmatizations, and words that are not nouns or adjectives\n",
    "        tokens = [lemma\n",
    "                  for lemma in tokens\n",
    "                  if not lemma in [\"'s\",  \"’s\", \"’\"] and not lemma in allowed_postags]\n",
    "        # Remove stop words\n",
    "        tokens = [token for token in tokens if token not in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "        yield tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the `preprocess()` function to each comment in the dataframe, producing a `docs` output. This call might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VHOu2vdsfl3Q"
   },
   "outputs": [],
   "source": [
    "docs = [line for line in preprocess(df, text_col='selftext')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create bigrams. Bigrams consist of pairs of words that appear commonly together (e.g., \"New York\"). `gensim` provides some functions to detect bigrams by finding words appear often enough together that we should include them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 126716,
     "status": "ok",
     "timestamp": 1639949141836,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "C4RT1pLBfgaz",
    "outputId": "ae080bf7-fd7b-40a5-df98-79f35ed717aa"
   },
   "outputs": [],
   "source": [
    "# Create bigram model: pass docs into Phrases class\n",
    "bigrams = Phrases(docs, min_count=20, threshold=300)\n",
    "# Create a \"frozen\" bigram model using the Phraser class\n",
    "bigram_phraser = Phraser(bigrams)\n",
    "# Now, create bigrams \n",
    "docs_bigrams = [bigram_phraser[doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's nothing stopping us from going further: we can create trigrams or even $n$-grams. We'll make some trigrams and build our word2vec model on top of them. A trigram can be constructed by simply looking for bigrams in a bigrams corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = Phrases(bigrams[docs], min_count=20, threshold=100)  \n",
    "trigram_phraser = Phraser(trigrams)\n",
    "docs_trigrams = [trigram_phraser[doc] for doc in docs_bigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the data to an external JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GS_5VJXogUMn"
   },
   "outputs": [],
   "source": [
    "with open('aita_com_top_lemmas.json', 'w') as write:\n",
    "    json.dump(docs_trigrams, write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the same file works as follows:\n",
    "with open(\"aita_com_top_lemmas.json\") as f:\n",
    "    trigrams = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wk2mxX1HZOpk"
   },
   "source": [
    "## Constructing a Word2Vec Model\n",
    "\n",
    "Let's create our word embeddings model. \n",
    "\n",
    "While last week's LDA method was focused on finding topics in a collection of documents (or in our case, submissions), word embeddings models focus on individual words, and learning vector representations of these words.\n",
    "\n",
    "The input to the model is a text corpus split up into sentences – in word embeddings, there is no concept of \"documents\". The model's output is a set of \"vectors\" (one for each word) in N dimensions (a parameter of the model). Think of these vectors as \"features\", capturing latent meaning.\n",
    "\n",
    "This model allows us to group the vectors of similar words together in vector space. We can then reduce the dimensionality to visualize the results in a way humans can understand (such as in a 2-dimensional space), or to perform linear algebra operations in order to find out to what extent words are related.\n",
    "\n",
    "Word2Vec is one example of a word embeddings model. It learns by taking words and their contexts (e.g. sentences) into account, and can then try to predict other words. Given enough data, usage and contexts, word2vec can make accurate guesses about a word’s meaning based on its appearances. Those guesses can be used to establish a word’s association with other words (e.g. \"Paris\" is to \"France\" as “Berlin” is to “Germany”), or cluster documents and classify them by topic.\n",
    "\n",
    "We now instantiate and train our Word2Vec model, using the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "sQq117-pZOpl"
   },
   "outputs": [],
   "source": [
    "# Count the number of cores you have at your disposal\n",
    "cores = multiprocessing.cpu_count()\n",
    "# Word vector dimensionality (how many features each word will be given)\n",
    "n_features = 300\n",
    "# Minimum word count to be taken into account\n",
    "min_word_count = 10\n",
    "# Number of threads to run in parallel (equal to your amount of cores)\n",
    "n_workers = cores\n",
    "# Context window size\n",
    "window = 5\n",
    "# Downsample setting for frequent words\n",
    "downsampling = 1e-2\n",
    "# Seed for the random number generator (to create reproducible results)\n",
    "seed = 1 \n",
    "# Skip-gram = 1, CBOW = 0\n",
    "sg = 1\n",
    "epochs = 20\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=trigrams,\n",
    "    workers=n_workers,\n",
    "    vector_size=n_features,\n",
    "    min_count=min_word_count,\n",
    "    window=window,\n",
    "    sample=downsampling,\n",
    "    seed=seed,\n",
    "    sg=sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 282395,
     "status": "ok",
     "timestamp": 1639949578498,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "tjydPTHRbJwR",
    "outputId": "b1fcf929-7255-426a-fc65-58c5747b23be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24518840, 25511520)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(trigrams, total_examples=model.corpus_count, epochs=10)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrNV3eYeZOpo"
   },
   "source": [
    "That was it! We have a Word Embeddings model now. Let's save it so that we don't have to train it again. Then, we can reload the embeddings so that we don't have to train it every single time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "JRX-lzwSzFI5"
   },
   "outputs": [],
   "source": [
    "model.save('aita.emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "SZMDwWBuGTn7"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('aita.emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWQC4S9IZOpu"
   },
   "source": [
    "How many terms are in our vocabulary? Whenever interacting with the word vector dictionary, we use the `wv` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12295"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVdNakwKfm3H"
   },
   "source": [
    "Let's take a peek at the word vectors our model has learned. We can take a look at the individual words using the `index_to_key` attribute, and the word vectors themselves can be accessed with the `vectors` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 1224,
     "status": "ok",
     "timestamp": 1640016826009,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "R6A19I38fjTT",
    "outputId": "e6d18f9d-cd53-4ae6-d17e-7cf353618735"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'said'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index_to_key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06102639,  0.19767517, -0.09929685, -0.01651838, -0.11792817,\n",
       "       -0.12836273,  0.10424978,  0.35728016,  0.02528324, -0.13426565,\n",
       "        0.12556353,  0.08988883,  0.12721542, -0.02497321, -0.13635816,\n",
       "       -0.2221032 ,  0.01603243, -0.01290215, -0.10347308, -0.06935027,\n",
       "        0.04374868, -0.0986355 ,  0.2338665 ,  0.24940981,  0.07474294,\n",
       "        0.02770644, -0.08778619, -0.07598864, -0.06600711, -0.12008175,\n",
       "       -0.10145769, -0.03904285,  0.20689853,  0.04596431,  0.11964171,\n",
       "       -0.12708524,  0.05245314, -0.09969737,  0.00313492,  0.09943998,\n",
       "       -0.00700299,  0.33826628, -0.00694402, -0.00755043,  0.0834844 ,\n",
       "        0.10920402,  0.08264863,  0.11614382,  0.0428956 ,  0.19038281,\n",
       "       -0.07969172,  0.03352622, -0.1976534 , -0.05535923, -0.0638773 ,\n",
       "       -0.06511011,  0.11220443,  0.04737068,  0.1741564 ,  0.05661403,\n",
       "       -0.06075669,  0.01387172, -0.09167124, -0.0693732 , -0.0759397 ,\n",
       "        0.09873556, -0.17701876,  0.04444606, -0.19546069,  0.00745527,\n",
       "       -0.12062532, -0.05083803,  0.11174779,  0.17149019,  0.05203431,\n",
       "       -0.07080929,  0.10091871,  0.14018917, -0.0730159 ,  0.06881811,\n",
       "       -0.10046691, -0.15265988,  0.05121565,  0.24546032,  0.02435929,\n",
       "       -0.03058134,  0.09560125, -0.00515778, -0.00035837,  0.2016101 ,\n",
       "       -0.03004411, -0.01568766,  0.06446924,  0.01165155,  0.17646518,\n",
       "        0.25739282,  0.06338199, -0.09041923, -0.32805914, -0.10848323,\n",
       "        0.03645537, -0.07705008,  0.10935907, -0.0494789 ,  0.03178906,\n",
       "       -0.01818164,  0.22428627,  0.15260471, -0.18601602, -0.0922729 ,\n",
       "       -0.18637806, -0.24887283,  0.00793098, -0.20228988,  0.05925756,\n",
       "        0.05822539,  0.12566431,  0.22259897,  0.08250666, -0.03171266,\n",
       "        0.0749796 , -0.15231334,  0.23014711,  0.25024062, -0.02846579,\n",
       "        0.09220428, -0.00168029, -0.08233103, -0.111348  ,  0.12077702,\n",
       "       -0.01562438,  0.06977792,  0.06915139, -0.15297951,  0.1423267 ,\n",
       "       -0.00079182, -0.09838398, -0.07030986, -0.07821326, -0.05294602,\n",
       "        0.10123502, -0.05726195, -0.03102988, -0.03406704, -0.02588735,\n",
       "        0.11913244, -0.12178338,  0.06368126, -0.12722944, -0.09252114,\n",
       "        0.22116177, -0.22630768,  0.08434901, -0.27043164, -0.16256368,\n",
       "       -0.05592522,  0.01179024, -0.01728155, -0.04688575,  0.1226784 ,\n",
       "       -0.07223544,  0.11838879, -0.05158553,  0.13139111, -0.05478809,\n",
       "        0.24683128,  0.03759978,  0.09002168,  0.07712996,  0.22895424,\n",
       "       -0.12178268, -0.06761352, -0.01958428, -0.00403656,  0.06869257,\n",
       "        0.0318032 , -0.01174992, -0.05865213,  0.03467289, -0.08708597,\n",
       "       -0.1821778 , -0.04120287, -0.02545207, -0.07751004, -0.10963869,\n",
       "        0.0874944 ,  0.00192919, -0.06170957, -0.04016175, -0.21689907,\n",
       "       -0.01126445, -0.00710863,  0.07575686, -0.1903746 ,  0.00180719,\n",
       "       -0.06070336,  0.066433  , -0.03018592, -0.02098453,  0.16044785,\n",
       "       -0.189321  ,  0.14490785, -0.10275505, -0.11843414,  0.18303479,\n",
       "       -0.02146196,  0.04599549,  0.04154687, -0.13908233, -0.05122018,\n",
       "       -0.1093075 , -0.09415656, -0.08256661, -0.22803134,  0.06970213,\n",
       "       -0.03030678,  0.04872759,  0.14777862, -0.04591924, -0.01944545,\n",
       "        0.04773529,  0.14164284,  0.02360011, -0.14974959, -0.05770489,\n",
       "       -0.11389763,  0.10137622,  0.06919452, -0.07979332,  0.01638657,\n",
       "        0.17481227, -0.06356177, -0.08987442,  0.1388293 , -0.22655152,\n",
       "        0.03312619,  0.21545191,  0.10901622,  0.03061479, -0.21641073,\n",
       "        0.07563978,  0.06629311,  0.03242455,  0.00907622,  0.04914527,\n",
       "       -0.15594035, -0.07570733,  0.05991376, -0.06499492,  0.15717691,\n",
       "       -0.0451152 , -0.11189884,  0.16444078, -0.03252379, -0.10742037,\n",
       "       -0.0012231 ,  0.18065296,  0.03226897, -0.27259663, -0.10290638,\n",
       "        0.16471493,  0.05761499,  0.0906753 , -0.08492972, -0.04219906,\n",
       "       -0.07754901,  0.0904967 ,  0.03316967, -0.20413062, -0.12968631,\n",
       "        0.04653436,  0.10868695,  0.06186033,  0.00933562, -0.02254931,\n",
       "        0.09176634,  0.10383158,  0.04502621, -0.0695845 , -0.00533107,\n",
       "        0.11763302, -0.31510633, -0.1085227 , -0.0487711 ,  0.06612708,\n",
       "        0.01767462, -0.17731464, -0.06335188,  0.20590176,  0.07490352,\n",
       "       -0.03958306,  0.05706879,  0.0907792 ,  0.101523  ,  0.17179748,\n",
       "        0.15518948,  0.03250927,  0.01272242,  0.199331  , -0.05127307],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at it won't make a whole lot of sense to us! It's just a bunch of numbers. However, we can do semantic operations on these vectors, such as getting related terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw4th__0ZOp0"
   },
   "source": [
    "### Word Similarity\n",
    "\n",
    "With the information in our word embeddings model, we can try to find similarities between words that interest us (i.e. words that have a similar vector). Let's create a function that retrieves related terms to some input. We're going to use the [`most_similar()`](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html) function in `gensim` as part of this helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "v1HVouFqZOp0"
   },
   "outputs": [],
   "source": [
    "def get_most_similar_terms(model, token, topn=20):\n",
    "    \"\"\"Look up the top N most similar terms to the token.\"\"\"\n",
    "    for word, similarity in model.wv.most_similar(positive=[token], topn=topn):\n",
    "        print(f\"{word}: {round(similarity, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1639950052918,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "CDyV-ntGZOp3",
    "outputId": "d72277e1-e4e1-451d-e706-70b7fb7ebb02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahole: 0.57\n",
      "ah: 0.545\n",
      "aita: 0.541\n",
      "ta: 0.534\n",
      "amita: 0.51\n",
      "aitah: 0.502\n",
      "unwarranted: 0.495\n",
      "jackass: 0.488\n",
      "b\\*tch: 0.475\n",
      "aita?&#x200b;update: 0.472\n",
      "ita: 0.47\n",
      "arsehole: 0.463\n",
      "i?edit: 0.46\n",
      "second_guessing: 0.456\n",
      "jerk: 0.454\n",
      "aita??edit: 0.452\n",
      "verdict: 0.451\n",
      "misogynist: 0.451\n",
      "aitaedit: 0.449\n",
      "uncaring: 0.441\n"
     ]
    }
   ],
   "source": [
    "get_most_similar_terms(model, 'asshole')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some other terms. What else interests you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kindness: 0.477\n",
      "compassion: 0.455\n",
      "aita??edit: 0.447\n",
      "overdramatic: 0.436\n",
      "lacks: 0.435\n",
      "validated: 0.429\n",
      "❤_️: 0.428\n",
      "empathize: 0.425\n",
      "thankyou: 0.418\n",
      "disregarding: 0.418\n",
      "deprive: 0.417\n",
      "her-: 0.414\n",
      "insights: 0.413\n",
      "empathetic: 0.412\n",
      "abilities: 0.412\n",
      "digress: 0.406\n",
      "confide: 0.406\n",
      "disregard: 0.398\n",
      "sympathy: 0.389\n",
      "disgraceful: 0.386\n"
     ]
    }
   ],
   "source": [
    "get_most_similar_terms(model, 'empathy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coparent: 0.51\n",
      "marriage: 0.503\n",
      "co_parenting: 0.486\n",
      "relationships: 0.474\n",
      "animosity: 0.464\n",
      "rocky: 0.459\n",
      "unfaithful: 0.454\n",
      "stepparent: 0.449\n",
      "51f: 0.445\n",
      "fwb: 0.442\n",
      "dynamic: 0.437\n",
      "bond: 0.43\n",
      "intimacy: 0.43\n",
      "friendship: 0.427\n",
      "lc: 0.424\n",
      "polyamorous: 0.419\n",
      "coparenting: 0.417\n",
      "romantically: 0.416\n",
      "poly: 0.416\n",
      "throwaway_accounti: 0.413\n"
     ]
    }
   ],
   "source": [
    "get_most_similar_terms(model, 'relationship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "veto: 0.339\n",
      "authority: 0.313\n",
      "merge: 0.3\n",
      "tripping: 0.299\n",
      "sustainable: 0.283\n",
      "articulate: 0.282\n",
      "skewed: 0.282\n",
      "circumcised: 0.281\n",
      "doormat: 0.28\n",
      "unilaterally: 0.28\n",
      "sinners: 0.278\n",
      "cognizant: 0.277\n",
      "steering: 0.275\n",
      "lined: 0.274\n",
      "exists: 0.274\n",
      "electrical: 0.273\n",
      "epic: 0.272\n",
      "unreliable: 0.27\n",
      "uproot: 0.27\n",
      "advise: 0.27\n"
     ]
    }
   ],
   "source": [
    "get_most_similar_terms(model, 'power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman: 0.476\n",
      "bloke: 0.426\n",
      "mans: 0.376\n",
      "father: 0.369\n",
      "lacy: 0.361\n",
      "intimidating: 0.358\n",
      "nervously: 0.354\n",
      "men: 0.351\n",
      "ftm: 0.351\n",
      "spun: 0.349\n",
      "\\[my: 0.348\n",
      "intimidated: 0.348\n",
      "guy: 0.348\n",
      "flattered: 0.346\n",
      "handsome: 0.346\n",
      "soldier: 0.343\n",
      "heterosexual: 0.343\n",
      "angie: 0.343\n",
      "cis: 0.339\n",
      "marries: 0.339\n"
     ]
    }
   ],
   "source": [
    "get_most_similar_terms(model, 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man: 0.476\n",
      "bloke: 0.407\n",
      "guy: 0.391\n",
      "girl: 0.384\n",
      "lady: 0.376\n",
      "seventeen: 0.369\n",
      "ftm: 0.368\n",
      "phobic: 0.361\n",
      "handsome: 0.357\n",
      "conventionally: 0.356\n",
      "lia: 0.353\n",
      "4ish: 0.352\n",
      "opinionated: 0.346\n",
      "men: 0.34\n",
      "women: 0.339\n",
      "tempered: 0.339\n",
      "claude: 0.338\n",
      "identifies: 0.337\n",
      "iran: 0.336\n",
      "isle: 0.332\n"
     ]
    }
   ],
   "source": [
    "get_most_similar_terms(model, 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAF3aRsHZOp6"
   },
   "source": [
    "### Word Analogies\n",
    "\n",
    "One of the most famous usages of `word2vec` is via word analogies. For example:\n",
    "\n",
    "`Paris : France :: Berlin : Germany`\n",
    "\n",
    "Here, the analogy is between (Paris, France) and (Berlin, Germany), with \"capital city\" being the concept that connects them. We can abstract the \"analogy\" relationship to vector modeling. Let's pretend we're working with each of the vectors. Then, the analogy is\n",
    "\n",
    "$$\\mathbf{v}_{\\text{France}} - \\mathbf{v}_{\\text{Paris}} \\approx \\mathbf{v}_{\\text{Germany}} - \\mathbf{v}_{\\text{Berlin}}.$$\n",
    "\n",
    "The vector difference here represents the notion of \"capital city\". Presumably, going from the Paris vector to the France vector (i.e., the vector difference) will be the same as going from the Berlin vector to the Germany vector, if that difference carries similar semantic meaning.\n",
    "\n",
    "Let's test this directly. We'll do so by rewriting the above expression:\n",
    "\n",
    "$$\\mathbf{v}_{\\text{France}} - \\mathbf{v}_{\\text{Paris}} + \\mathbf{v}_{\\text{Berlin}} \\approx \\mathbf{v}_{\\text{Germany}}.$$\n",
    "\n",
    "The core idea is that once words are represented as numerical vectors, you can do \"math\" with them. In `gensim`, this works with the `most_similar` function, which takes `positive` and `negative` arguments. In the above scenario, the positive terms would be Berlin and France, while the negative term is Paris. You can roughly think of this as: \"What is the vector most similar to Berlin and France, but opposite Paris?\"\n",
    "\n",
    "We can't do this example in our corpus, because we don't have all these words represented. Another example we can do is perhaps t he most well known example:\n",
    "\n",
    "`Man : King :: Woman : ?`\n",
    "\n",
    "What does the function tell us is on the other side of the analogy? Remember, analogies are constructions by humans: they quite literally encode semantic relationships, and thus enforce norms. The fact that the word embedding learns this analogy implies that it has inherited norms practiced by humans, whether those norms are biased or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1639950123172,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "hXhZB8SOZOp9",
    "outputId": "93a8e656-42b6-49ea-eacb-13fff7504f30",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'a' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-4dc929383f0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'woman'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'king'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'man'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'a' not present\""
     ]
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'king'], negative='man')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6Y4eXciZOqb"
   },
   "source": [
    "## Clustering Word Vectors\n",
    "\n",
    "One convenience of word embeddings is that we can \"cluster\" them. We can find a group of word vectors that are close to each other, and call it a related \"cluster\". Since we expected word vectors that are semantically similar to be close to each other in space, we might expect the clusters to be semantically meaningful. The clustering algorithm we use is called **K-means clustering**. \n",
    "\n",
    "K-Means clustering aims to group $N$ observations into $K$ clusters (we choose $K$) in which each observation belongs to the cluster with the nearest mean (called the \"cluster center\"), which serves as a prototype of the cluster.\n",
    "\n",
    "Since our words are all represented as vectors, applying K-means is easy to do since the clustering algorithm will simply look at differences between vectors (and centers).\n",
    "\n",
    "A package called [`scikit-learn`](https://scikit-learn.org/stable/) provides us a couple algorithms that will be useful for this section: [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and [`KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html?highlight=kdtree#sklearn.neighbors.KDTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a helper function to perform the clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "rJodQkitZOqb"
   },
   "outputs": [],
   "source": [
    "def clustering_on_wordvecs(word_vecs, n_clusters):\n",
    "    \"\"\"Clusters a set of word vectors and returns the center of each cluster.\"\"\"\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++')\n",
    "    cluster_ids = kmeans.fit_predict(word_vecs)\n",
    "    return kmeans.cluster_centers_, cluster_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the raw word vectors with the `vectors` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Em2CvpgRZOqf"
   },
   "outputs": [],
   "source": [
    "n_clusters = 20\n",
    "centers, cluster_ids = clustering_on_wordvecs(model.wv.vectors, n_clusters)\n",
    "centroid_map = dict(zip(model.wv.index_to_key, centers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15_523YKZOqj"
   },
   "source": [
    "Next, we get words in each cluster that are closest to the cluster center. To do this, we initialize a data structure called a KDTree on the word vectors, and query it for the top $K$ words on each cluster center. We will use a helper function to print this into a convenient dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "O9aD76xcZOqk"
   },
   "outputs": [],
   "source": [
    "def get_top_words(model, n_closest, centers):\n",
    "    \"\"\"Get the words closest to each cluster center.\"\"\"\n",
    "    # Create KD Tree\n",
    "    tree = KDTree(model.wv.vectors)\n",
    "    # Use closest points for each cluster center to query the closest points to it\n",
    "    closest_points = tree.query(centers, k=n_closest)[1]\n",
    "    # Query word index for each position\n",
    "    closest_words = {}\n",
    "    for cluster_idx, cluster in enumerate(closest_points):\n",
    "        closest_words[f'Cluster {cluster_idx + 1}'] = [model.wv.index_to_key[idx] for idx in cluster]\n",
    "    # Create DataFrame from dictionary\n",
    "    df = pd.DataFrame(closest_words)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtBFZL_HZOqm"
   },
   "source": [
    "Let’s get the top 50 words for each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster 1</th>\n",
       "      <th>Cluster 2</th>\n",
       "      <th>Cluster 3</th>\n",
       "      <th>Cluster 4</th>\n",
       "      <th>Cluster 5</th>\n",
       "      <th>Cluster 6</th>\n",
       "      <th>Cluster 7</th>\n",
       "      <th>Cluster 8</th>\n",
       "      <th>Cluster 9</th>\n",
       "      <th>Cluster 10</th>\n",
       "      <th>Cluster 11</th>\n",
       "      <th>Cluster 12</th>\n",
       "      <th>Cluster 13</th>\n",
       "      <th>Cluster 14</th>\n",
       "      <th>Cluster 15</th>\n",
       "      <th>Cluster 16</th>\n",
       "      <th>Cluster 17</th>\n",
       "      <th>Cluster 18</th>\n",
       "      <th>Cluster 19</th>\n",
       "      <th>Cluster 20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scrubbing</td>\n",
       "      <td>said</td>\n",
       "      <td>steamed</td>\n",
       "      <td>conferences</td>\n",
       "      <td>laurie</td>\n",
       "      <td>told</td>\n",
       "      <td>linkedin</td>\n",
       "      <td>wolf</td>\n",
       "      <td>arab</td>\n",
       "      <td>10pm</td>\n",
       "      <td>uti</td>\n",
       "      <td>i(28</td>\n",
       "      <td>anemia</td>\n",
       "      <td>crybaby</td>\n",
       "      <td>puddle</td>\n",
       "      <td>looser</td>\n",
       "      <td>500k</td>\n",
       "      <td>festivities</td>\n",
       "      <td>amita</td>\n",
       "      <td>😊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scrubbed</td>\n",
       "      <td>like</td>\n",
       "      <td>frying</td>\n",
       "      <td>elite</td>\n",
       "      <td>told</td>\n",
       "      <td>like</td>\n",
       "      <td>tweet</td>\n",
       "      <td>leashed</td>\n",
       "      <td>philippines</td>\n",
       "      <td>winding</td>\n",
       "      <td>prognosis</td>\n",
       "      <td>i(28f</td>\n",
       "      <td>therapies</td>\n",
       "      <td>amita</td>\n",
       "      <td>inspect</td>\n",
       "      <td>neon</td>\n",
       "      <td>65k</td>\n",
       "      <td>i(28f</td>\n",
       "      <td>wierd</td>\n",
       "      <td>sooooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fountain</td>\n",
       "      <td>embarassing</td>\n",
       "      <td>caramel</td>\n",
       "      <td>economics</td>\n",
       "      <td>i(28f</td>\n",
       "      <td>said</td>\n",
       "      <td>thirdly</td>\n",
       "      <td>sans</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>tuesdays</td>\n",
       "      <td>contracting</td>\n",
       "      <td>i(27</td>\n",
       "      <td>stunts</td>\n",
       "      <td>told</td>\n",
       "      <td>honked</td>\n",
       "      <td>cardigan</td>\n",
       "      <td>aud</td>\n",
       "      <td>laurie</td>\n",
       "      <td>like</td>\n",
       "      <td>clarifications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tide</td>\n",
       "      <td>told</td>\n",
       "      <td>macaroni</td>\n",
       "      <td>emt</td>\n",
       "      <td>m16</td>\n",
       "      <td>time</td>\n",
       "      <td>speculation</td>\n",
       "      <td>speculation</td>\n",
       "      <td>norway</td>\n",
       "      <td>time</td>\n",
       "      <td>therapies</td>\n",
       "      <td>m16</td>\n",
       "      <td>moods</td>\n",
       "      <td>caving</td>\n",
       "      <td>shrieked</td>\n",
       "      <td>pins</td>\n",
       "      <td>6,000</td>\n",
       "      <td>14th</td>\n",
       "      <td>think</td>\n",
       "      <td>trolling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>toothpaste</td>\n",
       "      <td>jab</td>\n",
       "      <td>burritos</td>\n",
       "      <td>inconsistent</td>\n",
       "      <td>said</td>\n",
       "      <td>know</td>\n",
       "      <td>contemplating</td>\n",
       "      <td>wandering</td>\n",
       "      <td>i(28f</td>\n",
       "      <td>decompress</td>\n",
       "      <td>observation</td>\n",
       "      <td>f35</td>\n",
       "      <td>observation</td>\n",
       "      <td>said</td>\n",
       "      <td>hammered</td>\n",
       "      <td>hairstyles</td>\n",
       "      <td>25,000</td>\n",
       "      <td>newlyweds</td>\n",
       "      <td>fathom</td>\n",
       "      <td>inputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>canvas</td>\n",
       "      <td>frown</td>\n",
       "      <td>refried</td>\n",
       "      <td>dang</td>\n",
       "      <td>seventeen</td>\n",
       "      <td>want</td>\n",
       "      <td>\"that</td>\n",
       "      <td>fires</td>\n",
       "      <td>catholics</td>\n",
       "      <td>infants</td>\n",
       "      <td>mri</td>\n",
       "      <td>f34</td>\n",
       "      <td>medicated</td>\n",
       "      <td>dictator</td>\n",
       "      <td>tripping</td>\n",
       "      <td>rubber</td>\n",
       "      <td>bitcoin</td>\n",
       "      <td>going</td>\n",
       "      <td>laughable</td>\n",
       "      <td>here?update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>perfumes</td>\n",
       "      <td>barked</td>\n",
       "      <td>oatmeal</td>\n",
       "      <td>thirdly</td>\n",
       "      <td>wierd</td>\n",
       "      <td>wierd</td>\n",
       "      <td>caution</td>\n",
       "      <td>angels</td>\n",
       "      <td>sweden</td>\n",
       "      <td>whip</td>\n",
       "      <td>sterile</td>\n",
       "      <td>i(27f</td>\n",
       "      <td>decreased</td>\n",
       "      <td>like</td>\n",
       "      <td>shoveling</td>\n",
       "      <td>horns</td>\n",
       "      <td>revenue</td>\n",
       "      <td>told</td>\n",
       "      <td>blindly</td>\n",
       "      <td>p.s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sippy</td>\n",
       "      <td>whip</td>\n",
       "      <td>seasoned</td>\n",
       "      <td>skillset</td>\n",
       "      <td>granddaughters</td>\n",
       "      <td>observation</td>\n",
       "      <td>reposting</td>\n",
       "      <td>homeowners</td>\n",
       "      <td>eastern</td>\n",
       "      <td>doubles</td>\n",
       "      <td>hpv</td>\n",
       "      <td>51f</td>\n",
       "      <td>prolonged</td>\n",
       "      <td>oversight</td>\n",
       "      <td>smacked</td>\n",
       "      <td>unicorn</td>\n",
       "      <td>biweekly</td>\n",
       "      <td>autumn</td>\n",
       "      <td>funnily</td>\n",
       "      <td>😅</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aggravating</td>\n",
       "      <td>prob</td>\n",
       "      <td>edible</td>\n",
       "      <td>delaying</td>\n",
       "      <td>delight</td>\n",
       "      <td>going</td>\n",
       "      <td>told</td>\n",
       "      <td>like</td>\n",
       "      <td>ultra</td>\n",
       "      <td>drowned</td>\n",
       "      <td>testosterone</td>\n",
       "      <td>46f</td>\n",
       "      <td>irritation</td>\n",
       "      <td>wierd</td>\n",
       "      <td>vicinity</td>\n",
       "      <td>vibrant</td>\n",
       "      <td>recoup</td>\n",
       "      <td>time</td>\n",
       "      <td>observation</td>\n",
       "      <td>aita?&amp;#x200b;update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rubber</td>\n",
       "      <td>prodding</td>\n",
       "      <td>sausages</td>\n",
       "      <td>i(28f</td>\n",
       "      <td>like</td>\n",
       "      <td>think</td>\n",
       "      <td>bases</td>\n",
       "      <td>peculiar</td>\n",
       "      <td>poorer</td>\n",
       "      <td>frustrates</td>\n",
       "      <td>spike</td>\n",
       "      <td>f15</td>\n",
       "      <td>wolf</td>\n",
       "      <td>delusions</td>\n",
       "      <td>gps</td>\n",
       "      <td>eccentric</td>\n",
       "      <td>airfare</td>\n",
       "      <td>said</td>\n",
       "      <td>know</td>\n",
       "      <td>me?edit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rearranging</td>\n",
       "      <td>choking</td>\n",
       "      <td>hamburger</td>\n",
       "      <td>architecture</td>\n",
       "      <td>bitterly</td>\n",
       "      <td>laughable</td>\n",
       "      <td>know</td>\n",
       "      <td>umbrella</td>\n",
       "      <td>evangelical</td>\n",
       "      <td>like</td>\n",
       "      <td>believable</td>\n",
       "      <td>seventeen</td>\n",
       "      <td>deteriorating</td>\n",
       "      <td>\"aita</td>\n",
       "      <td>barked</td>\n",
       "      <td>pictured</td>\n",
       "      <td>100,000</td>\n",
       "      <td>speculation</td>\n",
       "      <td>flaw</td>\n",
       "      <td>reposting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sponge</td>\n",
       "      <td>bruh</td>\n",
       "      <td>almond</td>\n",
       "      <td>interns</td>\n",
       "      <td>carl</td>\n",
       "      <td>speculation</td>\n",
       "      <td>noting</td>\n",
       "      <td>whip</td>\n",
       "      <td>info-</td>\n",
       "      <td>barked</td>\n",
       "      <td>vaginal</td>\n",
       "      <td>m35</td>\n",
       "      <td>mri</td>\n",
       "      <td>fixated</td>\n",
       "      <td>whip</td>\n",
       "      <td>fabrics</td>\n",
       "      <td>luxurious</td>\n",
       "      <td>impromptu</td>\n",
       "      <td>jab</td>\n",
       "      <td>post](https://www.reddit.com_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>edible</td>\n",
       "      <td>know</td>\n",
       "      <td>berries</td>\n",
       "      <td>collage</td>\n",
       "      <td>know</td>\n",
       "      <td>thirdly</td>\n",
       "      <td>warrants</td>\n",
       "      <td>'cause</td>\n",
       "      <td>scotland</td>\n",
       "      <td>television</td>\n",
       "      <td>succeeded</td>\n",
       "      <td>f29</td>\n",
       "      <td>fainting</td>\n",
       "      <td>justifying</td>\n",
       "      <td>mirrors</td>\n",
       "      <td>patches</td>\n",
       "      <td>millionaire</td>\n",
       "      <td>luxurious</td>\n",
       "      <td>scummy</td>\n",
       "      <td>caution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>closets</td>\n",
       "      <td>bummer</td>\n",
       "      <td>toppings</td>\n",
       "      <td>credentials</td>\n",
       "      <td>theo</td>\n",
       "      <td>appealing</td>\n",
       "      <td>referenced</td>\n",
       "      <td>autumn</td>\n",
       "      <td>stumped</td>\n",
       "      <td>dawned</td>\n",
       "      <td>coma</td>\n",
       "      <td>i(26f</td>\n",
       "      <td>ache</td>\n",
       "      <td>sociopath</td>\n",
       "      <td>zip</td>\n",
       "      <td>jumpsuit</td>\n",
       "      <td>60,000</td>\n",
       "      <td>50th</td>\n",
       "      <td>invalidate</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>usable</td>\n",
       "      <td>strikes</td>\n",
       "      <td>hamburgers</td>\n",
       "      <td>stats</td>\n",
       "      <td>jab</td>\n",
       "      <td>scummy</td>\n",
       "      <td>said</td>\n",
       "      <td>mowed</td>\n",
       "      <td>crosses</td>\n",
       "      <td>snuggled</td>\n",
       "      <td>objections</td>\n",
       "      <td>i(24f</td>\n",
       "      <td>delusions</td>\n",
       "      <td>think</td>\n",
       "      <td>chucked</td>\n",
       "      <td>lacy</td>\n",
       "      <td>exceed</td>\n",
       "      <td>informal</td>\n",
       "      <td>unknowingly</td>\n",
       "      <td>voicing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hammered</td>\n",
       "      <td>patronizing</td>\n",
       "      <td>chops</td>\n",
       "      <td>academics</td>\n",
       "      <td>n.</td>\n",
       "      <td>laurie</td>\n",
       "      <td>laughable</td>\n",
       "      <td>dock</td>\n",
       "      <td>era</td>\n",
       "      <td>3am</td>\n",
       "      <td>pressures</td>\n",
       "      <td>f17</td>\n",
       "      <td>pressures</td>\n",
       "      <td>disgraceful</td>\n",
       "      <td>unplug</td>\n",
       "      <td>layer</td>\n",
       "      <td>12k</td>\n",
       "      <td>bummer</td>\n",
       "      <td>besties</td>\n",
       "      <td>blinded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>chucked</td>\n",
       "      <td>blindly</td>\n",
       "      <td>noodle</td>\n",
       "      <td>statistics</td>\n",
       "      <td>time</td>\n",
       "      <td>i(28f</td>\n",
       "      <td>crosses</td>\n",
       "      <td>sunlight</td>\n",
       "      <td>soldier</td>\n",
       "      <td>reasons:1</td>\n",
       "      <td>deteriorating</td>\n",
       "      <td>48f</td>\n",
       "      <td>spells</td>\n",
       "      <td>prodding</td>\n",
       "      <td>'cause</td>\n",
       "      <td>sleeveless</td>\n",
       "      <td>inconsistent</td>\n",
       "      <td>dibs</td>\n",
       "      <td>presumptuous</td>\n",
       "      <td>amitheasshole_comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>drywall</td>\n",
       "      <td>bewildered</td>\n",
       "      <td>parmesan</td>\n",
       "      <td>majored</td>\n",
       "      <td>mom</td>\n",
       "      <td>inconsistent</td>\n",
       "      <td>highlighted</td>\n",
       "      <td>chaotic</td>\n",
       "      <td>nationality</td>\n",
       "      <td>told</td>\n",
       "      <td>sh</td>\n",
       "      <td>m37</td>\n",
       "      <td>diagnoses</td>\n",
       "      <td>meddling</td>\n",
       "      <td>wolf</td>\n",
       "      <td>braided</td>\n",
       "      <td>estimates</td>\n",
       "      <td>spree</td>\n",
       "      <td>blinded</td>\n",
       "      <td>subscribers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ikea</td>\n",
       "      <td>awe</td>\n",
       "      <td>sprinkle</td>\n",
       "      <td>leadership</td>\n",
       "      <td>husband</td>\n",
       "      <td>amita</td>\n",
       "      <td>fooling</td>\n",
       "      <td>ontario</td>\n",
       "      <td>adapted</td>\n",
       "      <td>wolf</td>\n",
       "      <td>dermatologist</td>\n",
       "      <td>f36</td>\n",
       "      <td>psychiatric</td>\n",
       "      <td>altercation</td>\n",
       "      <td>unload</td>\n",
       "      <td>booty</td>\n",
       "      <td>50,000</td>\n",
       "      <td>fussed</td>\n",
       "      <td>fixated</td>\n",
       "      <td>observation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>doubles</td>\n",
       "      <td>'cause</td>\n",
       "      <td>cider</td>\n",
       "      <td>associates</td>\n",
       "      <td>sister</td>\n",
       "      <td>blindly</td>\n",
       "      <td>peaked</td>\n",
       "      <td>i(28f</td>\n",
       "      <td>asians</td>\n",
       "      <td>hyped</td>\n",
       "      <td>regretful</td>\n",
       "      <td>60f</td>\n",
       "      <td>sh</td>\n",
       "      <td>harsher</td>\n",
       "      <td>coats</td>\n",
       "      <td>curl</td>\n",
       "      <td>totaling</td>\n",
       "      <td>hitch</td>\n",
       "      <td>tactics</td>\n",
       "      <td>pardon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sinks</td>\n",
       "      <td>justifying</td>\n",
       "      <td>crisps</td>\n",
       "      <td>academy</td>\n",
       "      <td>prodding</td>\n",
       "      <td>frown</td>\n",
       "      <td>distressing</td>\n",
       "      <td>willy</td>\n",
       "      <td>iran</td>\n",
       "      <td>uninterrupted</td>\n",
       "      <td>vaccinations</td>\n",
       "      <td>49f</td>\n",
       "      <td>dawned</td>\n",
       "      <td>ganged</td>\n",
       "      <td>tile</td>\n",
       "      <td>wavy</td>\n",
       "      <td>7k</td>\n",
       "      <td>christening</td>\n",
       "      <td>speculation</td>\n",
       "      <td>wrong?edit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>silicone</td>\n",
       "      <td>yah</td>\n",
       "      <td>pudding</td>\n",
       "      <td>begrudge</td>\n",
       "      <td>remarry</td>\n",
       "      <td>spiel</td>\n",
       "      <td>info-</td>\n",
       "      <td>accustomed</td>\n",
       "      <td>unknowingly</td>\n",
       "      <td>8pm</td>\n",
       "      <td>complication</td>\n",
       "      <td>~3</td>\n",
       "      <td>arise</td>\n",
       "      <td>racists</td>\n",
       "      <td>speculation</td>\n",
       "      <td>sizing</td>\n",
       "      <td>agents</td>\n",
       "      <td>bffs</td>\n",
       "      <td>vendetta</td>\n",
       "      <td>pming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wrench</td>\n",
       "      <td>crybaby</td>\n",
       "      <td>nutritious</td>\n",
       "      <td>preferably</td>\n",
       "      <td>i(27</td>\n",
       "      <td>gamble</td>\n",
       "      <td>like</td>\n",
       "      <td>dawned</td>\n",
       "      <td>prejudice</td>\n",
       "      <td>said</td>\n",
       "      <td>examined</td>\n",
       "      <td>m36</td>\n",
       "      <td>xanax</td>\n",
       "      <td>classist</td>\n",
       "      <td>stunts</td>\n",
       "      <td>fuzzy</td>\n",
       "      <td>250k</td>\n",
       "      <td>availability</td>\n",
       "      <td>weirdest</td>\n",
       "      <td>😁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>frying</td>\n",
       "      <td>uncomfortably</td>\n",
       "      <td>inedible</td>\n",
       "      <td>sponsored</td>\n",
       "      <td>marries</td>\n",
       "      <td>delaying</td>\n",
       "      <td>justifying</td>\n",
       "      <td>observation</td>\n",
       "      <td>wierd</td>\n",
       "      <td>stunts</td>\n",
       "      <td>operated</td>\n",
       "      <td>doozy</td>\n",
       "      <td>overworked</td>\n",
       "      <td>'cause</td>\n",
       "      <td>prob</td>\n",
       "      <td>necklaces</td>\n",
       "      <td>40,000</td>\n",
       "      <td>like</td>\n",
       "      <td>\"aita</td>\n",
       "      <td>thorough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>unplug</td>\n",
       "      <td>fuckin</td>\n",
       "      <td>sauces</td>\n",
       "      <td>perks</td>\n",
       "      <td>speculation</td>\n",
       "      <td>elses</td>\n",
       "      <td>trace</td>\n",
       "      <td>increases</td>\n",
       "      <td>vietnam</td>\n",
       "      <td>prob</td>\n",
       "      <td>delusions</td>\n",
       "      <td>52f</td>\n",
       "      <td>autoimmune</td>\n",
       "      <td>ganging</td>\n",
       "      <td>told</td>\n",
       "      <td>violet</td>\n",
       "      <td>halves</td>\n",
       "      <td>i(24f</td>\n",
       "      <td>troublemaker</td>\n",
       "      <td>thirdly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sneaked</td>\n",
       "      <td>frustrates</td>\n",
       "      <td>cauliflower</td>\n",
       "      <td>enlisted</td>\n",
       "      <td>wife</td>\n",
       "      <td>intrude</td>\n",
       "      <td>dawned</td>\n",
       "      <td>visitor</td>\n",
       "      <td>uneducated</td>\n",
       "      <td>aggravating</td>\n",
       "      <td>told</td>\n",
       "      <td>m22</td>\n",
       "      <td>stemming</td>\n",
       "      <td>jab</td>\n",
       "      <td>fountain</td>\n",
       "      <td>awe</td>\n",
       "      <td>4,000</td>\n",
       "      <td>itinerary</td>\n",
       "      <td>delusions</td>\n",
       "      <td>out?edit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>whip</td>\n",
       "      <td>wolf</td>\n",
       "      <td>herbs</td>\n",
       "      <td>reopen</td>\n",
       "      <td>going</td>\n",
       "      <td>misled</td>\n",
       "      <td>reprimand</td>\n",
       "      <td>lawns</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>ungodly</td>\n",
       "      <td>dawned</td>\n",
       "      <td>3y</td>\n",
       "      <td>spike</td>\n",
       "      <td>b\\*tch</td>\n",
       "      <td>scooter</td>\n",
       "      <td>rainbows</td>\n",
       "      <td>laughable</td>\n",
       "      <td>16th</td>\n",
       "      <td>\"i'm</td>\n",
       "      <td>wierd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>peculiar</td>\n",
       "      <td>ribbing</td>\n",
       "      <td>dominos</td>\n",
       "      <td>apprentice</td>\n",
       "      <td>festivities</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>gps</td>\n",
       "      <td>rambunctious</td>\n",
       "      <td>origin</td>\n",
       "      <td>intrude</td>\n",
       "      <td>entails</td>\n",
       "      <td>47f</td>\n",
       "      <td>tender</td>\n",
       "      <td>souls</td>\n",
       "      <td>squad</td>\n",
       "      <td>conservatively</td>\n",
       "      <td>90k</td>\n",
       "      <td>greece</td>\n",
       "      <td>said</td>\n",
       "      <td>normalized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>vienna</td>\n",
       "      <td>hammered</td>\n",
       "      <td>milkshake</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>i(28</td>\n",
       "      <td>endanger</td>\n",
       "      <td>cognizant</td>\n",
       "      <td>stinky</td>\n",
       "      <td>visitor</td>\n",
       "      <td>6:45</td>\n",
       "      <td>seventeen</td>\n",
       "      <td>m23</td>\n",
       "      <td>thyroid</td>\n",
       "      <td>deceptive</td>\n",
       "      <td>tying</td>\n",
       "      <td>gray</td>\n",
       "      <td>inflation</td>\n",
       "      <td>know</td>\n",
       "      <td>crosses</td>\n",
       "      <td>misunderstandings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>spoons</td>\n",
       "      <td>fixated</td>\n",
       "      <td>shredded</td>\n",
       "      <td>pilot</td>\n",
       "      <td>think</td>\n",
       "      <td>arise</td>\n",
       "      <td>transactions</td>\n",
       "      <td>goofy</td>\n",
       "      <td>baptist</td>\n",
       "      <td>bummer</td>\n",
       "      <td>urgently</td>\n",
       "      <td>m34</td>\n",
       "      <td>lemme</td>\n",
       "      <td>scummy</td>\n",
       "      <td>pizzeria</td>\n",
       "      <td>plunge</td>\n",
       "      <td>destitute</td>\n",
       "      <td>barbeque</td>\n",
       "      <td>pictured</td>\n",
       "      <td>unknowingly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>vents</td>\n",
       "      <td>think</td>\n",
       "      <td>jelly</td>\n",
       "      <td>flaw</td>\n",
       "      <td>funnily</td>\n",
       "      <td>parting</td>\n",
       "      <td>observation</td>\n",
       "      <td>fountain</td>\n",
       "      <td>boston</td>\n",
       "      <td>i(28f</td>\n",
       "      <td>certainty</td>\n",
       "      <td>m25</td>\n",
       "      <td>fatigue</td>\n",
       "      <td>tricking</td>\n",
       "      <td>visitor</td>\n",
       "      <td>buff</td>\n",
       "      <td>vienna</td>\n",
       "      <td>buisness</td>\n",
       "      <td>spiel</td>\n",
       "      <td>downvoted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>razors</td>\n",
       "      <td>piped</td>\n",
       "      <td>eggplant</td>\n",
       "      <td>buisness</td>\n",
       "      <td>enlisted</td>\n",
       "      <td>prioritise</td>\n",
       "      <td>erased</td>\n",
       "      <td>unleashed</td>\n",
       "      <td>sinners</td>\n",
       "      <td>8:00</td>\n",
       "      <td>laughable</td>\n",
       "      <td>f30</td>\n",
       "      <td>targets</td>\n",
       "      <td>\"i'm</td>\n",
       "      <td>said</td>\n",
       "      <td>peculiar</td>\n",
       "      <td>thirdly</td>\n",
       "      <td>frown</td>\n",
       "      <td>told</td>\n",
       "      <td>advices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tripping</td>\n",
       "      <td>unknowingly</td>\n",
       "      <td>tenders</td>\n",
       "      <td>tasked</td>\n",
       "      <td>grudgingly</td>\n",
       "      <td>aita?i</td>\n",
       "      <td>bait</td>\n",
       "      <td>amita</td>\n",
       "      <td>funnily</td>\n",
       "      <td>going</td>\n",
       "      <td>checkups</td>\n",
       "      <td>1f</td>\n",
       "      <td>screened</td>\n",
       "      <td>disowning</td>\n",
       "      <td>rhe</td>\n",
       "      <td>heals</td>\n",
       "      <td>overdue</td>\n",
       "      <td>mallory</td>\n",
       "      <td>noble</td>\n",
       "      <td>contemplating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>like</td>\n",
       "      <td>dawned</td>\n",
       "      <td>gravy</td>\n",
       "      <td>overworked</td>\n",
       "      <td>want</td>\n",
       "      <td>caving</td>\n",
       "      <td>unknowingly</td>\n",
       "      <td>untrained</td>\n",
       "      <td>pardon</td>\n",
       "      <td>o'clock</td>\n",
       "      <td>bases</td>\n",
       "      <td>years</td>\n",
       "      <td>troublemaker</td>\n",
       "      <td>pettiness</td>\n",
       "      <td>gestured</td>\n",
       "      <td>modified</td>\n",
       "      <td>perks</td>\n",
       "      <td>furloughed</td>\n",
       "      <td>voicing</td>\n",
       "      <td>aita??edit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>dripping</td>\n",
       "      <td>going</td>\n",
       "      <td>smoothies</td>\n",
       "      <td>prominent</td>\n",
       "      <td>besties</td>\n",
       "      <td>frustrates</td>\n",
       "      <td>titles</td>\n",
       "      <td>passage</td>\n",
       "      <td>bash</td>\n",
       "      <td>owl</td>\n",
       "      <td>arise</td>\n",
       "      <td>41f</td>\n",
       "      <td>nutritionist</td>\n",
       "      <td>speculation</td>\n",
       "      <td>hurried</td>\n",
       "      <td>tightly</td>\n",
       "      <td>merge</td>\n",
       "      <td>reopen</td>\n",
       "      <td>thirdly</td>\n",
       "      <td>viewpoints</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>tile</td>\n",
       "      <td>peculiar</td>\n",
       "      <td>gourmet</td>\n",
       "      <td>noting</td>\n",
       "      <td>appealing</td>\n",
       "      <td>besties</td>\n",
       "      <td>wierd</td>\n",
       "      <td>dang</td>\n",
       "      <td>desi</td>\n",
       "      <td>4/5</td>\n",
       "      <td>brink</td>\n",
       "      <td>50f</td>\n",
       "      <td>chews</td>\n",
       "      <td>know</td>\n",
       "      <td>choking</td>\n",
       "      <td>fade</td>\n",
       "      <td>profits</td>\n",
       "      <td>chaotic</td>\n",
       "      <td>rebel</td>\n",
       "      <td>provoked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>scraping</td>\n",
       "      <td>instinctively</td>\n",
       "      <td>coffees</td>\n",
       "      <td>instrument</td>\n",
       "      <td>60f</td>\n",
       "      <td>lemme</td>\n",
       "      <td>void</td>\n",
       "      <td>barked</td>\n",
       "      <td>newfound</td>\n",
       "      <td>9:00</td>\n",
       "      <td>screened</td>\n",
       "      <td>f20</td>\n",
       "      <td>uti</td>\n",
       "      <td>blindly</td>\n",
       "      <td>sans</td>\n",
       "      <td>tasteless</td>\n",
       "      <td>speculation</td>\n",
       "      <td>thirdly</td>\n",
       "      <td>frown</td>\n",
       "      <td>eta2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>pees</td>\n",
       "      <td>aggravating</td>\n",
       "      <td>wraps</td>\n",
       "      <td>frown</td>\n",
       "      <td>believable</td>\n",
       "      <td>persona</td>\n",
       "      <td>subscribers</td>\n",
       "      <td>m16</td>\n",
       "      <td>aussie</td>\n",
       "      <td>2:30</td>\n",
       "      <td>know</td>\n",
       "      <td>fraternal</td>\n",
       "      <td>abdominal</td>\n",
       "      <td>indecent</td>\n",
       "      <td>like</td>\n",
       "      <td>dinosaurs</td>\n",
       "      <td>wired</td>\n",
       "      <td>postponing</td>\n",
       "      <td>strikes</td>\n",
       "      <td>it-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>rubbish</td>\n",
       "      <td>believable</td>\n",
       "      <td>pepsi</td>\n",
       "      <td>robotics</td>\n",
       "      <td>regretful</td>\n",
       "      <td>caveat</td>\n",
       "      <td>caving</td>\n",
       "      <td>said</td>\n",
       "      <td>singular</td>\n",
       "      <td>laurie</td>\n",
       "      <td>overdose</td>\n",
       "      <td>info-</td>\n",
       "      <td>believable</td>\n",
       "      <td>degenerate</td>\n",
       "      <td>punches</td>\n",
       "      <td>vampire</td>\n",
       "      <td>scammed</td>\n",
       "      <td>doses</td>\n",
       "      <td>ghosts</td>\n",
       "      <td>correction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>shards</td>\n",
       "      <td>stabbed</td>\n",
       "      <td>pint</td>\n",
       "      <td>targets</td>\n",
       "      <td>i(26f</td>\n",
       "      <td>info-</td>\n",
       "      <td>celebrities</td>\n",
       "      <td>hammered</td>\n",
       "      <td>ness</td>\n",
       "      <td>medicines</td>\n",
       "      <td>fertile</td>\n",
       "      <td>m26</td>\n",
       "      <td>aches</td>\n",
       "      <td>bruh</td>\n",
       "      <td>ro</td>\n",
       "      <td>persona</td>\n",
       "      <td>maxed</td>\n",
       "      <td>graves</td>\n",
       "      <td>souls</td>\n",
       "      <td>educating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>lounging</td>\n",
       "      <td>amita</td>\n",
       "      <td>marshmallows</td>\n",
       "      <td>harvard</td>\n",
       "      <td>crybaby</td>\n",
       "      <td>bases</td>\n",
       "      <td>elses</td>\n",
       "      <td>fetch</td>\n",
       "      <td>m16</td>\n",
       "      <td>sneaked</td>\n",
       "      <td>diagnoses</td>\n",
       "      <td>despises</td>\n",
       "      <td>diligent</td>\n",
       "      <td>condoning</td>\n",
       "      <td>strapped</td>\n",
       "      <td>oversized</td>\n",
       "      <td>bases</td>\n",
       "      <td>hyped</td>\n",
       "      <td>recollection</td>\n",
       "      <td>warrants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>zip</td>\n",
       "      <td>visitor</td>\n",
       "      <td>platter</td>\n",
       "      <td>gamble</td>\n",
       "      <td>transpired</td>\n",
       "      <td>tasked</td>\n",
       "      <td>approving</td>\n",
       "      <td>told</td>\n",
       "      <td>strikes</td>\n",
       "      <td>hurried</td>\n",
       "      <td>specialists</td>\n",
       "      <td>triplet</td>\n",
       "      <td>scarred</td>\n",
       "      <td>stabbed</td>\n",
       "      <td>wrench</td>\n",
       "      <td>like</td>\n",
       "      <td>payday</td>\n",
       "      <td>jab</td>\n",
       "      <td>lemme</td>\n",
       "      <td>i?edit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>crystals</td>\n",
       "      <td>speculation</td>\n",
       "      <td>pesto</td>\n",
       "      <td>speculation</td>\n",
       "      <td>parting</td>\n",
       "      <td>flaw</td>\n",
       "      <td>secretive</td>\n",
       "      <td>patrol</td>\n",
       "      <td>fathom</td>\n",
       "      <td>day</td>\n",
       "      <td>prematurely</td>\n",
       "      <td>m21</td>\n",
       "      <td>crippling</td>\n",
       "      <td>poisoned</td>\n",
       "      <td>intersection</td>\n",
       "      <td>provocative</td>\n",
       "      <td>negotiated</td>\n",
       "      <td>hunts</td>\n",
       "      <td>dont</td>\n",
       "      <td>reddit?edit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>strips</td>\n",
       "      <td>giggle</td>\n",
       "      <td>loaf</td>\n",
       "      <td>practitioner</td>\n",
       "      <td>fixated</td>\n",
       "      <td>moot</td>\n",
       "      <td>amita</td>\n",
       "      <td>know</td>\n",
       "      <td>souls</td>\n",
       "      <td>justifying</td>\n",
       "      <td>appealing</td>\n",
       "      <td>m20</td>\n",
       "      <td>outbursts</td>\n",
       "      <td>enabler</td>\n",
       "      <td>umbrella</td>\n",
       "      <td>outline</td>\n",
       "      <td>6k</td>\n",
       "      <td>fo</td>\n",
       "      <td>ness</td>\n",
       "      <td>funnily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>suitcase</td>\n",
       "      <td>f\\*\\*\\</td>\n",
       "      <td>chilli</td>\n",
       "      <td>appealing</td>\n",
       "      <td>prob</td>\n",
       "      <td>justifying</td>\n",
       "      <td>accurately</td>\n",
       "      <td>time</td>\n",
       "      <td>dots</td>\n",
       "      <td>decreased</td>\n",
       "      <td>wierd</td>\n",
       "      <td>m24</td>\n",
       "      <td>fibromyalgia</td>\n",
       "      <td>intrude</td>\n",
       "      <td>time</td>\n",
       "      <td>unlucky</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>m16</td>\n",
       "      <td>prodding</td>\n",
       "      <td>things:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>plushies</td>\n",
       "      <td>tenth</td>\n",
       "      <td>salads</td>\n",
       "      <td>entails</td>\n",
       "      <td>objections</td>\n",
       "      <td>x200b;the</td>\n",
       "      <td>administrator</td>\n",
       "      <td>tender</td>\n",
       "      <td>ontario</td>\n",
       "      <td>dang</td>\n",
       "      <td>morbid</td>\n",
       "      <td>pardon</td>\n",
       "      <td>productivity</td>\n",
       "      <td>approving</td>\n",
       "      <td>fooling</td>\n",
       "      <td>jumper</td>\n",
       "      <td>told</td>\n",
       "      <td>chicago</td>\n",
       "      <td>persona</td>\n",
       "      <td>blindly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>splashed</td>\n",
       "      <td>fooling</td>\n",
       "      <td>scoops</td>\n",
       "      <td>furloughed</td>\n",
       "      <td>ricky</td>\n",
       "      <td>increases</td>\n",
       "      <td>prominent</td>\n",
       "      <td>pits</td>\n",
       "      <td>russia</td>\n",
       "      <td>sans</td>\n",
       "      <td>p.</td>\n",
       "      <td>philippines</td>\n",
       "      <td>increases</td>\n",
       "      <td>ik</td>\n",
       "      <td>peculiar</td>\n",
       "      <td>bases</td>\n",
       "      <td>tasked</td>\n",
       "      <td>suprised</td>\n",
       "      <td>arise</td>\n",
       "      <td>here?&amp;#x200b;edit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>stinky</td>\n",
       "      <td>shrieked</td>\n",
       "      <td>foil</td>\n",
       "      <td>info-</td>\n",
       "      <td>envious</td>\n",
       "      <td>progressing</td>\n",
       "      <td>lewd</td>\n",
       "      <td>info-</td>\n",
       "      <td>peculiar</td>\n",
       "      <td>work</td>\n",
       "      <td>multiples</td>\n",
       "      <td>f33</td>\n",
       "      <td>brink</td>\n",
       "      <td>traitor</td>\n",
       "      <td>marched</td>\n",
       "      <td>septum</td>\n",
       "      <td>60/40</td>\n",
       "      <td>culminated</td>\n",
       "      <td>trolling</td>\n",
       "      <td>lemme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>seal</td>\n",
       "      <td>frightened</td>\n",
       "      <td>kfc</td>\n",
       "      <td>titles</td>\n",
       "      <td>culminated</td>\n",
       "      <td>vendetta</td>\n",
       "      <td>flooding</td>\n",
       "      <td>it‘s</td>\n",
       "      <td>know</td>\n",
       "      <td>clocked</td>\n",
       "      <td>distressing</td>\n",
       "      <td>44f</td>\n",
       "      <td>traumas</td>\n",
       "      <td>retaliated</td>\n",
       "      <td>furiously</td>\n",
       "      <td>lace</td>\n",
       "      <td>fo</td>\n",
       "      <td>want</td>\n",
       "      <td>classist</td>\n",
       "      <td>faq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>puddle</td>\n",
       "      <td>intrude</td>\n",
       "      <td>satisfying</td>\n",
       "      <td>inflation</td>\n",
       "      <td>14th</td>\n",
       "      <td>bitterly</td>\n",
       "      <td>singular</td>\n",
       "      <td>unlucky</td>\n",
       "      <td>slang</td>\n",
       "      <td>culminated</td>\n",
       "      <td>thirdly</td>\n",
       "      <td>m33</td>\n",
       "      <td>flaw</td>\n",
       "      <td>lash</td>\n",
       "      <td>flashed</td>\n",
       "      <td>scarred</td>\n",
       "      <td>affording</td>\n",
       "      <td>wierd</td>\n",
       "      <td>peculiar</td>\n",
       "      <td>commenter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cluster 1      Cluster 2     Cluster 3     Cluster 4       Cluster 5  \\\n",
       "0     scrubbing           said       steamed   conferences          laurie   \n",
       "1      scrubbed           like        frying         elite            told   \n",
       "2      fountain    embarassing       caramel     economics           i(28f   \n",
       "3          tide           told      macaroni           emt             m16   \n",
       "4    toothpaste            jab      burritos  inconsistent            said   \n",
       "5        canvas          frown       refried          dang       seventeen   \n",
       "6      perfumes         barked       oatmeal       thirdly           wierd   \n",
       "7         sippy           whip      seasoned      skillset  granddaughters   \n",
       "8   aggravating           prob        edible      delaying         delight   \n",
       "9        rubber       prodding      sausages         i(28f            like   \n",
       "10  rearranging        choking     hamburger  architecture        bitterly   \n",
       "11       sponge           bruh        almond       interns            carl   \n",
       "12       edible           know       berries       collage            know   \n",
       "13      closets         bummer      toppings   credentials            theo   \n",
       "14       usable        strikes    hamburgers         stats             jab   \n",
       "15     hammered    patronizing         chops     academics              n.   \n",
       "16      chucked        blindly        noodle    statistics            time   \n",
       "17      drywall     bewildered      parmesan       majored             mom   \n",
       "18         ikea            awe      sprinkle    leadership         husband   \n",
       "19      doubles         'cause         cider    associates          sister   \n",
       "20        sinks     justifying        crisps       academy        prodding   \n",
       "21     silicone            yah       pudding      begrudge         remarry   \n",
       "22       wrench        crybaby    nutritious    preferably            i(27   \n",
       "23       frying  uncomfortably      inedible     sponsored         marries   \n",
       "24       unplug         fuckin        sauces         perks     speculation   \n",
       "25      sneaked     frustrates   cauliflower      enlisted            wife   \n",
       "26         whip           wolf         herbs        reopen           going   \n",
       "27     peculiar        ribbing       dominos    apprentice     festivities   \n",
       "28       vienna       hammered     milkshake    unreliable            i(28   \n",
       "29       spoons        fixated      shredded         pilot           think   \n",
       "30        vents          think         jelly          flaw         funnily   \n",
       "31       razors          piped      eggplant      buisness        enlisted   \n",
       "32     tripping    unknowingly       tenders        tasked      grudgingly   \n",
       "33         like         dawned         gravy    overworked            want   \n",
       "34     dripping          going     smoothies     prominent         besties   \n",
       "35         tile       peculiar       gourmet        noting       appealing   \n",
       "36     scraping  instinctively       coffees    instrument             60f   \n",
       "37         pees    aggravating         wraps         frown      believable   \n",
       "38      rubbish     believable         pepsi      robotics       regretful   \n",
       "39       shards        stabbed          pint       targets           i(26f   \n",
       "40     lounging          amita  marshmallows       harvard         crybaby   \n",
       "41          zip        visitor       platter        gamble      transpired   \n",
       "42     crystals    speculation         pesto   speculation         parting   \n",
       "43       strips         giggle          loaf  practitioner         fixated   \n",
       "44     suitcase         f\\*\\*\\        chilli     appealing            prob   \n",
       "45     plushies          tenth        salads       entails      objections   \n",
       "46     splashed        fooling        scoops    furloughed           ricky   \n",
       "47       stinky       shrieked          foil         info-         envious   \n",
       "48         seal     frightened           kfc        titles      culminated   \n",
       "49       puddle        intrude    satisfying     inflation            14th   \n",
       "\n",
       "       Cluster 6      Cluster 7     Cluster 8    Cluster 9     Cluster 10  \\\n",
       "0           told       linkedin          wolf         arab           10pm   \n",
       "1           like          tweet       leashed  philippines        winding   \n",
       "2           said        thirdly          sans      nigeria       tuesdays   \n",
       "3           time    speculation   speculation       norway           time   \n",
       "4           know  contemplating     wandering        i(28f     decompress   \n",
       "5           want          \"that         fires    catholics        infants   \n",
       "6          wierd        caution        angels       sweden           whip   \n",
       "7    observation      reposting    homeowners      eastern        doubles   \n",
       "8          going           told          like        ultra        drowned   \n",
       "9          think          bases      peculiar       poorer     frustrates   \n",
       "10     laughable           know      umbrella  evangelical           like   \n",
       "11   speculation         noting          whip        info-         barked   \n",
       "12       thirdly       warrants        'cause     scotland     television   \n",
       "13     appealing     referenced        autumn      stumped         dawned   \n",
       "14        scummy           said         mowed      crosses       snuggled   \n",
       "15        laurie      laughable          dock          era            3am   \n",
       "16         i(28f        crosses      sunlight      soldier      reasons:1   \n",
       "17  inconsistent    highlighted       chaotic  nationality           told   \n",
       "18         amita        fooling       ontario      adapted           wolf   \n",
       "19       blindly         peaked         i(28f       asians          hyped   \n",
       "20         frown    distressing         willy         iran  uninterrupted   \n",
       "21         spiel          info-    accustomed  unknowingly            8pm   \n",
       "22        gamble           like        dawned    prejudice           said   \n",
       "23      delaying     justifying   observation        wierd         stunts   \n",
       "24         elses          trace     increases      vietnam           prob   \n",
       "25       intrude         dawned       visitor   uneducated    aggravating   \n",
       "26        misled      reprimand         lawns    manhattan        ungodly   \n",
       "27    unreliable            gps  rambunctious       origin        intrude   \n",
       "28      endanger      cognizant        stinky      visitor           6:45   \n",
       "29         arise   transactions         goofy      baptist         bummer   \n",
       "30       parting    observation      fountain       boston          i(28f   \n",
       "31    prioritise         erased     unleashed      sinners           8:00   \n",
       "32        aita?i           bait         amita      funnily          going   \n",
       "33        caving    unknowingly     untrained       pardon        o'clock   \n",
       "34    frustrates         titles       passage         bash            owl   \n",
       "35       besties          wierd          dang         desi            4/5   \n",
       "36         lemme           void        barked     newfound           9:00   \n",
       "37       persona    subscribers           m16       aussie           2:30   \n",
       "38        caveat         caving          said     singular         laurie   \n",
       "39         info-    celebrities      hammered         ness      medicines   \n",
       "40         bases          elses         fetch          m16        sneaked   \n",
       "41        tasked      approving          told      strikes        hurried   \n",
       "42          flaw      secretive        patrol       fathom            day   \n",
       "43          moot          amita          know        souls     justifying   \n",
       "44    justifying     accurately          time         dots      decreased   \n",
       "45     x200b;the  administrator        tender      ontario           dang   \n",
       "46     increases      prominent          pits       russia           sans   \n",
       "47   progressing           lewd         info-     peculiar           work   \n",
       "48      vendetta       flooding          it‘s         know        clocked   \n",
       "49      bitterly       singular       unlucky        slang     culminated   \n",
       "\n",
       "       Cluster 11   Cluster 12     Cluster 13   Cluster 14    Cluster 15  \\\n",
       "0             uti         i(28         anemia      crybaby        puddle   \n",
       "1       prognosis        i(28f      therapies        amita       inspect   \n",
       "2     contracting         i(27         stunts         told        honked   \n",
       "3       therapies          m16          moods       caving      shrieked   \n",
       "4     observation          f35    observation         said      hammered   \n",
       "5             mri          f34      medicated     dictator      tripping   \n",
       "6         sterile        i(27f      decreased         like     shoveling   \n",
       "7             hpv          51f      prolonged    oversight       smacked   \n",
       "8    testosterone          46f     irritation        wierd      vicinity   \n",
       "9           spike          f15           wolf    delusions           gps   \n",
       "10     believable    seventeen  deteriorating        \"aita        barked   \n",
       "11        vaginal          m35            mri      fixated          whip   \n",
       "12      succeeded          f29       fainting   justifying       mirrors   \n",
       "13           coma        i(26f           ache    sociopath           zip   \n",
       "14     objections        i(24f      delusions        think       chucked   \n",
       "15      pressures          f17      pressures  disgraceful        unplug   \n",
       "16  deteriorating          48f         spells     prodding        'cause   \n",
       "17             sh          m37      diagnoses     meddling          wolf   \n",
       "18  dermatologist          f36    psychiatric  altercation        unload   \n",
       "19      regretful          60f             sh      harsher         coats   \n",
       "20   vaccinations          49f         dawned       ganged          tile   \n",
       "21   complication           ~3          arise      racists   speculation   \n",
       "22       examined          m36          xanax     classist        stunts   \n",
       "23       operated        doozy     overworked       'cause          prob   \n",
       "24      delusions          52f     autoimmune      ganging          told   \n",
       "25           told          m22       stemming          jab      fountain   \n",
       "26         dawned           3y          spike       b\\*tch       scooter   \n",
       "27        entails          47f         tender        souls         squad   \n",
       "28      seventeen          m23        thyroid    deceptive         tying   \n",
       "29       urgently          m34          lemme       scummy      pizzeria   \n",
       "30      certainty          m25        fatigue     tricking       visitor   \n",
       "31      laughable          f30        targets         \"i'm          said   \n",
       "32       checkups           1f       screened    disowning           rhe   \n",
       "33          bases        years   troublemaker    pettiness      gestured   \n",
       "34          arise          41f   nutritionist  speculation       hurried   \n",
       "35          brink          50f          chews         know       choking   \n",
       "36       screened          f20            uti      blindly          sans   \n",
       "37           know    fraternal      abdominal     indecent          like   \n",
       "38       overdose        info-     believable   degenerate       punches   \n",
       "39        fertile          m26          aches         bruh            ro   \n",
       "40      diagnoses     despises       diligent    condoning      strapped   \n",
       "41    specialists      triplet        scarred      stabbed        wrench   \n",
       "42    prematurely          m21      crippling     poisoned  intersection   \n",
       "43      appealing          m20      outbursts      enabler      umbrella   \n",
       "44          wierd          m24   fibromyalgia      intrude          time   \n",
       "45         morbid       pardon   productivity    approving       fooling   \n",
       "46             p.  philippines      increases           ik      peculiar   \n",
       "47      multiples          f33          brink      traitor       marched   \n",
       "48    distressing          44f        traumas   retaliated     furiously   \n",
       "49        thirdly          m33           flaw         lash       flashed   \n",
       "\n",
       "        Cluster 16    Cluster 17    Cluster 18    Cluster 19  \\\n",
       "0           looser          500k   festivities         amita   \n",
       "1             neon           65k         i(28f         wierd   \n",
       "2         cardigan           aud        laurie          like   \n",
       "3             pins         6,000          14th         think   \n",
       "4       hairstyles        25,000     newlyweds        fathom   \n",
       "5           rubber       bitcoin         going     laughable   \n",
       "6            horns       revenue          told       blindly   \n",
       "7          unicorn      biweekly        autumn       funnily   \n",
       "8          vibrant        recoup          time   observation   \n",
       "9        eccentric       airfare          said          know   \n",
       "10        pictured       100,000   speculation          flaw   \n",
       "11         fabrics     luxurious     impromptu           jab   \n",
       "12         patches   millionaire     luxurious        scummy   \n",
       "13        jumpsuit        60,000          50th    invalidate   \n",
       "14            lacy        exceed      informal   unknowingly   \n",
       "15           layer           12k        bummer       besties   \n",
       "16      sleeveless  inconsistent          dibs  presumptuous   \n",
       "17         braided     estimates         spree       blinded   \n",
       "18           booty        50,000        fussed       fixated   \n",
       "19            curl      totaling         hitch       tactics   \n",
       "20            wavy            7k   christening   speculation   \n",
       "21          sizing        agents          bffs      vendetta   \n",
       "22           fuzzy          250k  availability      weirdest   \n",
       "23       necklaces        40,000          like         \"aita   \n",
       "24          violet        halves         i(24f  troublemaker   \n",
       "25             awe         4,000     itinerary     delusions   \n",
       "26        rainbows     laughable          16th          \"i'm   \n",
       "27  conservatively           90k        greece          said   \n",
       "28            gray     inflation          know       crosses   \n",
       "29          plunge     destitute      barbeque      pictured   \n",
       "30            buff        vienna      buisness         spiel   \n",
       "31        peculiar       thirdly         frown          told   \n",
       "32           heals       overdue       mallory         noble   \n",
       "33        modified         perks    furloughed       voicing   \n",
       "34         tightly         merge        reopen       thirdly   \n",
       "35            fade       profits       chaotic         rebel   \n",
       "36       tasteless   speculation       thirdly         frown   \n",
       "37       dinosaurs         wired    postponing       strikes   \n",
       "38         vampire       scammed         doses        ghosts   \n",
       "39         persona         maxed        graves         souls   \n",
       "40       oversized         bases         hyped  recollection   \n",
       "41            like        payday           jab         lemme   \n",
       "42     provocative    negotiated         hunts          dont   \n",
       "43         outline            6k            fo          ness   \n",
       "44         unlucky    unreliable           m16      prodding   \n",
       "45          jumper          told       chicago       persona   \n",
       "46           bases        tasked      suprised         arise   \n",
       "47          septum         60/40    culminated      trolling   \n",
       "48            lace            fo          want      classist   \n",
       "49         scarred     affording         wierd      peculiar   \n",
       "\n",
       "                        Cluster 20  \n",
       "0                                😊  \n",
       "1                           sooooo  \n",
       "2                   clarifications  \n",
       "3                         trolling  \n",
       "4                           inputs  \n",
       "5                      here?update  \n",
       "6                              p.s  \n",
       "7                                😅  \n",
       "8              aita?&#x200b;update  \n",
       "9                          me?edit  \n",
       "10                       reposting  \n",
       "11  post](https://www.reddit.com_r  \n",
       "12                         caution  \n",
       "13                     informative  \n",
       "14                         voicing  \n",
       "15                         blinded  \n",
       "16          amitheasshole_comments  \n",
       "17                     subscribers  \n",
       "18                     observation  \n",
       "19                          pardon  \n",
       "20                      wrong?edit  \n",
       "21                           pming  \n",
       "22                               😁  \n",
       "23                        thorough  \n",
       "24                         thirdly  \n",
       "25                        out?edit  \n",
       "26                           wierd  \n",
       "27                      normalized  \n",
       "28               misunderstandings  \n",
       "29                     unknowingly  \n",
       "30                       downvoted  \n",
       "31                         advices  \n",
       "32                   contemplating  \n",
       "33                      aita??edit  \n",
       "34                      viewpoints  \n",
       "35                        provoked  \n",
       "36                            eta2  \n",
       "37                             it-  \n",
       "38                      correction  \n",
       "39                       educating  \n",
       "40                        warrants  \n",
       "41                          i?edit  \n",
       "42                     reddit?edit  \n",
       "43                         funnily  \n",
       "44                        things:1  \n",
       "45                         blindly  \n",
       "46               here?&#x200b;edit  \n",
       "47                           lemme  \n",
       "48                             faq  \n",
       "49                       commenter  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_words(model, 50, centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EF6bB0rVReh"
   },
   "source": [
    "## Visualizing High Dimensional Spaces with $t$-SNE\n",
    "\n",
    "The word embeddings we created are what's called a **high-dimensional representation** of the text. That is, we take a word in the corpus, and represent it using, in this case, 300 numbers. We can plot 3 numbers at a time - that's in 3 dimensions - but there's no way for humans to visualize something in a 300-dimensional space. \n",
    "\n",
    "So, **dimensionality reduction** is a big part of machine learning. How can we take vectors that are 300-dimensional, and visualize them in 2-dimensions, while keeping the structure between vectors the same? How can we reduce the dimensionality?\n",
    "\n",
    "One of the most popular methods for dimensionality reduction is called $t$-SNE ($t$-Distributed Stochastic Neighbor Embedding). The details are not important, but using it in practice is a useful skill to learn (if you want to read more, [here](https://lvdmaaten.github.io/tsne/) is a good starting point). Roughly, it tries to keep the relative distances between points as closely as possible in both high-dimensional and low-dimensional space.\n",
    "\n",
    "So, we'll use $t$-SNE to take all the word vectors, and obtain a **low dimensional representation**. We can then visualize it, which may reveal semantic and syntactic trends in the data.\n",
    "\n",
    "A [$t$-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html?highlight=tsne#sklearn.manifold.TSNE) implementation is available via `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "5pri9cdRfHlJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "jd60bqFxe9z1"
   },
   "outputs": [],
   "source": [
    "# Create some filepaths to save our model\n",
    "tsne_path = 'tsne_model'\n",
    "tsne_vectors_path = 'tsne_vectors.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U4'), dtype('float32')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-b1bc4d60e0c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtsne_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \"\"\"\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    796\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                           skip_num_points=skip_num_points)\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     def _tsne(self, P, degrees_of_freedom, n_samples, X_embedded,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0mP\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_exaggeration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m         params, kl_divergence, it = _gradient_descent(obj_func, params,\n\u001b[0;32m--> 837\u001b[0;31m                                                       **opt_args)\n\u001b[0m\u001b[1;32m    838\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m             print(\"[t-SNE] KL divergence after %d iterations with early \"\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_gain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mgains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U4'), dtype('float32')) -> None"
     ]
    }
   ],
   "source": [
    "tsne = TSNE(init='pca', learning_rate='auto')\n",
    "tsne_vectors = tsne.fit_transform(model.wv.vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our low dimensional representation. Now, let's store the 2 dimensions in a dataframe, with the word as the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tsne_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-f83fcff134ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Store the t-SNE vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m tsne_vectors = pd.DataFrame(tsne_vectors,\n\u001b[0m\u001b[1;32m      3\u001b[0m                             \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             columns=['x', 'y'])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tsne_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "# Store the t-SNE vectors\n",
    "tsne_vectors = pd.DataFrame(tsne_vectors,\n",
    "                            index=pd.Index(model.wv.index_to_key),\n",
    "                            columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "QBe7lNE3e7oQ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tsne_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-41429753e7b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtsne_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsne_vectors_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tsne_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "with open(tsne_path, 'wb') as f:\n",
    "    pickle.dump(tsne, f)\n",
    "\n",
    "tsne_vectors.to_pickle(tsne_vectors_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a convenient code block to load this data, to start from this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tsne_vectors.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-f2213a4ed68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtsne_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsne_vectors_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     ) as handles:\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tsne_vectors.pkl'"
     ]
    }
   ],
   "source": [
    "with open(tsne_path, 'rb') as f:\n",
    "    tsne = pickle.load(f)\n",
    "    \n",
    "tsne_vectors = pd.read_pickle(tsne_vectors_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to visualize the 2-dimensional space using a package called `bokeh`. This package is nice for this because it allows for some degree of interactivity: we can go over each point and dynamically get information about the word denoting that vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bokeh in /Users/emilygrabowski/opt/anaconda3/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already satisfied: six>=1.5.2 in /Users/emilygrabowski/opt/anaconda3/lib/python3.7/site-packages (from bokeh) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/emilygrabowski/opt/anaconda3/lib/python3.7/site-packages (from bokeh) (2.8.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /Users/emilygrabowski/opt/anaconda3/lib/python3.7/site-packages (from bokeh) (5.3)\n",
      "Requirement already satisfied: packaging>=16.8 in /Users/emilygrabowski/opt/anaconda3/lib/python3.7/site-packages (from bokeh) (21.3)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /Users/emilygrabowski/opt/anaconda3/lib/python3.7/site-packages (from bokeh) (2.11.1)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /Users/emilygrabowski/opt/anaconda3/lib/python3.7/site-packages (from bokeh) (1.21.5)\n",
      "Requirement already satisfied: pillow>=4.0 in /Users/emilygrabowski/opt/anaconda3/lib/python3.7/site-packages (from bokeh) (9.1.1)\n",
      "Requirement already satisfied: tornado>=4.3 in /Users/emilygrabowski/opt/anaconda3/lib/python3.7/site-packages (from bokeh) (6.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/emilygrabowski/opt/anaconda3/lib/python3.7/site-packages (from Jinja2>=2.7->bokeh) (1.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/emilygrabowski/opt/anaconda3/lib/python3.7/site-packages (from packaging>=16.8->bokeh) (2.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AE4-OrmKmKG"
   },
   "outputs": [],
   "source": [
    "import bokeh\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "\n",
    "output_notebook()\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "executionInfo": {
     "elapsed": 747,
     "status": "ok",
     "timestamp": 1640017607454,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "myJtActVdjn4",
    "outputId": "5f283a90-c0f4-4c31-88ea-5f7bfa6917ee"
   },
   "outputs": [],
   "source": [
    "# Add our DataFrame as a ColumnDataSource for Bokeh\n",
    "plot_data = ColumnDataSource(tsne_vectors)\n",
    "\n",
    "# Create the plot and configure the title, dimensions, and tools\n",
    "tsne_plot = figure(title='t-SNE Word Embeddings',\n",
    "                   plot_width=800,\n",
    "                   plot_height=800)\n",
    "\n",
    "# Add a hover tool to display words on roll-over\n",
    "tsne_plot.add_tools(HoverTool(tooltips='@index') )\n",
    "\n",
    "# Draw the words as circles on the plot\n",
    "tsne_plot.circle('x', 'y',\n",
    "                 source=plot_data,\n",
    "                 color='blue',\n",
    "                 line_alpha=0.2,\n",
    "                 fill_alpha=0.1,\n",
    "                 size=10,\n",
    "                 hover_line_color='black')\n",
    "\n",
    "# Configure visual elements of the plot\n",
    "tsne_plot.xaxis.visible = False\n",
    "tsne_plot.yaxis.visible = False\n",
    "tsne_plot.grid.grid_line_color = None\n",
    "tsne_plot.outline_line_color = None\n",
    "\n",
    "# Engage!\n",
    "show(tsne_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection: The Hermeneutics of Word Embeddings\n",
    "\n",
    "“In vector space, identities and differences change in nature. Similarity and belonging no longer rely on resemblance or a common genesis but on measures of proximity or distance, on flat loci that run as vectors through the space.” (Dourish 2018: 73-4)\n",
    "\n",
    "As we've seen, word embeddings are essentially a set of vectors. We should reflect on this. What is vectorization? It is reducing linguistic complexity. Or rather, it produces a common space that juxtaposes and mixes complex localized realities. Anything can be turned into a vector operation, but what do we lose when doing so? "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 4-1 Word Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
