{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Uds2RtAZOog"
   },
   "source": [
    "# Data Science for Social Justice Workshop: Module 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfjNP33MZOoi"
   },
   "source": [
    "## Word Embeddings\n",
    "\n",
    "In this notebook, we'll work with word embeddings using `gensim`.\n",
    "\n",
    "The goal of word embedding models is to learn a **numerical representation** of a text corpus. We already did that to a certain extent when we did topic modeling. In this case, we're going to be more explicit about how we construct that numerical representation: for each word, we're going to find a **vector** of numbers to represent it. The actual numbers themselves won't be meaningful to us as humans. However, if successful, the vectors for each term should encode information about the meaning or concept the term represents, as well as the relationship between it and other terms in the vocabulary.\n",
    "\n",
    "Word vector models are fully unsupervised: they learn all of these meanings and relationships without any advance knowledge. Unsupervised learning requires the specification of a right task. We won't go into detail in this lesson, but you can roughly think of the  Read [this post](https://tomvannuenen.medium.com/analyzing-reddit-communities-with-python-part-6-word-embeddings-f92bba876d60) for a deeper introduction to word embeddings.\n",
    "\n",
    "This notebook is designed to help you:\n",
    "\n",
    "* Use `gensim`'s `word2vec` method to create word vectors for a corpus;\n",
    "* Use these word vectors to reflect on implicit binaries and normativities in your data;\n",
    "* Visualize topic models using K-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "As we will be considering the language biases in the next notebook, we will use the comments of our subreddit this time. The thinking behind this is that this data will be derived from more people, and include more evaluative statements (after all, comments on r/amitheasshole generally evaluate the original posts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory\n",
    "# We include two ../ because we want to go two levels up in the file structure\n",
    "os.chdir(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5723,
     "status": "ok",
     "timestamp": 1639942437840,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "bIB1SsShP9iL",
    "outputId": "01fa486d-4a39-48ef-f6f1-076478c6e0fc"
   },
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "df = pd.read_csv('aita_com_top.csv')\n",
    "df.head(3)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remove comments that were removed or deleted, and additionally only take comments that are sufficiently long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1639942442529,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "B2igBxlIu7Hw",
    "outputId": "806927f1-bd1e-4517-ec05-a45773bb92d8"
   },
   "outputs": [],
   "source": [
    "# Remove comments that are [removed] or [deleted]\n",
    "df = df[~df['body'].isin(['[removed]', '[deleted]'])].dropna(subset=['body'])\n",
    "# Remove comments less than 15 characters long\n",
    "df = df[df['body'].str.len() >= 15]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll import `spacy` and `gensim` to do some preprocessing. We have functions written here for you to help streamline the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = nlp('You are being a very bad dog, mister.')\n",
    "print(parsed)\n",
    "\n",
    "for token in parsed:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wt4BIM-cfhK3"
   },
   "outputs": [],
   "source": [
    "def clean(token):\n",
    "    \"\"\"Helper function that specifies whether a token is:\n",
    "        - punctuation\n",
    "        - space\n",
    "        - digit\n",
    "    \"\"\"\n",
    "    return token.is_punct or token.is_space or token.is_digit\n",
    "\n",
    "def line_read(df, text_col='body'):\n",
    "    \"\"\"\n",
    "    Generator function to read in text from df and get rid of line breaks.\n",
    "    \"\"\"    \n",
    "    for text in df['body']:\n",
    "        yield text.replace('\\n', '')\n",
    "\n",
    "def preprocess(df, allowed_postags=['NOUN', 'ADJ']):\n",
    "    \"\"\"Preprocessing function to apply to a dataframe.\n",
    "    \n",
    "    \"\"\"\n",
    "    for parsed in nlp.pipe(line_read(df), batch_size=1000, disable=[\"tok2vec\", \"ner\"]):\n",
    "        # Gather lowercased, lemmatized tokens, \n",
    "        tokens = [token.lemma_.lower() #if token.lemma_ != '-PRON-'\n",
    "                  #else token.lower_ \n",
    "                  for token in parsed if not clean(token)]\n",
    "        tokens = [lemma\n",
    "                  for lemma in tokens\n",
    "                  if not lemma in [\"'s\",  \"’s\", \"’\"] and not lemma in allowed_postags]\n",
    "        tokens = [token for token in tokens if token not in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "        yield tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the `preprocess()` function to each comment in the dataframe, producing a `docs` output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHOu2vdsfl3Q"
   },
   "outputs": [],
   "source": [
    "docs = [line for line in preprocess(df)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create bi-grams. Bi-grams consist of pairs of words that appear commonly together (e.g., \"New York\"). `gensim` provides some functions to detect bi-grams that appear often enough that we should include them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 126716,
     "status": "ok",
     "timestamp": 1639949141836,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "C4RT1pLBfgaz",
    "outputId": "ae080bf7-fd7b-40a5-df98-79f35ed717aa"
   },
   "outputs": [],
   "source": [
    "# Create bigram model: pass docs into Phrases class\n",
    "bigrams = Phrases(tokens, min_count=20, threshold=300)\n",
    "# Create a \"frozen\" bigram model using the Phraser class\n",
    "bigram_phraser = Phraser(bigrams)\n",
    "# Now, create bigrams \n",
    "docs_bigrams = [bigram_phraser[doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams[docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's nothing stopping us from going further: we can create tri-grams or even $n$-grams. We'll make some tri-grams and build our word2vec model on top of them. A tri-gram can be constructed by simply looking for bi-grams in a bi-grams corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = Phrases(bigrams[docs], min_count=20, threshold=100)  \n",
    "trigram_phraser = Phraser(trigrams)\n",
    "docs_trigrams = [trigram_phraser[doc] for doc in docs_bigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the data to an external JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GS_5VJXogUMn"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('aita_com_top_lemmas.json', 'w') as write:\n",
    "    json.dump(docs_trigrams, write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the same file works as follows:\n",
    "with open(\"aita_com_top_lemmas.json\") as f:\n",
    "    trigrams = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wk2mxX1HZOpk"
   },
   "source": [
    "## Constructing a Word2Vec Model\n",
    "\n",
    "Let's create our word embeddings model. \n",
    "\n",
    "While last week's LDA method was focused on finding topics in a collection of documents (or in our case, submissions), word embeddings models focus on individual words, and learning vector representations of these words.\n",
    "\n",
    "The input to the model is a text corpus split up in sentences – in word embeddings, there is no concept of \"documents\". The model's output is a set of \"vectors\" (one for each word) in N dimensions. Think of these vectors as \"features\", capturing latent meaning.\n",
    "\n",
    "This model allows us to group the vectors of similar words together in vector space. We can then reduce the dimensionality to visualize the results in a way humans can understand (such as in a 2-dimensional space), or to perform linear algebra operations in order to find out to what extent words are related.\n",
    "\n",
    "Word2Vec is one example of a word embeddings model. It learns by taking words and their contexts (e.g. sentences) into account, and can then try to predict other words. Given enough data, usage and contexts, word2vec can make accurate guesses about a word’s meaning based on its appearances. Those guesses can be used to establish a word’s association with other words (e.g. \"Paris\" is to \"France\" as “Berlin” is to “Germany”), or cluster documents and classify them by topic.\n",
    "\n",
    "We now instantiate and train our Word2Vec model, using the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQq117-pZOpl"
   },
   "outputs": [],
   "source": [
    "# Count the number of cores you have at your disposal\n",
    "cores = multiprocessing.cpu_count()\n",
    "# Word vector dimensionality (how many features each word will be given)\n",
    "n_features = 300\n",
    "# Minimum word count to be taken into account\n",
    "min_word_count = 10\n",
    "# Number of threads to run in parallel (equal to your amount of cores)\n",
    "n_workers = cores\n",
    "# Context window size\n",
    "window = 5\n",
    "# Downsample setting for frequent words\n",
    "downsampling = 1e-2\n",
    "# Seed for the random number generator (to create reproducible results)\n",
    "seed = 1 \n",
    "# Skip-gram = 1, CBOW = 0\n",
    "sg = 1\n",
    "epochs = 20\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=trigrams,\n",
    "    workers=num_workers,\n",
    "    vector_size=n_features,\n",
    "    min_count=min_word_count,\n",
    "    window=window,\n",
    "    sample=downsampling,\n",
    "    seed=seed,\n",
    "    sg=sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 282395,
     "status": "ok",
     "timestamp": 1639949578498,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "tjydPTHRbJwR",
    "outputId": "b1fcf929-7255-426a-fc65-58c5747b23be"
   },
   "outputs": [],
   "source": [
    "model.train(trigrams, total_examples=model.corpus_count, epochs=10)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrNV3eYeZOpo"
   },
   "source": [
    "That was it! We have a Word Embeddings model now. Let's save it so that we don't have to train it again. Then, we'll reload the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRX-lzwSzFI5"
   },
   "outputs": [],
   "source": [
    "model.save('aita.emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZMDwWBuGTn7"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('aita.emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWQC4S9IZOpu"
   },
   "source": [
    "How many terms are in our vocabulary? Whenever interacting with the word vector dictionary, we use the `wv` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVdNakwKfm3H"
   },
   "source": [
    "Let's take a peek at the word vectors our model has learned. We can take a look at the individual words using the `index_to_key` attribute, and the word vectors themselves can be accessed with the `vectors` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 1224,
     "status": "ok",
     "timestamp": 1640016826009,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "R6A19I38fjTT",
    "outputId": "e6d18f9d-cd53-4ae6-d17e-7cf353618735"
   },
   "outputs": [],
   "source": [
    "model.wv.index_to_key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at it - it doesn't make a whole lot of sense to us! It's just a bunch of numbers. However, we can do semantic operations on these vectors, such as getting related terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw4th__0ZOp0"
   },
   "source": [
    "### Word Similarity\n",
    "\n",
    "With the information in our word embeddings model, we can try to find similarities between words that interest us (i.e. words that have a similar vector). Let's create a function that retrieves related terms to some input. We're going to use the `most_similar()` function in `gensim` as part of this helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1HVouFqZOp0"
   },
   "outputs": [],
   "source": [
    "def get_most_similar_terms(model, token, topn=20):\n",
    "    \"\"\"Look up the top N most similar terms to the token.\"\"\"\n",
    "    for word, similarity in model.wv.most_similar(positive=[token], topn=topn):\n",
    "        print(f\"{word}: {round(similarity, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1639950052918,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "CDyV-ntGZOp3",
    "outputId": "d72277e1-e4e1-451d-e706-70b7fb7ebb02"
   },
   "outputs": [],
   "source": [
    "get_related_terms('asshole')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some other terms. What else interests you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_related_terms('empathy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_related_terms('relationship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_related_terms('power')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAF3aRsHZOp6"
   },
   "source": [
    "### Word Algebra\n",
    "\n",
    "One of the most famous usages of `word2vec` is via word analogies. For example:\n",
    "\n",
    "`Paris : France :: Berlin : Germany`\n",
    "\n",
    "Here, the analogy is between (Paris, France) and (Berlin, Germany), with \"capital city\" being the concept that connects them. We can abstract the \"analogy\" relationship to vector modeling. Let's pretend we're working with each of the vectors. Then, the analogy is\n",
    "\n",
    "$$\\mathbf{v}_{\\text{France}} - \\mathbf{v}_{\\text{Paris}} \\approx \\mathbf{v}_{\\text{Germany}} - \\mathbf{v}_{\\text{Berlin}}.$$\n",
    "\n",
    "The vector difference here represents the notion of \"capital city\". Presumably, going from the Paris vector to the France vector (i.e., the vector difference) will be the same as going from the Berlin vector to the Germany vector, if that difference carries similar semantic meaning.\n",
    "\n",
    "Let's test this directly. We'll do so by rewriting the above expression:\n",
    "\n",
    "$$\\mathbf{v}_{\\text{France}} - \\mathbf{v}_{\\text{Paris}} + \\mathbf{v}_{\\text{Berlin}} \\approx \\mathbf{v}_{\\text{Germany}}.$$\n",
    "\n",
    "The core idea is that once words are represented as numerical vectors, you can do \"math\" with them. The mathematical procedure works as follows:\n",
    "\n",
    "1. Provide a set of words or phrases you want to add or subtract.\n",
    "2. Look up the vectors that represent those terms in the word vector model.\n",
    "3. Add and subtract those vectors to produce a new, combined vector.\n",
    "4. Look up the most similar vector(s) to this new, combined vector via cosine similarity.\n",
    "5. Return the word(s) associated with the similar vector(s).\n",
    "\n",
    "Let's try it out. We'll create a function that does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8RsoWBAZOp7"
   },
   "outputs": [],
   "source": [
    "def word_algebra(add=[], subtract=[], topn=10):\n",
    "    \"\"\"\n",
    "    combine the vectors associated with the words provided\n",
    "    in add= and subtract=, look up the topn most similar\n",
    "    terms to the combined vector, and print the result(s)\n",
    "    \"\"\"\n",
    "    answers = model.wv.most_similar(positive=add, negative=subtract, topn=topn)\n",
    "    \n",
    "    for term, similarity in answers:\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1639950123172,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "hXhZB8SOZOp9",
    "outputId": "93a8e656-42b6-49ea-eacb-13fff7504f30",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=['men', 'dating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1639944397863,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "ho-F51R3ZOqB",
    "outputId": "e5776148-5783-472b-ceb4-5d0003182dde"
   },
   "outputs": [],
   "source": [
    "word_algebra(add=['women', 'dating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6Y4eXciZOqb"
   },
   "source": [
    "## K-means Clustering\n",
    "\n",
    "One convenience of word embeddings is that we can cluster them using, for instance, K-Means clustering. \n",
    "\n",
    "K-Means clustering aims to partition N observations into K clusters in which each observation belongs to the cluster with the nearest mean (called the \"cluster centre\"), which serves as a prototype of the cluster.\n",
    "\n",
    "Since our words are all represented as vectors, applying K-Means is easy to do since the clustering algorithm will simply look at differences between vectors (and centers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJodQkitZOqb"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def clustering_on_wordvecs(word_vectors, num_clusters):\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++');\n",
    "    idx = kmeans_clustering.fit_predict(word_vectors);\n",
    "    return kmeans_clustering.cluster_centers_, idx;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1639950154155,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "qPRtxyZfZOqd",
    "outputId": "b61c53f2-15b4-4f16-cdc2-375906fe1b92"
   },
   "outputs": [],
   "source": [
    "Z = model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Em2CvpgRZOqf"
   },
   "outputs": [],
   "source": [
    "centers, clusters = clustering_on_wordvecs(Z, 20);\n",
    "centroid_map = dict(zip(model.wv.index_to_key, clusters));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15_523YKZOqj"
   },
   "source": [
    "Next, we get words in each cluster that are closest to the cluster center. To do this, we initialize a KDTree on the word vectors, and query it for the Top K words on each cluster center. Using the Index 2 word dictionary, we than correspond each word vector back to it’s original word representation and add them to a dataframe for easier printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9aD76xcZOqk"
   },
   "outputs": [],
   "source": [
    "def get_top_words(index2word, k, centers, wordvecs):\n",
    "    tree = KDTree(wordvecs);\n",
    "    # Use closest points for each cluster center to query closest 20 points to it\n",
    "    closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers];\n",
    "    closest_words_idxs = [x[1] for x in closest_points];\n",
    "    # Query Word Index  for each position in the above array, and added to a Dictionary\n",
    "    closest_words = {};\n",
    "    for i in range(0, len(closest_words_idxs)):\n",
    "        closest_words['Cluster #' + str(i)] = [index2word[j] for j in closest_words_idxs[i][0]]\n",
    "    # Create DataFrame from dictionary\n",
    "    df = pd.DataFrame(closest_words);\n",
    "    df.index = df.index+1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtBFZL_HZOqm"
   },
   "source": [
    "Let’s get the top words and print the first 20 in each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwzjSQJlZOqn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "top_words = get_top_words(model.wv.index_to_key, 5000, centers, Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1639950178187,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "tI19jNzsZOqq",
    "outputId": "7cbda38d-d912-42ec-9c20-3fcda8998588",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_words[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EF6bB0rVReh"
   },
   "source": [
    "# T-SNE\n",
    "\n",
    "The word embeddings made by the model can be visualised by reducing dimensionality of the words to 2 dimensions using tSNE.\n",
    "\n",
    "T-Distributed Stochastic Neighbor Embedding, or t-SNE, is a dimensionality reduction technique to assist with visualizing high-dimensional datasets. It attempts to map high-dimensional data onto a low two- or three-dimensional representation. It tries to keep the relative distances between points as closely as possible in both high-dimensional and low-dimensional space.\n",
    "\n",
    "Visualisations can be used to notice semantic and syntactic trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pri9cdRfHlJ"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne_input = model.wv.drop(spacy.lang.en.stop_words.STOP_WORDS, errors=u'ignore')\n",
    "tsne_input = tsne_input.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1640016851980,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "zdgeT5flnyW-",
    "outputId": "97d65f27-56d2-4878-8a80-dd489cad6fb6"
   },
   "outputs": [],
   "source": [
    "tsne_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jd60bqFxe9z1"
   },
   "outputs": [],
   "source": [
    "# Create some filepaths\n",
    "tsne_filepath = 'tsne_model'\n",
    "tsne_vectors_filepath = 'tsne_vectors.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBe7lNE3e7oQ"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if 1 == 1:\n",
    "    \n",
    "    tsne = TSNE()\n",
    "    tsne_vectors = tsne.fit_transform(tsne_input.values)\n",
    "    \n",
    "    with open(tsne_filepath, 'wb') as f:\n",
    "        pickle.dump(tsne, f)\n",
    "\n",
    "    pd.np.save(tsne_vectors_filepath, tsne_vectors)\n",
    "    \n",
    "with open(tsne_filepath, 'rb') as f:\n",
    "    tsne = pickle.load(f)\n",
    "    \n",
    "tsne_vectors = pd.np.load(tsne_vectors_filepath)\n",
    "\n",
    "tsne_vectors = pd.DataFrame(tsne_vectors,\n",
    "                            index=pd.Index(tsne_input.index),\n",
    "                            columns=[u'x_coord', u'y_coord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AE4-OrmKmKG"
   },
   "outputs": [],
   "source": [
    "import bokeh\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "\n",
    "output_notebook()\n",
    "bokeh.io.output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "executionInfo": {
     "elapsed": 747,
     "status": "ok",
     "timestamp": 1640017607454,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "myJtActVdjn4",
    "outputId": "5f283a90-c0f4-4c31-88ea-5f7bfa6917ee"
   },
   "outputs": [],
   "source": [
    "# add our DataFrame as a ColumnDataSource for Bokeh\n",
    "plot_data = ColumnDataSource(tsne_vectors)\n",
    "\n",
    "# create the plot and configure the\n",
    "# title, dimensions, and tools\n",
    "tsne_plot = figure(title='t-SNE Word Embeddings',\n",
    "                   plot_width = 800,\n",
    "                   plot_height = 800)\n",
    "\n",
    "# add a hover tool to display words on roll-over\n",
    "tsne_plot.add_tools( HoverTool(tooltips = '@index') )\n",
    "\n",
    "# draw the words as circles on the plot\n",
    "tsne_plot.circle('x_coord', 'y_coord', source=plot_data,\n",
    "                 color='blue', line_alpha=0.2, fill_alpha=0.1,\n",
    "                 size=10, hover_line_color='black')\n",
    "\n",
    "# configure visual elements of the plot\n",
    "tsne_plot.xaxis.visible = False\n",
    "tsne_plot.yaxis.visible = False\n",
    "tsne_plot.grid.grid_line_color = None\n",
    "tsne_plot.outline_line_color = None\n",
    "\n",
    "# engage!\n",
    "show(tsne_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection: The Hermeneutics of Word Embeddings\n",
    "\n",
    "“In vector space, identities and differences change in nature. Similarity and belonging no longer rely on resemblance or a common genesis but on measures of proximity or distance, on flat loci that run as vectors through the space.” (Dourish 2018: 73-4)\n",
    "\n",
    "As we've seen, word embeddings are essentially a set of vectors. We should reflect on this. What is vectorization? It is reducing linguistic complexity. Or rather, it produces a common space that juxtaposes and mixes complex localized realities. Anything can be turned into a vector operation, but what do we lose when doing so? "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 4-1 Word Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
