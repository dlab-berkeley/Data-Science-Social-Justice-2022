{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science for Social Justice Workshop: Module 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we introduce topic modeling. Topic modeling aims to use statistical models to discover abstract \"topics\" that occur in a collection of documents. It is frequently used in NLP to aid the discovery of hidden semantic structures in a collection of texts.\n",
    "\n",
    "Before you start, please read the first three sections of [this post](https://tomvannuenen.medium.com/analyzing-reddit-communities-with-python-part-5-topic-modeling-a5b0d119add) for an explainer of how topic modeling (and LDA, which is just one form of topic modeling) works.\n",
    "\n",
    "Specifically, we'll implement Latent Dirichlet Allocation (LDA), which is a classic method for topic modeling. Specifically, LDA is a \"mixture model\", meaning every document is assumed to be \"about\" various topics, and we try to estimate the proportion each topic contributes to a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is a *Bayesian* model that captures how specific topics can generate documents. This means that, at the end of the day, it's modeling *probabilities* of tokens appearing in certain topics, rather than carving out distinct topics. It was [introduced](https://jmlr.csail.mit.edu/papers/v3/blei03a.htmlhttps://jmlr.csail.mit.edu/papers/v3/blei03a.html) in machine learning by Blei et al. It is one of the oldest models applied to perform topic modeling.\n",
    "\n",
    "LDA is also a *generative* model. This means that it can, theoretically, be used to generate new samples, or documents. Let's walk through that process. Assume we have a number of topics $T$. Then, we generate a new document as follows:\n",
    "\n",
    "1. Choose a number of words $N$ (this is done probabilistically according to what's called a *Poisson distribution*: again, this is the Bayesian part).\n",
    "2. Choose several values $\\boldsymbol{\\theta}=(\\theta_1, \\theta_2, \\ldots, \\theta_T)$, once again, probabilistically, according to a Dirichlet distribution. The details of a Dirichlet distribution aren't important other than that it guarantees all of the $\\theta_i$ add up to 1, and are positive. So, we can think of the $\\theta_i$ as proportions, or probabilities. That is, each $\\theta_i$ is the probability that topic $i$ is represented in a document.\n",
    "3. For each of the $N$ words $w_n$, where $n$ goes from 1 to $N$:\n",
    "- Choose a topic $t_n$, using the probabilities given by the $\\theta_i$. If $\\theta_i$ is larger, it's more likely to be chosen.\n",
    "- Each topic has a probability that each word will appear in it (mathematically, this is represented by the probability distribution $p(w_n|t_n)$, or $w_n$ conditioned on $t_n$). Draw the word according to these probabilities.\n",
    "\n",
    "LDA does not model the order of the words, so in the end, it produces a collection of words - just like the bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lda](../../img/lda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of variables there, so let's consider a concrete example. Let's suppose we have two topics: soccer and basketball. These are $t_1$ and $t_2$. \n",
    "\n",
    "Some topics are more likely to contains words than others. For example, soccer is more likely to contain `liverpool` and `freekick`, but probably not `nba`. Basketball meanwhile will very likely contain `rebound` and `nba`. Furthermore, even though it's unlikely, a soccer topic might still refer to the `nba`. This unlikeliness is captured through the probabilities assigned in the distribution $p(w_n|t_n)$.\n",
    "\n",
    "Next, each document might consist of multiple \"proportions\" of topics. So, Document 1 might mainly be about soccer, and not really reference basketball - this would be reflected in the probabilities $\\boldsymbol{\\theta}=(0.9, 0.1)$. Meanwhile, another document might equally reference soccer and basketball, so we'd need a different set of probabilities $\\boldsymbol{\\theta}=(0.5, 0.5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Topic Models on AITA\n",
    "\n",
    "Now, let's build a topic model on our data. We're going to use the lemmatized, preprocessed data we considered in the previous lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the data directory\n",
    "os.chdir(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('aita_sub_top_sm_lemmas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is built on top of the TF-IDF matrix we considered in the previous lesson (so word order does not influence the topics). Let's create the TF-IDF matrix as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X = df['lemmas']\n",
    "# Vectorize, using only the top 5000 TF-IDF values\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "tfidf =  vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply our LDA model. We have to choose the number of topics - in this case, called `n_components` - prior to fitting the model. We call this a *hyperparameter*. Choosing the right hyperparameter - the number of topics - can be tricky business. There are some heuristics for it, but for now, we'll just try 5 topics and see what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=5, max_iter=20, random_state=0)\n",
    "lda = lda.fit(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we provide a function which you can use to plot the words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words=10, n_row=1, n_col=5, normalize=False):\n",
    "    \"\"\"Plot the top words for an LDA model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : LatentDirichletAllocation object\n",
    "        The trained LDA model.\n",
    "    feature_names : list\n",
    "        A list of strings containing the feature names.\n",
    "    n_top_words : int\n",
    "        The number of top words to show for each topic.\n",
    "    n_row : int\n",
    "        The number of rows to use in the subplots.\n",
    "    n_col : int\n",
    "        The number of columns to use in the subplots.\n",
    "    normalize : bool\n",
    "        If True, normalizes the topic model weights.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(n_row, n_col, figsize=(3 * n_col, 5 * n_row), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    components = model.components_\n",
    "    if normalize:\n",
    "        components = components / components.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    for topic_idx, topic in enumerate(components):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_names = vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, token_names, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This topics may look slightly different, because LDA is not fully deterministic in how it generate these topics. However, you'll likely see:\n",
    "- A topic which seems to cover family relationships\n",
    "- A topic with many names\n",
    "- A topic covering food\n",
    "- A topic on finances\n",
    "- A topic which has no clear theme.\n",
    "\n",
    "At the end of the day, the model is only fitting probabilities. *We* are the ones who are trying to assign meaning to the topics, and place them in a context that makes sense to humans. The *interpretation* has a great degree of power in how a model gets used and is viewed by society, and we need to take great care that we handle this task properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Weights Across Documents\n",
    "\n",
    "One thing we may want to do with the output is compare the prevalence of each topic across documents. A simple way to do this, is to merge the topic distribution back into the `pandas` dataframe.\n",
    "\n",
    "Topic modeling has several practical applications. One of them is to determine what topic a Reddit post is about. To figure this out, we find the topic number that has the highest percentage contribution to that thread.\n",
    "\n",
    "First, we need to take our TF-IDF matrix, and *transform* it into the topic proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions = lda.transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf.shape)\n",
    "print(topic_distributions.shape)\n",
    "print(topic_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you explain what the different shapes correspond to? What did this transformation do? What do these values mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataframe with these topic values. We're going to, by default, just name the topics \"Topic 1\", \"Topic 2\", etc. But you can feel free to name the topics more specifically, if you feel comfortable with the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic topic names\n",
    "columns = [\n",
    "    \"Topic 1\",\n",
    "    \"Topic 2\",\n",
    "    \"Topic 3\",\n",
    "    \"Topic 4\",\n",
    "    \"Topic 5\"\n",
    "]\n",
    "# Or, choose topics\n",
    "columns = [\n",
    "    \"Family/Relationships\",\n",
    "    \"Names\",\n",
    "    \"Food\",\n",
    "    \"Finance\",\n",
    "    \"Misc\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = pd.DataFrame(topic_distributions, columns=columns)\n",
    "topic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bring the original text back into this dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df.insert(loc=0, column='text', value=df['selftext'])\n",
    "topic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at a couple documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [100, 1000, 10000]\n",
    "\n",
    "for idx in idxs:\n",
    "    print(topic_df['text'].iloc[idx][:500])\n",
    "    print(topic_df.iloc[idx, 1:])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These generally make sense post-hoc, but the topics could still use improvement. For one, we could, perhaps, use more topics. We can also supplement this analysis with qualitative work to motivate the number of topics. In this way, the qualitative analysis drive the modeling, rather than vice versa (which will, in general, be a more robust research approach)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Similarity\n",
    "\n",
    "Once we break down our documents into topics, we can more easily perform a quantitative assessment of the *similarity* between documents. You may recall the **cosine similarity**, which allows us to quantify the degree to which two vectors are similar: it's 1 if they're the same, and decreases to zero the more dissimilar they become. \n",
    "\n",
    "Let's apply the cosine similarity to these topics to find documents that are very similar to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarities\n",
    "similarities = cosine_similarity(topic_distributions)\n",
    "print(similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity between document 23 and 1000\n",
    "similarities[23, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_documents(similarities):\n",
    "    \"\"\"Find the pair of most similar documents.\"\"\"\n",
    "    copy = similarities.copy()\n",
    "    np.fill_diagonal(copy, 0)\n",
    "    copy[copy == 1] = 0.\n",
    "    idxs = np.unravel_index(np.argmax(copy), copy.shape)\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar = most_similar_documents(similarities)\n",
    "print(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[most_similar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext'].iloc[most_similar[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext'].iloc[most_similar[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the Number of Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the previous section, you can consider changing the number of topics which can influence how interpretable our topic models become.\n",
    "\n",
    "Try retraining the LDA witha different number of topics, say 10. What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=10, max_iter=20, random_state=0)\n",
    "lda = lda.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_words(lda, token_names, 20, n_row=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection: The hermeneutics of topic modeling\n",
    "\n",
    "One thought to end with: for most topic models you will create, it will be hard to apply a meaningful interpretation to each topic. Not every topic will have some meaningful insight \"fall out of it\" upon first inspection. This is a typical issue in machine learning, which can pick up on patterns that might not make sense to humans.\n",
    "\n",
    "It is an open question to which extent you should let yourself be surprised by particular combinations of words in a topic, or if topic models primarily should follow the intuitions you already have as a researcher. What makes for a \"good\" topic model probably straddles the boundaries of surprise and expectation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
