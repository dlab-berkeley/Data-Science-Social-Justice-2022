{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNXZ3qG4plMW"
   },
   "source": [
    "# Data Science for Social Justice Workshop: Module 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtIBMFnNrQlL"
   },
   "source": [
    "## Language Biases and Word Embeddings\n",
    "\n",
    "Language carries implicit biases, functioning both as a reflection and a perpetuation of stereotypes that people carry with them. Using natural language processing tools, we can trace these biases in the many language datasets to be found online.\n",
    "\n",
    "One way to discover language biases is via word embeddings. In order to do so, we first need to postulate concepts for terms along which we might expect biases to exist. For example, humans often associate words with masculinity or femininity, which reflects both a normative construction of a gender binary, as well as stereotypes *within* that binary. Word embeddings provide us word vectors for many terms, including \"male\" and \"female\", as well as other words that may be more strongly associated with the concepts of \"male\" and \"female\", as reflected in language use. We can attempt to quantify this directly using notions of distance with word embeddings.\n",
    "\n",
    "Specifically, we develop *target concepts* for the terms we aim to identify biases along (e.g., \"male\" and \"female\"). Then, we can then compute relative similarities of other word vectors – particularly, words that act as evaluative attributes such as \"strong\" and \"sensitive\". These words can be categorised through clustering algorithms and labeled through a semantic analysis system into more general (conceptual) biases, yielding a broad picture of the biases present in a discourse community.\n",
    "\n",
    "This notebook is based off a paper examining language biases in Reddit: see [here](https://xfold.github.io/WE-GenderBiasVisualisationWeb/) for a web demo\n",
    "and [here](https://github.com/xfold/LanguageBiasesInReddit) for the full repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "import os\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19056,
     "status": "ok",
     "timestamp": 1647300674056,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "e8vdjKO4fOh5",
    "outputId": "24d80e36-10eb-4ff4-facc-e0cb15bc4933"
   },
   "outputs": [],
   "source": [
    "# Change directory\n",
    "# We include two ../ because we want to go two levels up in the file structure\n",
    "os.chdir(\"../../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the Word Embeddings model we made in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2076396,
     "status": "ok",
     "timestamp": 1647308247064,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "5TRHImTC4wPs",
    "outputId": "120f9b7e-c7af-46c6-d34c-9281593b7072"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('aita.emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSbUVbC-rQlY"
   },
   "source": [
    "## Obtaining Biased Words Relative to Target Concepts\n",
    "\n",
    "We now run our method of finding biased words towards our target sets. We will be using functions in an external file - `utils.py` - which you are free to look through if you're interested. Otherwise, feel free to use the functions as desired.\n",
    "\n",
    "Given a vocabulary and two sets of target words (such as, in this case, those for *women* and *men*), we rank the words from least to most biased. As such, we obtain two ordered lists of the most biased words towards each target set, obtaining an overall view of the bias distribution in that particular community with respect to those two target sets. \n",
    "\n",
    "Here's what happening in the next block of code:\n",
    "- We calculate the centroid of a target set by averaging the embedding vectors in our target set (e.g. the vectors for `he, son, his, him, father, male` for our target concept `male`);\n",
    "- We calculate the cosine similarity between the vectors for all words in our vocabulary as compared to our two centroids (we also apply POS-filtering to only work with parts of speech we expect to be relevant);\n",
    "- We use a threshold based on standard deviation to determine how severe a bias needs to be before we include it;\n",
    "- We rank the words in the vocabulary of our word embeddings model based on their bias towards either target concept.\n",
    "\n",
    "Thus, the implicit hypothesis here is that word vectors much closer to the centroid of one target concept than the other are used more similarly with respect that concept. This, in effect, is an effort to quantify the bias of words used in general discourse within a community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function to calculate biased words\n",
    "from utils import calculate_biased_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are our target concept vectors. This is a critical choice: the centroid of these represents the concept at hand. What happens if you change the words included?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1 = [\"sister\" , \"female\" , \"woman\" , \"girl\" , \"daughter\" , \"she\" , \"hers\" , \"her\"]\n",
    "target2 = [\"brother\" , \"male\" , \"man\" , \"boy\" , \"son\" , \"he\" , \"his\" , \"him\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11526,
     "status": "ok",
     "timestamp": 1647303428120,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "SIzdtQV5rQlh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[b1, b2] = calculate_biased_words(model, target1, target2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJpg5BDydlYT"
   },
   "source": [
    "Let's print some biases. Here you see the most-biased words towards our target concepts (1 being *women*, 2 being *men*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1647303438347,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "qc-9g8BErQll",
    "outputId": "93aa1d97-e9ad-4f17-9ce3-bef7d4079209"
   },
   "outputs": [],
   "source": [
    "print('Biased words towards target set 1')\n",
    "print([word for word in b1.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Biased words towards target set 2')\n",
    "print([word for word in b2.keys()] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zpk0lx2xplMf"
   },
   "source": [
    "## Visualizing Biases using $t$-SNE\n",
    "\n",
    "We now return to our dimensionality reduction technique, $t$-SNE, to try and visualize these biased words in a 2-d space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(\n",
    "    perplexity=40,\n",
    "    n_components=2,\n",
    "    init='pca',\n",
    "    n_iter=5000,\n",
    "    random_state=23,\n",
    "    learning_rate='auto')\n",
    "X_tsne = tsne.fit_transform(model.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1_idx = np.array([model.wv.key_to_index[key] for key in b1.keys()])\n",
    "target2_idx = np.array([model.wv.key_to_index[key] for key in b2.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_target1 = X_tsne[target1_idx]\n",
    "X_target2 = X_tsne[target2_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "ax.scatter(X_target1[:, 0], X_target1[:, 1], color='C0')\n",
    "ax.scatter(X_target2[:, 0], X_target2[:, 1], color='C1')\n",
    "\n",
    "for idx, point in enumerate(X_target1):\n",
    "    ax.annotate(\n",
    "        model.wv.index_to_key[target1_idx[idx]],\n",
    "        xy=(point[0], point[1]),\n",
    "        xytext=(6, 3),\n",
    "        textcoords='offset points',\n",
    "        ha='right',\n",
    "        va='bottom')\n",
    "    \n",
    "for idx, point in enumerate(X_target2):\n",
    "    ax.annotate(\n",
    "        model.wv.index_to_key[target1_idx[idx]],\n",
    "        xy=(point[0], point[1]),\n",
    "        xytext=(6, 3),\n",
    "        textcoords='offset points',\n",
    "        ha='right',\n",
    "        va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riINX9wVyFRJ"
   },
   "source": [
    "## Existing Target Sets\n",
    "\n",
    "Here are some other target sets that have been previously used in the literature:\n",
    "\n",
    "* *Gender target sets taken from Nosek, Banaji, and Greenwald 2002.*\n",
    "    - Female: `sister, female, woman, girl, daughter, she, hers, her`.\n",
    "    - Male: `brother, male, man, boy, son, he, his, him`.\n",
    "* *Religion target sets taken from Garg et al. 2018.*\n",
    "    - Islam: `allah, ramadan, turban, emir, salaam, sunni, koran, imam, sultan, prophet, veil, ayatollah, shiite, mosque, islam, sheik, muslim, muhammad`.\n",
    "    - Christianity: `baptism, messiah, catholicism, resurrection, christianity, salva-tion, protestant, gospel, trinity, jesus, christ, christian, cross,catholic, church`.\n",
    "* *Racial target sets taken from Garg et al. 2017*\n",
    "    - White last names: `harris, nelson, robinson, thompson, moore, wright, anderson, clark, jackson, taylor, scott, davis, allen, adams, lewis, williams, jones, wilson, martin, johnson`.\n",
    "    - Hispanic last names: `ruiz, alvarez, vargas, castillo, gomez, soto,gonzalez, sanchez, rivera, mendoza, martinez, torres, ro-driguez, perez, lopez, medina, diaz, garcia, castro, cruz`.\n",
    "    - Asian last names: `cho, wong, tang, huang, chu, chung, ng,wu, liu, chen, lin, yang, kim, chang, shah, wang, li, khan,singh, hong`.\n",
    "    - Russian last names: `gurin, minsky, sokolov, markov, maslow, novikoff, mishkin, smirnov, orloff, ivanov, sokoloff, davidoff, savin, romanoff, babinski, sorokin, levin, pavlov, rodin, agin`.\n",
    "* *Career/family target sets taken from Garg et al. 2018.*\n",
    "    - Career: `executive, management, professional, corporation, salary, office, business, career`.\n",
    "    - Family: `home, parents, children, family, cousins, marriage, wedding, relatives.Math: math, algebra, geometry, calculus, equations, computation, numbers, addition`.\n",
    "* *Arts/Science target sets taken from Garg et al. 2018.*\n",
    "    - Arts: `poetry, art, sculpture, dance, literature, novel, symphony, drama`.\n",
    "    - Science: `science, technology, physics, chemistry, Einstein, NASA, experiment, astronomy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHCHMAPYTCVX"
   },
   "source": [
    "### Sources\n",
    "\n",
    "Nosek, B. A., Banaji, M. R., & Greenwald, A. G. (2002). Harvesting implicit group attitudes and beliefs from a demonstration web site. Group Dynamics, 6(1), 101–115. https://doi.org/10.1037/1089-2699.6.1.101\n",
    "\n",
    "Garg, N., Schiebinger, L., Jurafsky, D., & Zou, J. (2017). Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes, 1–33."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 4-2 Language Biases.ipynb",
   "provenance": [
    {
     "file_id": "1IgWcAC_gXRgwysSbqDr5ZIO9bxSVb6dl",
     "timestamp": 1601554822392
    },
    {
     "file_id": "1HXx9kevQetMI9pE0B9XCTFjgUHiGC4hm",
     "timestamp": 1594637500809
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
