{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science for Social Justice Workshop: Module 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis\n",
    "\n",
    "In this notebook, we'll access the Reddit Application Programming Interface (API) to do a small network analysis of \"influencers\" in our data. An API is effectively a protocol for how to obtain information from someone else's servers. For example, Reddit has all its users, comments, upvotes, downvotes, etc., stored in its own databases. We could obtain some of this information by simply scraping the web - but this is an arduous and time-consuming process. So, Reddit has provided a way to access portions of its database in a streamlined manner, and the API is the \"guidebook\" for how to do so. We can use this API to get more information on the users in the AITA subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting an API Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Sign Up.** First, you will need to sign up with Reddit to run some of the code. Go to http://www.reddit.com and **sign up** for an account.\n",
    "\n",
    "2. **Create an App.** Go to [this page](https://ssl.reddit.com/prefs/apps/) and click on the `are you a developer? create an app` button at the bottom.\n",
    "\n",
    "3. **Fill Out the Form.** Fill out the form that appears. For the name, you can enter whatever you'd like. Select \"script\". Enter the redirect uri as shown. Otherwise, you can leave everything else blank. Then, click \"create app\".\n",
    "\n",
    "![redditapi](../../img/reddit_api.png)\n",
    "\n",
    "4. **Note API Credentials.** You should see a new box appear, with some important information. This includes:\n",
    "    - Client ID: A 14-character string (at least) listed just under “personal use script” for the desired developed application.\n",
    "    - Client Secret: A 27-character string (at least) listed adjacent to secret for the application.\n",
    "    - Username: The username of the Reddit account used to register the application.\n",
    "    - Password: This is not shown here, but you should remember your password to your account.\n",
    "    \n",
    "![redditapi2](../../img/reddit_api2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Using `praw`\n",
    "\n",
    "Even though we're set up with the API, we still need to have a way to use Python to interface with the API. Luckily, this is already done for us via the Python Reddit API Wrapper: `praw`. This is a package we can download and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `praw`, you need to grab the information that you noted before from your API. Fill in the details below. **Do not share your credentials with anyone. Be especially careful not to share them via a public portal (e.g., GitHub). If you do so, you should consider them comprised, and obtain a new set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id='YOUR_CLIENT_ID_HERE',\n",
    "                     client_secret='YOUR_CLIENT_SECRET_HERE',\n",
    "                     password='YOUR_REDDIT_PSW_HERE',\n",
    "                     user_agent='Get Reddit network data, v1.0, by /u/YOUR_USERNAME_HERE',\n",
    "                     username='YOUR_USERNAME_HERE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis: Finding Influencers in Reddit Data\n",
    "\n",
    "When working with Reddit data, we can't determine the most influential users at a glance. Other social media platforms have follower counts which directly quantify the amount of reach a user is likely to have, while Redditors only have karma, i.e., the net total up and down votes since account creation, and a log of their posts and comments in different subreddits. These two statistics can give a rough idea of a user's activity.\n",
    "\n",
    "It has already been found that a very small percentage of Reddit’s users create the vast majority of the site’s content, so we would not be surprised if only a few users could influence the discourse of entire subreddits. Identifying these users would help us understand how a subreddit's discourse is shaped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('aita_sub_top_sm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort by score and just get the top 1000 posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='score', ascending=False)[:1000]\n",
    "# Sanity check\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many *unique* authors do we have in our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.author.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a potential relationship between score and number of comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot('score', 'num_comments', kind='scatter', color='black', alpha=0.25, logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scatter plot shows that the number of comments don’t necessarily increase with posts that have a higher net score.\n",
    "\n",
    "Let's only look at the users who posted more than once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating = df[df.duplicated(['author'], keep=False)]\n",
    "# Get rid of deleted users\n",
    "repeating = repeating[repeating['author'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of all posts, this is the amount of people who posted more than once \n",
    "repeating.author.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to decide which of these users we consider to be \"influencers\". Let's first have a look at where and how often these popular authors are posting. We'll define a function that can get us the other posts by the influencers we have found in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_posts(author, n_submissions):\n",
    "    \"\"\"Gets the posts by a Reddit user.\"\"\"\n",
    "    try:\n",
    "        # Create a \"redditor\" object\n",
    "        redditor = reddit.redditor(author)\n",
    "        user_posts_list = []\n",
    "        # Iterate over the top N submissions for the redditor\n",
    "        for submission in redditor.submissions.top(limit=n_submissions):\n",
    "            # Obtain information about each submission\n",
    "            info_list = [submission.id,\n",
    "                         submission.score,\n",
    "                         str(submission.author),\n",
    "                         submission.num_comments,\n",
    "                         str(submission.subreddit)]\n",
    "            user_posts_list.append(info_list)\n",
    "    # Dealing with errors in case redditors have been banned, deleted their accounts, etc.\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Sort submissions in decreasing order of score\n",
    "    sorted_submissions = sorted(user_posts_list, key=lambda x: x[1], reverse=True)\n",
    "    # Place submissions in a dataframe.\n",
    "    user_posts_df = pd.DataFrame(sorted_submissions,\n",
    "                                 columns=['id', 'score', 'author', 'n_comments', 'subreddit'])\n",
    "    return user_posts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty dataframe\n",
    "authors_df =  pd.DataFrame().fillna(0)\n",
    "# Loops through every \"influencer\" user and gets 20 top posts per user\n",
    "for author in authors:\n",
    "    user_posts_df = get_user_posts(author, 20)\n",
    "    authors_df = pd.concat([authors_df, user_posts_df]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df = authors_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(authors_df.shape)\n",
    "authors_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's find out where else these influencers posted. We'll compile a list of authors that appeared more than once on other subreddits. In order to form a network graph, we need data about the particular subreddits where our influencers appeared. For the sake of simplicity, we visualized those subreddits with at least 2 or more posts made by the influencers. The Y-axis is the number of submissions and the X-axis are the respective subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of each subreddit\n",
    "counts = authors_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only plot the subreddits that appear more than twice\n",
    "ax = counts[counts > 2].plot(\n",
    "    kind='bar',\n",
    "    title='Distribution of other subreddits where influencers post',\n",
    "    figsize=(10, 5),\n",
    "    rot=15) \n",
    "ax.set_xlabel('Subreddits')\n",
    "ax.set_ylabel('Number of Posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conducting a Network Analysis\n",
    "\n",
    "Finally, we're going to use a package called `networkx` to visualize the subreddits that people post in. The graphical representation will allow us to better assess how each Redditor posts, and where they are influential.\n",
    "\n",
    "First, let's install `networkx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we extract the author and subreddit in our data frame. This is what we're going to use to create our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for network graph purposes \n",
    "network_df = authors_df[['author', 'subreddit']]\n",
    "network_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of unique subreddits to use in network graph\n",
    "subreddits = list(network_df.subreddit.unique()) \n",
    "# Make list of unique authors to use in network graph \n",
    "authors = list(network_df.author.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the graph. There's a lot of moving parts here. In effect, what we're doing is visualizing the *other* subreddits that the top posters on AITA are posting in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "plt.figure(figsize=(18, 18))\n",
    "\n",
    "# Create the graph from the dataframe\n",
    "g = nx.from_pandas_edgelist(network_df, source='author', target='subreddit') \n",
    "\n",
    "# Create a layout for nodes \n",
    "layout = nx.spring_layout(g, iterations=50, scale=2)\n",
    "\n",
    "# Draw the parts we want, edges thin and grey\n",
    "# Influencers appear small and grey\n",
    "# Subreddits appear in blue and sized according to their respective number of connections.\n",
    "# People who have more connections are highlighted in color \n",
    "\n",
    "# Go through every subbreddit, ask the graph how many connections it has. \n",
    "# Multiply that by 80 to get the circle size\n",
    "sub_size = [g.degree(sub) * 80 for sub in subreddits]\n",
    "nx.draw_networkx_nodes(g, \n",
    "                       layout, \n",
    "                       nodelist=subreddits, \n",
    "                       node_size=sub_size, # a LIST of sizes, based on g.degree\n",
    "                       node_color='lightblue')\n",
    "\n",
    "# Draw all the entities \n",
    "nx.draw_networkx_nodes(g, layout, nodelist=authors_df['author'], node_color='#cccccc', node_size=100)\n",
    "\n",
    "# Draw highly connected influencers \n",
    "influencers = [person for person in authors_df['author'] if g.degree(person) > 1]\n",
    "nx.draw_networkx_nodes(g, layout, nodelist=influencers, node_color='orange', node_size=100)\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(g, layout, width=1, edge_color=\"#cccccc\")\n",
    "\n",
    "# Labels for subreddits and authors\n",
    "node_labels = dict(zip(subreddits, subreddits))\n",
    "auth_labels = dict(zip(authors, authors))\n",
    "\n",
    "nx.draw_networkx_labels(g, layout, labels=node_labels)\n",
    "nx.draw_networkx_labels(g, layout, labels=auth_labels)\n",
    "\n",
    "# No axis needed\n",
    "plt.axis('off')\n",
    "plt.title(\"Network Graph of Related Subreddits\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph, influencer nodes appear small and grey. The influencers who have more connections than just r/amitheasshole are highlighted in yellow. The subreddits appear in blue and sized according to their respective number of connections. All the redditors listed post in AITA, but they also post in other communities: what do those communities tell you about that subredditors and their interests?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 2 OPTIONAL - Reddit API.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
