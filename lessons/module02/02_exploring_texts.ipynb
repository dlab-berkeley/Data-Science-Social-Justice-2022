{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science for Social Justice Workshop: Module 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJHDHrQPbjif"
   },
   "source": [
    "## Exploring Texts with `pandas` and `nltk`\n",
    "\n",
    "This notebook introduces some methods to explore text using `pandas` and the Natural Language Toolkit, [`nltk`](https://www.nltk.org/). We'll keep building on our AITA data frame, discussing some simple ways to explore data. \n",
    "\n",
    "This notebook is designed to help you:\n",
    "1. Do some more data operations in `pandas`;\n",
    "2. Use NLTK's `Text()` object to perform some basic distant reading operations on a subreddit.\n",
    "\n",
    "You might not have `nltk` installed; if so, go ahead and install it with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if nltk is not installed\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6erwYIhh-cR"
   },
   "source": [
    "## Retrieving the Dataset\n",
    "\n",
    "Let's get the data. If you're not in the right directory, use `os.chdir` to navigate there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22903,
     "status": "ok",
     "timestamp": 1647477678422,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "qqV-FJC9aHuT",
    "outputId": "9126a240-03b0-4403-eda9-afb71f673262"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# We include two ../ because we want to go two levels up in the file structure\n",
    "os.chdir('../../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKZ6v_EzpUCi"
   },
   "source": [
    "Let's now import the processed data from the last notebook.\n",
    "\n",
    "**Note:** You can replace this line with whichever processed data you have saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5173,
     "status": "ok",
     "timestamp": 1647477683593,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "WBLyQUt1qZRG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('aita_sub_top_sm_lemmas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuIFFo8clcwo"
   },
   "source": [
    "## Diving Deeper into `pandas`\n",
    "\n",
    "Let's get started by looking at some more useful `pandas` operations. First, let's use `head()` to remind ourselves what the data look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 549
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1647477683596,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "zipTYx8rlbOo",
    "outputId": "b92d1d7b-071b-4223-f204-ffeb61a2c802"
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RW9zLOkjNok-"
   },
   "source": [
    "Using the `.sort_values()` method, we can sort the data frame by particular columms. We use two parameters: the `by` parameter indicates by which column we want to sort, the `ascending` parameter indicated whether sorting is performed in ascending or descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1647477683596,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "XogbDC5iNok-",
    "outputId": "73ab007f-8658-4813-db51-f2e3c2c3f3e7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idint</th>\n",
       "      <th>idstr</th>\n",
       "      <th>created</th>\n",
       "      <th>created_datetime</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>score</th>\n",
       "      <th>distinguish</th>\n",
       "      <th>textlen</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>flair_css_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2635</th>\n",
       "      <td>797709732</td>\n",
       "      <td>t3_d6xoro</td>\n",
       "      <td>1568998300</td>\n",
       "      <td>2019-09-20 16:51:40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DarthCharizard</td>\n",
       "      <td>META: This sub is moving towards a value syste...</td>\n",
       "      <td>I’ve enjoyed reading and posting on this sub f...</td>\n",
       "      <td>enjoyed reading posting sub months feel like n...</td>\n",
       "      <td>80915.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6215.0</td>\n",
       "      <td>META</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12793</th>\n",
       "      <td>1472895100</td>\n",
       "      <td>t3_ocx94s</td>\n",
       "      <td>1625315782</td>\n",
       "      <td>2021-07-03 12:36:22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OnlyInQuebec9</td>\n",
       "      <td>AITA for telling my wife the lock on my daught...</td>\n",
       "      <td>My brother in-law (Sammy) lost his home shortl...</td>\n",
       "      <td>brother law sammy lost home shortly divorce mo...</td>\n",
       "      <td>80334.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2664.0</td>\n",
       "      <td>5318.0</td>\n",
       "      <td>Not the A-hole</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>664921441</td>\n",
       "      <td>t3_azvko1</td>\n",
       "      <td>1552322462</td>\n",
       "      <td>2019-03-11 16:41:02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Renegadesrule33</td>\n",
       "      <td>UPDATE, AITA for despising my mentally handica...</td>\n",
       "      <td>I'm back like I said I would be,. My [original...</td>\n",
       "      <td>like said original_post](https://www.reddit.co...</td>\n",
       "      <td>72776.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>UPDATE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            idint      idstr     created    created_datetime  nsfw  \\\n",
       "2635    797709732  t3_d6xoro  1568998300 2019-09-20 16:51:40   0.0   \n",
       "12793  1472895100  t3_ocx94s  1625315782 2021-07-03 12:36:22   0.0   \n",
       "398     664921441  t3_azvko1  1552322462 2019-03-11 16:41:02   0.0   \n",
       "\n",
       "                author                                              title  \\\n",
       "2635    DarthCharizard  META: This sub is moving towards a value syste...   \n",
       "12793    OnlyInQuebec9  AITA for telling my wife the lock on my daught...   \n",
       "398    Renegadesrule33  UPDATE, AITA for despising my mentally handica...   \n",
       "\n",
       "                                                selftext  \\\n",
       "2635   I’ve enjoyed reading and posting on this sub f...   \n",
       "12793  My brother in-law (Sammy) lost his home shortl...   \n",
       "398    I'm back like I said I would be,. My [original...   \n",
       "\n",
       "                                                  lemmas    score distinguish  \\\n",
       "2635   enjoyed reading posting sub months feel like n...  80915.0         NaN   \n",
       "12793  brother law sammy lost home shortly divorce mo...  80334.0         NaN   \n",
       "398    like said original_post](https://www.reddit.co...  72776.0         NaN   \n",
       "\n",
       "       textlen  num_comments      flair_text flair_css_class  \n",
       "2635       9.0        6215.0            META             NaN  \n",
       "12793   2664.0        5318.0  Not the A-hole             not  \n",
       "398        9.0        1989.0          UPDATE             NaN  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort dataframe by highest scores\n",
    "df.sort_values(by=['score'], ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5TvYkHCqytt"
   },
   "source": [
    "One thing we often do when we’re exploring a dataset is filtering based on a given condition. For example, we might need to find all the rows in our dataset where the score is over 500. We can use the `.loc[]` method to do so.\n",
    "\n",
    "`.loc[]` is a powerful method that can be used for all kinds of research purposes, if you want to filter or prune your dataset based on some condition. For more info, see [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html).\n",
    "\n",
    "The basic structure of `.loc[]` is `pd.loc[rows, columns]`, where `rows` is the rows to select (usually a conditional as in the cell below), and `columns` is a list of names of columns to select. Either value can be replaced by `:` to mean \"select all columns/rows\".\n",
    "\n",
    "For instance, if we only want rows with a score higher than 500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1647477683596,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "5DZbBlbkqytt",
    "outputId": "9e72f5f5-54d3-4042-fc42-da09d1398414"
   },
   "outputs": [],
   "source": [
    "df_top = df.loc[df['score'] >= 500, :]\n",
    "len(df_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of different ways to subset data, but `.loc[]` is one of the most versatile and powerful ways to select different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XTgmxK_pOFx"
   },
   "source": [
    "## Value Counts\n",
    "\n",
    "We can also look at unique value counts for a column by running `value_counts()`. Value counts indicate the number of samples with each unique value for a column.\n",
    "\n",
    "Let's have a look at the column `\"flair_text\"`. A flair, in Reddit, allows users to tag posts or usernames in certain subreddits to add context or humor. Here, the flair attached to the posts are created by moderators after the community votes on whether an OP was or wasn't the asshole. There are also other flairs such as \"No assholes here\" or \"Everyone sucks\". See [here](https://www.reddit.com/r/AmItheAsshole/wiki/faq) for more information.\n",
    "\n",
    "This community-driven data segmentation is quite helpful for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1639766457857,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "rn3_LO27pL9S",
    "outputId": "291cef5e-7263-4f34-87e3-4e88a7f1cd2a"
   },
   "outputs": [],
   "source": [
    "df.flair_text.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['flair_text'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a2w-4BvmwYJ"
   },
   "source": [
    "## Type-token Ratio\n",
    "\n",
    "Next, let's figure out the **type-token ratio** (TTR) for our posts. Type-token ratio is a crude algorithm to gauge language complexity. First, we'll create a function that computes the TTR. From just reading the code, what does a higher TTR correspond to? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lo1Gz5hmwYJ"
   },
   "outputs": [],
   "source": [
    "def type_token_ratio(tokens):\n",
    "    \"\"\"Calculates type-token ratio on tokens.\"\"\"\n",
    "    numTokens = len(tokens)\n",
    "    numTypes = len(set(tokens))\n",
    "    return numTypes / numTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPddUWELmwYM"
   },
   "source": [
    "Finally, we loop over several lemmatized submissions in our dataframe. Do you think that the TTR is capturing a varation in language complexity?\n",
    "\n",
    "The benefit of the TTR is a single number telling us some characteristics of the text, while the drawbacks are just that: a single number will not capture al of the complexity of a text. However, in concert with other techniques, this can be a helpful tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1647470189383,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": 300
    },
    "id": "LN3t9CwKIvqy",
    "outputId": "7bb5dff4-00d2-4f08-d326-6e5b20e50b4d"
   },
   "outputs": [],
   "source": [
    "for text in df['lemmas'][100:105]:\n",
    "    tokens = text.split()\n",
    "    print('Text:\\n', text)\n",
    "    print('TTR:', type_token_ratio(tokens), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yK1DCuAJbjkh"
   },
   "source": [
    "## Processing and Analyzing Language with `Text()`\n",
    "\n",
    "Let's have another look at our data. Another powerful tool for initially exploring texts, `nltk` provides a `Text()` class that can process text and supports opreations such as **counting**, **concordancing**, and **collocation discovery**. \n",
    "\n",
    "Let's use our \"lemmas\" data we created in the last notebook. All we need to do to get the total tokens is run `split()` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for idx, row in enumerate(df['lemmas']):\n",
    "    # Notice that we put all tokens in the same list\n",
    "    tokens.extend(row.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1639772391510,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "cSnOXiTK8KjJ",
    "outputId": "fff12139-267a-4dbd-bc97-cef445f9d3c6"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.text import Text\n",
    "\n",
    "aita_tokens = Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAdI-HgMbjkl"
   },
   "source": [
    "Let's print out the \"docstring\" of the [`Text()`](https://www.nltk.org/api/nltk.text.html) object, as well as all the things you can do with this object. Have a read through this to see what it allows you to do! We will talk about a few simple operations here, but there's many others that can be helpful - take a look at the docstring below to read a bit more about them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvexG4e7bjkm"
   },
   "outputs": [],
   "source": [
    "help(Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPatGDHDOpaU"
   },
   "source": [
    "### Concordances\n",
    "\n",
    "One of the most basic, but quite helpful, ways to quickly get an overview of the contexts in which a word appears is through a **concordance** view. `Text.concordance()` takes as input a word and a window width, and shows all cases where that word appears in a text and a window of context around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1639772396041,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "nkDsXJkPbjkp",
    "outputId": "b9f3ec29-0485-438f-d1ab-628168a0a81e"
   },
   "outputs": [],
   "source": [
    "aita_tokens.concordance('mistake', width=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJxpt0sDgJ6A"
   },
   "source": [
    "### Collocations\n",
    "\n",
    "A **collocation** is a sequence of words that often appear together. The `.collocation_list()` method will help identify words that often appear together in our texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1763,
     "status": "ok",
     "timestamp": 1639772405159,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "rWz18pTIgGtq",
    "outputId": "75eb133e-f126-412a-b8a4-ae9929bc6fe3"
   },
   "outputs": [],
   "source": [
    "aita_tokens.collocation_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change input arguments\n",
    "aita_tokens.collocation_list(num=30, window_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5q_G1GKkZSy"
   },
   "source": [
    "### Similar Words\n",
    "\n",
    "We might also want to look at similar words in the dataset (as defined as appearing in the same context). This is **distributional similarity**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4512,
     "status": "ok",
     "timestamp": 1639772469227,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "mWhoVtkrjBx_",
    "outputId": "10e2820d-d39e-4c3c-8799-42a5b78f5c3a"
   },
   "outputs": [],
   "source": [
    "aita_tokens.similar('partner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOC5V4Pje0G9"
   },
   "source": [
    "### Common Context\n",
    "\n",
    "The `.common_contexts()` method allows us to study the **common context** of two or more words: when they occur in the same immediate context. We must enclose these words in square brackets and round brackets, separated by commas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1639772490454,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -60
    },
    "id": "ccJOlE_se6Dp",
    "outputId": "1e9e204d-36d5-4dcd-8fd8-fac68833f0d2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aita_tokens.common_contexts(['mom', 'dad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating Time\n",
    "\n",
    "One of the most valuable metadata variables working with text is time. If we know when a text was created, then we can explore all sorts of properties in the text, and their dynamics.\n",
    "\n",
    "In this dataset, the time is contained in the `created` column. It is a Unix timestamp: the number of seconds that have elapsed since the Unix epoch, minus leap seconds. The Unix epoch is 00:00:00 UTC on 1 January 1970. While this works great for computers, it's not a particularly helpful format for humans! So we use `pd.datetime()` to convert this number to a more readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(1207632114, unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new column in `pandas` is as easy as using the bracket notation to write a new column name, then assigning it. In this case, we just use the `.to_datetime()` method again to point to the entire \"created\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetimes = pd.to_datetime(df['created'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(loc=3, column='created_datetime', value=datetimes)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select particular components of the timestamps by calling `.year`, `.month`,`.day`, etc. on the datetime column.\n",
    "\n",
    "We'll first create a new df with just the submissions from 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2021 = df.loc[pd.DatetimeIndex(df['created_datetime']).year == 2021, :]\n",
    "len(df_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2021.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `month_name()` method of `DateTimeIndex`, we can see which month each post was written in:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_array = pd.DatetimeIndex(df_2021['created_datetime']).month_name()\n",
    "months_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our data with the datetime objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('aita_sub_top_sm_lemmas.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we covered some techniques for summarizing text:\n",
    "\n",
    "- Using `pandas` to sort, filter and summarize text.\n",
    "- Using metrics like the token-type-ratio.\n",
    "- Using the `nltk` package to quickly process text.\n",
    "\n",
    "These techniques are particularly helpful for getting to know your dataset, and exploring text in an efficient way. In the next notebook we will talk about converting text to numeric data that can be used for many useful machine learning techniques."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week 2 Distant Reading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
